#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Flux LoRA è®­ç»ƒè„šæœ¬ï¼ˆåŸºäº diffusers å®˜æ–¹æ–¹æ³•ï¼‰
é€‚é… H20 GPU å’Œä½ çš„è®­ç»ƒæ•°æ®æ ¼å¼
"""

import os
import torch
from pathlib import Path
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from diffusers import FluxPipeline
from peft import LoraConfig, get_peft_model
from accelerate import Accelerator
from tqdm import tqdm
import argparse


class HostDataset(Dataset):
    """ä¸»æŒäººè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_dir: str, tokenizer, tokenizer_2, size: int = 1024):
        self.data_dir = Path(data_dir)
        self.tokenizer = tokenizer  # CLIP tokenizer
        self.tokenizer_2 = tokenizer_2  # T5 tokenizer
        self.size = size
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
        self.images = []
        image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.JPG', '.JPEG', '.PNG', '.WEBP'}
        
        for img_file in sorted(self.data_dir.iterdir()):
            if img_file.suffix in image_extensions:
                prompt = self._extract_prompt_from_filename(img_file.name)
                self.images.append({
                    'path': img_file,
                    'prompt': prompt
                })
        
        print(f"âœ… æ‰¾åˆ° {len(self.images)} å¼ è®­ç»ƒå›¾ç‰‡")
    
    def _extract_prompt_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æç¤ºè¯"""
        if '_repeat_' in filename:
            parts = filename.split('_repeat_', 1)
            if len(parts) > 1:
                prompt_part = parts[1].split('_', 1)
                if len(prompt_part) > 1:
                    prompt = prompt_part[1]
                    prompt = prompt.rsplit('.', 1)[0]
                    return prompt
        return "ç§‘æ™®ä¸»æŒäººï¼Œä¸“ä¸šå½¢è±¡"
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        item = self.images[idx]
        
        # åŠ è½½å›¾ç‰‡
        image = Image.open(item['path']).convert('RGB')
        if image.size != (self.size, self.size):
            image = image.resize((self.size, self.size), Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸º tensor
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
        image_tensor = transform(image).float()
        
        # Tokenize æç¤ºè¯ï¼ˆFlux ä½¿ç”¨ CLIP + T5 åŒç¼–ç å™¨ï¼‰
        prompt = item['prompt']
        text_inputs_1 = self.tokenizer(
            prompt,
            padding="max_length",
            max_length=77,  # CLIP tokenizer é»˜è®¤é•¿åº¦
            truncation=True,
            return_tensors="pt"
        )
        text_inputs_2 = self.tokenizer_2(
            prompt,
            padding="max_length",
            max_length=512,  # T5 æ”¯æŒæ›´é•¿åºåˆ—
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            'pixel_values': image_tensor,
            'input_ids_1': text_inputs_1.input_ids.squeeze(),
            'input_ids_2': text_inputs_2.input_ids.squeeze(),
            'prompt': prompt
        }


def train_flux_lora(
    data_dir: str,
    output_dir: str,
    base_model_path: str,
    num_train_epochs: int = 10,
    train_batch_size: int = 2,  # H20 å¯ä»¥æ›´å¤§
    gradient_accumulation_steps: int = 2,
    learning_rate: float = 1e-4,
    lora_rank: int = 32,
    lora_alpha: int = 16,
    save_steps: int = 200,
    resolution: int = 1024,
    use_bf16: bool = True,
):
    """
    è®­ç»ƒ Flux LoRAï¼ˆä½¿ç”¨ diffusers å®˜æ–¹æ–¹æ³•ï¼‰
    
    åŸºäº diffusers å®˜æ–¹ Flux è®­ç»ƒç¤ºä¾‹ï¼Œé€‚é…ä½ çš„æ•°æ®æ ¼å¼
    """
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ Flux LoRAï¼ˆdiffusers å®˜æ–¹æ–¹æ³•ï¼‰")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=gradient_accumulation_steps,
        mixed_precision="bf16" if use_bf16 else "fp16"
    )
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¼˜åŒ–æ˜¾å­˜ï¼‰
    print(f"\nğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    # æ³¨æ„ï¼šä¸æŒ‡å®š variantï¼Œè®©æ¨¡å‹è‡ªåŠ¨ä½¿ç”¨ torch_dtype è½¬æ¢
    pipe = FluxPipeline.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,
        device_map="balanced"
    )
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    if hasattr(pipe.transformer, "enable_gradient_checkpointing"):
        pipe.transformer.enable_gradient_checkpointing()
        print("  âœ… å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    
    # 3. é…ç½® LoRAï¼ˆFlux DiT æ¶æ„ï¼‰
    print(f"\nğŸ”§ é…ç½® LoRA (rank={lora_rank}, alpha={lora_alpha})")
    
    # Flux transformer çš„æ³¨æ„åŠ›å±‚ï¼ˆDiT æ¶æ„ï¼‰
    target_modules = [
        "attn.to_k",
        "attn.to_q",
        "attn.to_v",
        "attn.to_out.0",
    ]
    
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.1,
    )
    
    # 4. åº”ç”¨ LoRA åˆ° transformer
    pipe.transformer = get_peft_model(pipe.transformer, lora_config)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆåœ¨åº”ç”¨ LoRA åï¼‰
    if hasattr(pipe.transformer, "enable_gradient_checkpointing"):
        pipe.transformer.enable_gradient_checkpointing()
        print("  âœ… LoRA æ¨¡å‹å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # 5. å‡†å¤‡æ•°æ®é›†
    print(f"\nğŸ“ å‡†å¤‡è®­ç»ƒæ•°æ®: {data_dir}")
    dataset = HostDataset(
        data_dir=data_dir,
        tokenizer=pipe.tokenizer,  # CLIP tokenizer
        tokenizer_2=pipe.tokenizer_2,  # T5 tokenizer
        size=resolution
    )
    
    if len(dataset) == 0:
        raise ValueError(f"æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼è¯·æ£€æŸ¥ç›®å½•: {data_dir}")
    
    dataloader = DataLoader(
        dataset,
        batch_size=train_batch_size,
        shuffle=True,
        num_workers=2
    )
    
    # 6. è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ 8bit AdamW èŠ‚çœæ˜¾å­˜ï¼‰
    try:
        import bitsandbytes as bnb
        optimizer = bnb.optim.AdamW8bit(
            pipe.transformer.parameters(),
            lr=learning_rate
        )
        print("  â„¹ ä½¿ç”¨ 8bit AdamW ä¼˜åŒ–å™¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    except ImportError:
        optimizer = torch.optim.AdamW(
            pipe.transformer.parameters(),
            lr=learning_rate
        )
        print("  â„¹ ä½¿ç”¨æ ‡å‡† AdamW ä¼˜åŒ–å™¨")
    
    # 7. å‡†å¤‡è®­ç»ƒ
    pipe.transformer, optimizer, dataloader = accelerator.prepare(
        pipe.transformer, optimizer, dataloader
    )
    
    # 8. è®­ç»ƒå¾ªç¯
    num_update_steps_per_epoch = len(dataloader) // gradient_accumulation_steps
    max_train_steps = num_train_epochs * num_update_steps_per_epoch
    
    print(f"\nğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   è®­ç»ƒè½®æ•°: {num_train_epochs}")
    print(f"   æ€»æ­¥æ•°: {max_train_steps}")
    print(f"   æ‰¹æ¬¡å¤§å°: {train_batch_size}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   åˆ†è¾¨ç‡: {resolution}x{resolution}")
    print(f"   ç²¾åº¦: {'bf16' if use_bf16 else 'fp16'}")
    print(f"   GPU: H20 (97GB æ˜¾å­˜)")
    
    global_step = 0
    progress_bar = tqdm(range(max_train_steps), desc="è®­ç»ƒä¸­")
    
    pipe.transformer.train()
    
    for epoch in range(num_train_epochs):
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(pipe.transformer):
                device = next(pipe.transformer.parameters()).device
                dtype = torch.bfloat16 if use_bf16 else torch.float16
                
                # VAE ç¼–ç 
                pixel_values = batch['pixel_values'].to(device, dtype=dtype)
                with torch.no_grad():
                    latents = pipe.vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * pipe.vae.config.scaling_factor
                
                # Flow Matching å™ªå£°æ·»åŠ 
                noise = torch.randn_like(latents, device=device, dtype=dtype)
                timesteps = torch.rand(
                    (latents.shape[0],),
                    device=device,
                    dtype=dtype
                )  # [0, 1]
                
                # Flow Matching: x_t = (1-t)*x_0 + t*x_1
                t = timesteps.view(-1, 1, 1, 1)
                noisy_latents = (1 - t) * latents + t * noise
                
                # ç¼–ç æç¤ºè¯ï¼ˆä½¿ç”¨ pipe.encode_prompt æ–¹æ³•ï¼‰
                prompts = batch['prompt']
                with torch.no_grad():
                    # ä½¿ç”¨ pipe çš„ encode_prompt æ–¹æ³•ï¼ˆè¿”å› prompt_embeds, pooled_prompt_embeds, text_idsï¼‰
                    prompt_embeds, pooled_prompt_embeds, text_ids = pipe.encode_prompt(
                        prompts,
                        num_images_per_prompt=1,
                        device=device
                    )
                    # prompt_embeds å·²ç»æ˜¯ T5 ç¼–ç å™¨çš„è¾“å‡ºï¼ˆåŒ…å«äº† CLIP å’Œ T5 çš„ç»„åˆï¼‰
                    encoder_hidden_states = prompt_embeds
                # æ¸…ç†æ˜¾å­˜
                torch.cuda.empty_cache()
                
                # å‡†å¤‡ latent image IDsï¼ˆFlux éœ€è¦ï¼‰
                latent_image_ids = FluxPipeline._prepare_latent_image_ids(
                    noisy_latents.shape[0],
                    noisy_latents.shape[2] // 2,
                    noisy_latents.shape[3] // 2,
                    device,
                    dtype
                )
                
                # æ‰“åŒ… latentsï¼ˆFlux éœ€è¦ï¼‰
                packed_noisy_latents = FluxPipeline._pack_latents(
                    noisy_latents,
                    batch_size=noisy_latents.shape[0],
                    num_channels_latents=noisy_latents.shape[1],
                    height=noisy_latents.shape[2],
                    width=noisy_latents.shape[3],
                )
                
                # å¤„ç† guidanceï¼ˆå¦‚æœéœ€è¦ï¼‰
                if hasattr(pipe.transformer.config, 'guidance_embeds') and pipe.transformer.config.guidance_embeds:
                    guidance = torch.tensor([3.5], device=device).expand(noisy_latents.shape[0])
                else:
                    guidance = None
                
                # é¢„æµ‹ï¼ˆFlux transformer è°ƒç”¨ï¼‰
                # timestep éœ€è¦é™¤ä»¥ 1000ï¼ˆæ ¹æ®å®˜æ–¹è„šæœ¬ï¼‰
                model_pred = pipe.transformer(
                    hidden_states=packed_noisy_latents,
                    timestep=timesteps / 1000.0,  # Flux éœ€è¦é™¤ä»¥ 1000
                    guidance=guidance,
                    pooled_projections=pooled_prompt_embeds,
                    encoder_hidden_states=encoder_hidden_states,
                    txt_ids=text_ids,
                    img_ids=latent_image_ids,
                    return_dict=False,
                )[0]
                
                # è§£åŒ… latents
                vae_scale_factor = 2 ** (len(pipe.vae.config.block_out_channels) - 1)
                model_pred = FluxPipeline._unpack_latents(
                    model_pred,
                    height=noisy_latents.shape[2] * vae_scale_factor,
                    width=noisy_latents.shape[3] * vae_scale_factor,
                    vae_scale_factor=vae_scale_factor,
                )
                
                # è®¡ç®—æŸå¤±ï¼ˆFlow Matchingï¼šé€Ÿåº¦åœºï¼‰
                target_velocity = noise - latents
                loss = torch.nn.functional.mse_loss(model_pred.float(), target_velocity.float())
                
                # è®°å½•æŸå¤±å€¼ï¼ˆåœ¨æ¸…ç†å‰ï¼‰
                loss_value = loss.item()
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                
                # æ¸…ç†æ˜¾å­˜
                del model_pred, target_velocity, loss
                torch.cuda.empty_cache()
                
                # æ›´æ–°è¿›åº¦æ¡
                if step % 10 == 0:
                    progress_bar.set_postfix({"loss": f"{loss_value:.4f}"})
            
            global_step += 1
            progress_bar.update(1)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if global_step % save_steps == 0:
                if accelerator.is_main_process:
                    checkpoint_dir = Path(output_dir) / f"checkpoint-{global_step}"
                    checkpoint_dir.mkdir(parents=True, exist_ok=True)
                    pipe.transformer.save_pretrained(str(checkpoint_dir))
                    print(f"\nğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_dir}")
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {output_dir}")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    if accelerator.is_main_process:
        pipe.transformer.save_pretrained(str(output_path))
        
        # ä¿å­˜ä¸º safetensors
        try:
            from safetensors.torch import save_file
            state_dict = {}
            for name, param in pipe.transformer.named_parameters():
                if 'lora' in name.lower():
                    state_dict[name] = param.data.cpu()
            if state_dict:
                safetensors_path = output_path / "pytorch_lora_weights.safetensors"
                save_file(state_dict, str(safetensors_path))
                print(f"âœ… å·²ä¿å­˜ safetensors: {safetensors_path}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ safetensors æ—¶å‡ºé”™: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è®­ç»ƒ Flux LoRAï¼ˆdiffusers å®˜æ–¹æ–¹æ³•ï¼‰")
    parser.add_argument("--data-dir", type=str, default="train_data/host_person")
    parser.add_argument("--output-dir", type=str, default="models/lora/host_person")
    parser.add_argument("--base-model", type=str, default="models/flux1-dev")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch-size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ 1ï¼‰")
    parser.add_argument("--gradient-accumulation", type=int, default=4, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶å¢åŠ ï¼‰")
    parser.add_argument("--learning-rate", type=float, default=1e-4)
    parser.add_argument("--lora-rank", type=int, default=32)
    parser.add_argument("--lora-alpha", type=int, default=16)
    parser.add_argument("--save-steps", type=int, default=200)
    parser.add_argument("--resolution", type=int, default=512, help="è®­ç»ƒåˆ†è¾¨ç‡ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ 512ï¼‰")
    parser.add_argument("--use-bf16", action="store_true", default=True, help="ä½¿ç”¨ bf16ï¼ˆH20 æ”¯æŒï¼‰")
    
    args = parser.parse_args()
    
    train_flux_lora(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        base_model_path=args.base_model,
        num_train_epochs=args.epochs,
        train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        save_steps=args.save_steps,
        resolution=args.resolution,
        use_bf16=args.use_bf16,
    )


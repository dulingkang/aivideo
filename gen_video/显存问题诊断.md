# 显存问题诊断报告

## 问题分析

从错误信息看，问题不是代码中的显存泄漏，而是**有其他进程占用了大量显存**：

```
Process 1236130: 63.18 GB 显存占用
Process 2952548: 12.98 GB 显存占用
可用显存: 19.06 GB
720p模型需要: 21.27 GB
```

## 已实施的优化

代码中已添加以下显存管理优化：

1. ✅ **生成前清理显存**：在生成视频前调用 `torch.cuda.empty_cache()` 和 `gc.collect()`
2. ✅ **Tensor转换后立即释放**：将GPU tensor转换为numpy后立即删除并清理显存
3. ✅ **生成后清理**：生成完成后删除中间变量并清理显存
4. ✅ **错误时清理**：发生错误时也会清理显存
5. ✅ **模型卸载功能**：添加了 `unload_model()` 方法，可以在加载新模型前卸载旧模型
6. ✅ **显存监控**：添加了详细的显存状态监控

## 解决方案

### 方案1：释放其他进程的显存（推荐）

检查并释放占用显存的进程：

```bash
# 查看占用显存的进程
nvidia-smi

# 查看进程详情
ps aux | grep <PID>

# 如果这些进程可以停止，先停止它们
kill <PID>
```

### 方案2：使用480p模型（立即可用）

480p模型的显存需求更低（约10-15GB），可以在当前环境下运行：

```yaml
hunyuanvideo:
  model_path: hunyuanvideo-community/HunyuanVideo-1.5-480p_i2v
  width: 640
  height: 480
  num_frames: 60
```

### 方案3：等待其他进程完成

如果其他进程是重要的任务，可以等待它们完成后再运行视频生成。

## 代码改进总结

1. ✅ 添加了 `unload_model()` 方法
2. ✅ 在 `load_model()` 中，如果已有模型，先卸载再加载
3. ✅ 在生成前后都添加了显存清理
4. ✅ 在tensor转换时立即释放GPU显存
5. ✅ 添加了详细的显存状态监控

这些优化确保了代码本身不会造成显存泄漏，但需要足够的可用显存才能运行720p模型。


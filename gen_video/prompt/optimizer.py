"""
Promptä¼˜åŒ–å™¨

è´Ÿè´£æ™ºèƒ½ä¼˜åŒ–Promptï¼ŒåŸºäºè¯­ä¹‰é‡è¦æ€§ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç¡®ä¿ä¸è¶…è¿‡tokené™åˆ¶ã€‚

æ¶æ„å‡çº§ï¼š
- æ”¯æŒæ–°çš„ä¸‰å±‚æ¶æ„ï¼ˆè¯­ä¹‰å±‚ + ç­–ç•¥å±‚ + æ¸²æŸ“å±‚ï¼‰
- ä¿ç•™æ—§çš„å­—ç¬¦ä¸²æ–¹æ³•ä½œä¸ºå…¼å®¹å±‚
"""

from typing import List, Dict, Any, Optional
from .token_estimator import TokenEstimator
from .parser import PromptParser
from .semantic import PromptNode, PromptAST
from .ast_builder import ASTBuilder
from .policy import PolicyEngine
from .enhancer import SemanticEnhancer
from .renderer import PromptRenderer
from .semantic_patterns import get_pattern_registry


class PromptOptimizer:
    """Promptä¼˜åŒ–å™¨"""
    
    def __init__(self, token_estimator: TokenEstimator, parser: PromptParser, ascii_only_prompt: bool = False):
        """
        åˆå§‹åŒ–Promptä¼˜åŒ–å™¨
        
        Args:
            token_estimator: Tokenä¼°ç®—å™¨
            parser: Promptè§£æå™¨
            ascii_only_prompt: æ˜¯å¦åªä½¿ç”¨ASCIIå­—ç¬¦
        """
        self.token_estimator = token_estimator
        self.parser = parser
        self.ascii_only_prompt = ascii_only_prompt
        
        # åˆå§‹åŒ–è¯­ä¹‰æ¨¡å¼æ³¨å†Œè¡¨
        self.pattern_registry = get_pattern_registry()
        
        # åˆå§‹åŒ–ä¸‰å±‚æ¶æ„ç»„ä»¶
        self.ast_builder = ASTBuilder(token_estimator=token_estimator, pattern_registry=self.pattern_registry)
        self.policy_engine = PolicyEngine()
        self.semantic_enhancer = SemanticEnhancer()
        self.renderer = PromptRenderer(token_estimator=token_estimator)
    
    def optimize(
        self, 
        parts: List[str], 
        max_tokens: int = 70,
        model_type: str = "default",
        use_ast: bool = True
    ) -> List[str]:
        """
        æ™ºèƒ½ä¼˜åŒ– promptï¼ŒåŸºäºè¯­ä¹‰é‡è¦æ€§ä¿ç•™å…³é”®ä¿¡æ¯
        
        Args:
            parts: prompt éƒ¨åˆ†åˆ—è¡¨
            max_tokens: æœ€å¤§ token æ•°
            model_type: æ¨¡å‹ç±»å‹ï¼ˆinstantid, flux, hunyuanvideo, sdxlï¼‰
            use_ast: æ˜¯å¦ä½¿ç”¨æ–°çš„ AST æ¶æ„ï¼ˆé»˜è®¤ Trueï¼‰
        
        Returns:
            ä¼˜åŒ–åçš„ prompt éƒ¨åˆ†åˆ—è¡¨
        """
        if not parts:
            return []
        
        # âš¡ æ–°æ¶æ„ï¼šä½¿ç”¨ AST + ç­–ç•¥ + æ¸²æŸ“
        if use_ast:
            return self._optimize_with_ast(parts, max_tokens, model_type)
        
        # æ—§æ¶æ„ï¼šä¿æŒå‘åå…¼å®¹
        return self._optimize_legacy(parts, max_tokens)
    
    def _optimize_with_ast(
        self, 
        parts: List[str], 
        max_tokens: int,
        model_type: str
    ) -> List[str]:
        """
        ä½¿ç”¨ AST æ¶æ„ä¼˜åŒ–ï¼ˆæ–°æ–¹æ³•ï¼‰
        
        æµç¨‹ï¼š
        1. å­—ç¬¦ä¸² â†’ ASTï¼ˆè¯­ä¹‰å±‚ï¼‰
        2. è¯­ä¹‰å¢å¼ºï¼ˆè¯­ä¹‰å±‚ï¼‰
        3. ç­–ç•¥åº”ç”¨ï¼ˆç­–ç•¥å±‚ï¼‰
        4. AST â†’ å­—ç¬¦ä¸²ï¼ˆæ¸²æŸ“å±‚ï¼‰
        """
        # 1. è§£æä¸º AST
        ast = self.ast_builder.parse_parts(parts)
        
        # 2. è¯­ä¹‰å¢å¼º
        ast = self.semantic_enhancer.enhance_ast(ast)
        
        # 3. åº”ç”¨ç­–ç•¥ï¼ˆæ¨¡å‹æ„ŸçŸ¥ï¼‰
        ast = self.policy_engine.apply_policy(ast, model_type)
        
        # 4. æ¸²æŸ“ä¸ºå­—ç¬¦ä¸²
        final_prompt = self.renderer.render(ast, max_tokens)
        
        # 5. åˆ†å‰²ä¸º partsï¼ˆä¿æŒæ¥å£å…¼å®¹ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥è¿”å›å•ä¸ªå­—ç¬¦ä¸²æˆ–partsåˆ—è¡¨
        # ä¸ºäº†å…¼å®¹ï¼Œæˆ‘ä»¬åˆ†å‰²ä¸ºparts
        return [p.strip() for p in final_prompt.split(",") if p.strip()]
    
    def _optimize_legacy(self, parts: List[str], max_tokens: int) -> List[str]:
        """
        æ—§æ¶æ„ä¼˜åŒ–æ–¹æ³•ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        """
        # 1. åˆ†ææ¯ä¸ªéƒ¨åˆ†çš„é‡è¦æ€§
        analyzed_parts = []
        for i, part in enumerate(parts):
            part_type = self._infer_part_type(part)
            analysis = self._analyze_importance(part, part_type)
            analyzed_parts.append({
                "text": part,
                "type": part_type,
                "analysis": analysis,
                "index": i
            })
        
        # 2. æŒ‰é‡è¦æ€§æ’åº
        analyzed_parts.sort(key=lambda x: x["analysis"]["importance"], reverse=True)
        
        # 3. æ™ºèƒ½é€‰æ‹©ï¼šä¼˜å…ˆä¿ç•™é«˜é‡è¦æ€§ä¸” token æ•ˆç‡é«˜çš„éƒ¨åˆ†
        selected_parts = self._select_parts(analyzed_parts, max_tokens)
        
        # 4. å»é‡ï¼šç§»é™¤é‡å¤å’Œè¯­ä¹‰ç›¸è¿‘çš„æè¿°
        selected_parts = self._remove_duplicate_and_similar(selected_parts)
        
        # 4.5. å¤„ç†å•ä¸ª part å†…éƒ¨çš„é‡å¤è¯æ±‡
        for part in selected_parts:
            part["text"] = self._remove_internal_duplicates(part["text"])
        
        # 5. æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åºï¼ˆä¿æŒé€»è¾‘é¡ºåºï¼‰
        selected_parts.sort(key=lambda x: x["index"])
        
        # 6. è¿”å›ä¼˜åŒ–åçš„æ–‡æœ¬åˆ—è¡¨
        optimized_parts = [p["text"] for p in selected_parts]
        
        # æ‰“å°ä¼˜åŒ–ä¿¡æ¯
        if len(optimized_parts) < len(parts):
            current_tokens = self.token_estimator.estimate(", ".join(optimized_parts))
            print(f"  ğŸ§  æ™ºèƒ½ä¼˜åŒ–: ä» {len(parts)} ä¸ªéƒ¨åˆ†ç²¾ç®€è‡³ {len(optimized_parts)} ä¸ªéƒ¨åˆ†")
            print(f"  ğŸ“Š Token ä½¿ç”¨: {current_tokens}/{max_tokens} tokens")
            for p in selected_parts:
                print(f"    - [{p['type']}] {p['text'][:50]}... (é‡è¦æ€§: {p['analysis']['importance']:.1f}, {p['analysis']['token_count']} tokens)")
        
        # æ£€æŸ¥è§’è‰²æè¿°æ˜¯å¦è¢«ä¿ç•™
        character_parts_in_result = [p for p in selected_parts if p['type'] == 'character']
        if character_parts_in_result:
            print(f"  âœ“ è§’è‰²æè¿°å·²ä¿ç•™: {len(character_parts_in_result)} ä¸ªéƒ¨åˆ†")
            for cp in character_parts_in_result:
                print(f"    - {cp['text'][:80]}...")
        else:
            # æ£€æŸ¥åŸå§‹partsä¸­æ˜¯å¦æœ‰è§’è‰²æè¿°
            original_character_parts = [p for p in analyzed_parts if p['type'] == 'character']
            if original_character_parts:
                print(f"  âš  è­¦å‘Š: åŸå§‹promptä¸­æœ‰ {len(original_character_parts)} ä¸ªè§’è‰²æè¿°éƒ¨åˆ†ï¼Œä½†ä¼˜åŒ–åè¢«ç§»é™¤äº†ï¼")
                for cp in original_character_parts:
                    print(f"    - è¢«ç§»é™¤çš„è§’è‰²æè¿°: {cp['text'][:80]}...")
        
        return optimized_parts
    
    def _infer_part_type(self, part: str) -> str:
        """
        æ¨æ–­promptéƒ¨åˆ†çš„ç±»å‹ï¼ˆä½¿ç”¨è¯­ä¹‰æ¨¡å¼æ³¨å†Œè¡¨ï¼‰
        
        âš¡ ä¸å†ç¡¬ç¼–ç è¯è¯­ï¼Œè€Œæ˜¯ä½¿ç”¨å¯é…ç½®çš„è¯­ä¹‰æ¨¡å¼
        
        Args:
            part: promptéƒ¨åˆ†å­—ç¬¦ä¸²
            
        Returns:
            æ¨æ–­å‡ºçš„ç±»å‹
        """
        # æå–çº¯å†…å®¹ï¼ˆç§»é™¤æƒé‡æ ‡è®°ï¼‰
        import re
        content = re.sub(r'^\(|\)$', '', part)
        content = re.sub(r':\d+\.?\d*\)?$', '', content).strip()
        
        # ä½¿ç”¨è¯­ä¹‰æ¨¡å¼æ³¨å†Œè¡¨è¿›è¡Œç±»å‹æ¨æ–­
        return self.pattern_registry.infer_type(content)
    
    def _analyze_importance(self, part: str, part_type: str) -> Dict[str, Any]:
        """
        åˆ†æpromptéƒ¨åˆ†çš„é‡è¦æ€§
        
        å…³é”®ä¿¡æ¯ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
        1. compositionï¼ˆæ„å›¾æè¿°ï¼ŒåŒ…å«åŠ¨ä½œå’Œåœºæ™¯ä¸»ä½“ï¼‰
        2. fxï¼ˆç‰¹æ•ˆï¼ŒåŒ…å«èƒ½é‡ã€å…‰æ•ˆç­‰å…³é”®è§†è§‰å…ƒç´ ï¼‰
        3. environmentï¼ˆç¯å¢ƒæè¿°ï¼ŒåŒ…å«èƒŒæ™¯å’Œæ°›å›´ï¼‰
        4. characterï¼ˆè§’è‰²æè¿°ï¼ŒåŒ…å«å¤–è§‚ç‰¹å¾ï¼‰
        5. actionï¼ˆåŠ¨ä½œæè¿°ï¼‰
        6. cameraï¼ˆé•œå¤´æè¿°ï¼Œä½†é¿å…é‡å¤ï¼‰
        7. styleï¼ˆé£æ ¼æè¿°ï¼‰
        8. backgroundï¼ˆèƒŒæ™¯ä¸€è‡´æ€§ï¼Œæ¬¡è¦ï¼‰
        
        Args:
            part: promptéƒ¨åˆ†æ–‡æœ¬
            part_type: éƒ¨åˆ†ç±»å‹
            
        Returns:
            åŒ…å«é‡è¦æ€§ã€tokenæ•°é‡ã€tokenæ•ˆç‡ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        token_count = self.token_estimator.estimate(part)
        
        # æ ¹æ®ç±»å‹è®¾ç½®åŸºç¡€é‡è¦æ€§
        # ç”¨æˆ·åé¦ˆï¼šåœºæ™¯è¡¨ç°ä¸å¤Ÿå¥½ï¼Œcompositionã€fxã€environmentæ˜¯å…³é”®ä¿¡æ¯ï¼Œå¿…é¡»ä¿ç•™
        # çº¦æŸæ¡ä»¶ï¼ˆå¦‚å•äººçº¦æŸï¼‰å…·æœ‰æœ€é«˜ä¼˜å…ˆçº§ï¼Œå¿…é¡»ä¿ç•™
        base_importance = {
            "constraint": 20.0,  # çº¦æŸæ¡ä»¶ï¼ˆå¦‚å•äººçº¦æŸï¼‰æœ€é«˜ä¼˜å…ˆçº§ï¼Œå¿…é¡»ä¿ç•™
            "composition": 12.0,  # æ„å›¾æè¿°æœ€é‡è¦ï¼ŒåŒ…å«åŠ¨ä½œå’Œåœºæ™¯ä¸»ä½“
            "fx": 11.0,  # ç‰¹æ•ˆæ¬¡é‡è¦ï¼ŒåŒ…å«èƒ½é‡ã€å…‰æ•ˆç­‰å…³é”®è§†è§‰å…ƒç´ 
            "environment": 10.0,  # ç¯å¢ƒæè¿°å¾ˆé‡è¦ï¼ŒåŒ…å«èƒŒæ™¯å’Œæ°›å›´
            "style": 12.0,  # é£æ ¼æè¿°ï¼ˆæé«˜ä¼˜å…ˆçº§ï¼Œç¡®ä¿xianxiaé£æ ¼ä¸è¢«ç§»é™¤ï¼‰
            "character": 15.0,  # è§’è‰²æè¿°ï¼ˆæé«˜ä¼˜å…ˆçº§ï¼Œç¡®ä¿æ€§åˆ«ã€æœé¥°ã€ä¿®ä»™æ°”è´¨ä¸è¢«ç§»é™¤ï¼‰
            "action": 7.5,  # åŠ¨ä½œæè¿°
            "camera": 6.5,  # é•œå¤´æè¿°ï¼ˆä½†é¿å…é‡å¤ï¼‰
            "background": 4.0,  # èƒŒæ™¯ä¸€è‡´æ€§ï¼Œæ¬¡è¦
            "scene": 7.0,  # åœºæ™¯æè¿°ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
            "other": 3.0
        }.get(part_type, 3.0)
        
        # æ ¹æ®æƒé‡æ ‡è®°è°ƒæ•´é‡è¦æ€§
        import re
        weight_match = re.search(r':(\d+\.?\d*)', part)
        if weight_match:
            weight = float(weight_match.group(1))
            # æƒé‡è¶Šé«˜ï¼Œé‡è¦æ€§è¶Šé«˜ï¼ˆä½†ä¸è¶…è¿‡åŸºç¡€é‡è¦æ€§çš„1.5å€ï¼‰
            importance = base_importance * min(1.0 + (weight - 1.0) * 0.1, 1.5)
        else:
            importance = base_importance
        
        # è®¡ç®—tokenæ•ˆç‡ï¼ˆé‡è¦æ€§/ tokenæ•°ï¼‰
        token_efficiency = importance / max(token_count, 1)
        
        return {
            "importance": importance,
            "token_count": token_count,
            "token_efficiency": token_efficiency
        }
    
    def _select_parts(self, analyzed_parts: List[Dict], max_tokens: int) -> List[Dict]:
        """é€‰æ‹©è¦ä¿ç•™çš„promptéƒ¨åˆ†"""
        selected_parts = []
        current_tokens = 0
        
        # å¿…é¡»ä¿ç•™çš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆcompositionã€fxã€environmentæ˜¯å…³é”®åœºæ™¯ä¿¡æ¯ï¼Œå¿…é¡»ä¿ç•™ï¼‰
        # ç”¨æˆ·åé¦ˆï¼šåœºæ™¯è¡¨ç°ä¸å¤Ÿå¥½ï¼Œcompositionã€fxã€environmentæ˜¯å…³é”®ä¿¡æ¯
        # çº¦æŸæ¡ä»¶ï¼ˆå¦‚å•äººçº¦æŸï¼‰å¿…é¡»ä¿ç•™ï¼Œå…·æœ‰æœ€é«˜ä¼˜å…ˆçº§
        # é‡è¦ï¼šè§’è‰²æè¿°ï¼ˆcharacterï¼‰å¿…é¡»ä¿ç•™ï¼Œç¡®ä¿æ€§åˆ«ã€æœé¥°ã€ä¿®ä»™æ°”è´¨ä¸è¢«ç§»é™¤
        # é‡è¦ï¼šåŒºåˆ†çœŸæ­£çš„compositionæè¿°ï¼ˆåŒ…å«åŠ¨ä½œå’Œåœºæ™¯ï¼‰å’Œç®€å•çš„æ ‡è®°ï¼ˆå¦‚maleï¼‰
        must_keep_types = ["constraint", "character", "composition", "fx", "environment", "style", "action"]
        must_keep_parts = [p for p in analyzed_parts if p["type"] in must_keep_types]
        
        # ç‰¹åˆ«å¤„ç†ï¼šç¡®ä¿åŒ…å«æœé¥°å’Œä¿®ä»™æ°”è´¨çš„è§’è‰²æè¿°è¢«ä¼˜å…ˆä¿ç•™
        character_parts = [p for p in must_keep_parts if p["type"] == "character"]
        for char_part in character_parts:
            text_lower = char_part["text"].lower()
            # å¦‚æœåŒ…å«æœé¥°æˆ–ä¿®ä»™æ°”è´¨å…³é”®è¯ï¼Œæé«˜é‡è¦æ€§
            if any(kw in text_lower for kw in ["robe", "cultivator", "é“è¢", "ä¿®ä»™", "xianxia", "æœé¥°", "clothes", "hair", "é•¿å‘"]):
                char_part["analysis"]["importance"] = max(char_part["analysis"]["importance"], 18.0)  # æé«˜åˆ°18.0ï¼Œç¡®ä¿ä¸è¢«ç§»é™¤
        
        # åˆ†ç¦»çœŸæ­£çš„compositionæè¿°å’Œç®€å•çš„æ ‡è®°
        constraint_parts = [p for p in must_keep_parts if p["type"] == "constraint"]
        composition_parts = [p for p in must_keep_parts if p["type"] == "composition"]
        
        # æ£€æŸ¥compositionéƒ¨åˆ†ï¼ŒåŒºåˆ†çœŸæ­£çš„æè¿°å’Œç®€å•æ ‡è®°
        real_composition_parts = []
        simple_marker_parts = []
        for p in composition_parts:
            text_lower = p["text"].lower()
            # çœŸæ­£çš„compositionæè¿°é€šå¸¸è¾ƒé•¿ï¼ŒåŒ…å«åŠ¨ä½œæˆ–åœºæ™¯æè¿°
            is_real_composition = (
                len(p["text"].split()) > 3 or  # é•¿åº¦è¶…è¿‡3ä¸ªè¯
                any(kw in text_lower for kw in ["lying", "sees", "strains", "recalls", "uses", "on", "above", "revealing", "han li", "éŸ©ç«‹"]) or
                "han li" in text_lower or "éŸ©ç«‹" in p["text"]
            )
            if is_real_composition:
                real_composition_parts.append(p)
            else:
                # ç®€å•æ ‡è®°ï¼ˆå¦‚(male:1.8)ï¼‰ä¼˜å…ˆçº§è¾ƒä½
                simple_marker_parts.append(p)
        
        # å…¶ä»–å¿…é¡»ä¿ç•™çš„éƒ¨åˆ†
        other_must_keep = [p for p in must_keep_parts if p["type"] not in ["constraint", "composition"]]
        
        # ç‰¹åˆ«å¤„ç†ï¼šè§’è‰²æè¿°å¿…é¡»ä¼˜å…ˆä¿ç•™ï¼ˆå³ä½¿tokenè¶…é™ä¹Ÿè¦ä¿ç•™ï¼‰
        character_parts_sorted = sorted([p for p in other_must_keep if p["type"] == "character"], key=lambda x: x["index"])
        other_must_keep_no_char = [p for p in other_must_keep if p["type"] != "character"]
        
        # æ’åºï¼šçº¦æŸæ¡ä»¶ > è§’è‰²æè¿°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰> çœŸæ­£çš„compositionæè¿° > å…¶ä»– > ç®€å•æ ‡è®°
        must_keep_parts = constraint_parts + character_parts_sorted + \
                          sorted(real_composition_parts, key=lambda x: x["index"]) + \
                          sorted(other_must_keep_no_char, key=lambda x: x["index"]) + \
                          sorted(simple_marker_parts, key=lambda x: x["index"])
        
        # å¿…é¡»ä¿ç•™çš„éƒ¨åˆ†ï¼Œå³ä½¿è¶…è¿‡é™åˆ¶ä¹Ÿè¦ä¿ç•™ï¼ˆä½†å¯ä»¥ç²¾ç®€ï¼‰
        # é‡è¦ï¼šcompositionæè¿°åŒ…å«å…³é”®åŠ¨ä½œï¼ˆå¦‚lying, sees, strainsç­‰ï¼‰ï¼Œå¿…é¡»ä¿ç•™ï¼Œä¸èƒ½ç²¾ç®€
        for part_info in must_keep_parts:
            test_parts = [p["text"] for p in selected_parts] + [part_info["text"]]
            test_prompt = ", ".join(test_parts)
            actual_tokens = self.token_estimator.estimate(test_prompt)
            
            if actual_tokens <= max_tokens:
                selected_parts.append(part_info)
                current_tokens = actual_tokens
            else:
                # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œå°è¯•ç²¾ç®€ï¼ˆä½†å¿…é¡»ä¿ç•™ï¼‰
                # é‡è¦ï¼šcompositionç±»å‹ä¸”åŒ…å«å…³é”®åŠ¨ä½œçš„ï¼Œä¸èƒ½ç²¾ç®€ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™
                part_text_lower = part_info["text"].lower()
                has_key_action = any(kw in part_text_lower for kw in [
                    "lying", "sees", "strains", "tilts", "recalls", "uses", "performing",
                    "èºº", "çœ‹è§", "è½¬å¤´", "å›å¿†", "ä½¿ç”¨", "æ–½å±•"
                ])
                
                # è§’è‰²æè¿°å¿…é¡»å¼ºåˆ¶ä¿ç•™ï¼ˆåŒ…å«æœé¥°ã€å‘å‹ã€ä¿®ä»™æ°”è´¨ç­‰å…³é”®ä¿¡æ¯ï¼‰
                if part_info["type"] == "character":
                    part_text_lower = part_info["text"].lower()
                    has_clothing_keywords = any(kw in part_text_lower for kw in [
                        "robe", "cultivator", "é“è¢", "ä¿®ä»™", "xianxia", "æœé¥°", "clothes", 
                        "hair", "é•¿å‘", "deep cyan", "dark green", "immortal"
                    ])
                    if has_clothing_keywords:
                        # åŒ…å«æœé¥°å’Œä¿®ä»™æ°”è´¨çš„è§’è‰²æè¿°ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™ï¼Œå³ä½¿è¶…è¿‡é™åˆ¶
                        print(f"  âš  æ£€æµ‹åˆ°è§’è‰²æè¿°ï¼ˆåŒ…å«æœé¥°/ä¿®ä»™æ°”è´¨ï¼Œ{part_info['text'][:50]}...ï¼‰ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™ï¼Œå³ä½¿è¶…è¿‡tokené™åˆ¶")
                        selected_parts.append(part_info)
                        current_tokens = actual_tokens
                    else:
                        # å…¶ä»–è§’è‰²æè¿°ä¹Ÿå¯ä»¥ç²¾ç®€ï¼Œä½†å¿…é¡»ä¿ç•™
                        compact_part = self._compact_part(part_info)
                        test_parts = [p["text"] for p in selected_parts] + [compact_part]
                        test_prompt = ", ".join(test_parts)
                        compact_tokens = self.token_estimator.estimate(test_prompt)
                        
                        if compact_tokens <= max_tokens:
                            part_info["text"] = compact_part
                            part_info["analysis"]["token_count"] = compact_tokens
                            selected_parts.append(part_info)
                            current_tokens = compact_tokens
                        else:
                            # å³ä½¿ç²¾ç®€åè¿˜æ˜¯è¶…é™ï¼Œä¹Ÿè¦å¼ºåˆ¶ä¿ç•™ï¼ˆè§’è‰²æè¿°å¤ªé‡è¦ï¼‰
                            print(f"  âš  è§’è‰²æè¿°ç²¾ç®€åä»è¶…é™ï¼Œä½†å¿…é¡»å¼ºåˆ¶ä¿ç•™: {part_info['text'][:50]}...")
                            selected_parts.append(part_info)
                            current_tokens = actual_tokens
                elif part_info["type"] == "composition" and has_key_action:
                    # åŒ…å«å…³é”®åŠ¨ä½œçš„compositionæè¿°ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™ï¼Œå³ä½¿è¶…è¿‡é™åˆ¶
                    # ä¼˜å…ˆç§»é™¤å…¶ä»–ä½é‡è¦æ€§éƒ¨åˆ†ï¼Œä¸ºå…³é”®compositionæè¿°è…¾å‡ºç©ºé—´
                    print(f"  âš  æ£€æµ‹åˆ°å…³é”®åŠ¨ä½œæè¿°ï¼ˆ{part_info['text'][:50]}...ï¼‰ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™ï¼Œå³ä½¿è¶…è¿‡tokené™åˆ¶")
                    selected_parts.append(part_info)
                    current_tokens = actual_tokens
                else:
                    # å…¶ä»–ç±»å‹å¯ä»¥ç²¾ç®€
                    compact_part = self._compact_part(part_info)
                    test_parts = [p["text"] for p in selected_parts] + [compact_part]
                    test_prompt = ", ".join(test_parts)
                    compact_tokens = self.token_estimator.estimate(test_prompt)
                    
                    if compact_tokens <= max_tokens:
                        part_info["text"] = compact_part
                        part_info["analysis"]["token_count"] = compact_tokens
                        selected_parts.append(part_info)
                        current_tokens = compact_tokens
        
        # æ·»åŠ å…¶ä»–é«˜é‡è¦æ€§éƒ¨åˆ†ï¼ˆåœºæ™¯ã€é•œå¤´ç­‰ï¼‰
        for part_info in analyzed_parts:
            if part_info in selected_parts:
                continue
            
            test_parts = [p["text"] for p in selected_parts] + [part_info["text"]]
            test_prompt = ", ".join(test_parts)
            actual_tokens = self.token_estimator.estimate(test_prompt)
            
            # å¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œæ·»åŠ é«˜é‡è¦æ€§æˆ–é«˜ token æ•ˆç‡çš„éƒ¨åˆ†
            # ç”¨æˆ·åé¦ˆï¼šéœ€è¦å……åˆ†è¡¨è¾¾æ„å›¾ï¼Œé™ä½é‡è¦æ€§é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯
            if actual_tokens <= max_tokens:
                if (part_info["analysis"]["importance"] >= 6.0 or  # ä»7.0é™ä½åˆ°6.0ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯
                    part_info["analysis"]["token_efficiency"] >= 0.4):  # ä»0.5é™ä½åˆ°0.4ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯
                    selected_parts.append(part_info)
                    current_tokens = actual_tokens
            else:
                # å¦‚æœç©ºé—´ä¸è¶³ï¼Œå°è¯•ç²¾ç®€è¿™ä¸ªéƒ¨åˆ†
                # ç”¨æˆ·åé¦ˆï¼šè§’è‰²å’Œé•œå¤´æè¿°å¾ˆé‡è¦ï¼Œé™ä½ç²¾ç®€é˜ˆå€¼
                if part_info["analysis"]["importance"] >= 7.0:  # ä»8.0é™ä½åˆ°7.0ï¼Œä¿ç•™æ›´å¤šé‡è¦ä¿¡æ¯
                    compact_part = self.parser.extract_first_keyword(part_info["text"])
                    compact_tokens = self.token_estimator.estimate(compact_part)
                    if current_tokens + compact_tokens <= max_tokens:
                        part_info["text"] = compact_part
                        part_info["analysis"]["token_count"] = compact_tokens
                        selected_parts.append(part_info)
                        current_tokens += compact_tokens
        
        return selected_parts
    
    def _compact_part(self, part_info: Dict) -> str:
        """ç²¾ç®€promptéƒ¨åˆ†"""
        part_type = part_info["type"]
        text = part_info["text"]
        
        if part_type == "style":
            # é£æ ¼æè¿°ï¼šxianxiaé£æ ¼å¿…é¡»ä¿ç•™
            text_lower = text.lower()
            if "xianxia" in text_lower or "ä»™ä¾ " in text or "chinese fantasy" in text_lower:
                # xianxiaé£æ ¼æ˜¯æ ¸å¿ƒï¼Œå¿…é¡»ä¿ç•™ï¼Œä¸ç²¾ç®€
                return text
            # é£æ ¼æè¿°ï¼šè‡³å°‘ä¿ç•™"ä»™ä¾ é£æ ¼"
            if "ä»™ä¾ é£æ ¼" in text and "å¤é£" in text:
                if "ä¿®ä»™" in text:
                    text = text.replace("ï¼Œä¿®ä»™", "").replace(", ä¿®ä»™", "").replace("ä¿®ä»™", "")
            else:
                text = "ä»™ä¾ é£æ ¼" if not self.ascii_only_prompt else "xianxia fantasy"
        elif part_type == "action":
            # åŠ¨ä½œæè¿°ï¼šç²¾ç®€ä½†ä¿ç•™æ ¸å¿ƒåŠ¨ä½œä¿¡æ¯
            import re
            if "èºº" in text or "lying" in text.lower():
                if "éŸ©ç«‹" in text:
                    text = "(éŸ©ç«‹èººåœ¨æ²™åœ°ä¸Š:1.6)" if not text.startswith("(") else text.split(":")[0] + ":1.6)"
                else:
                    text = "(èººåœ¨æ²™åœ°ä¸Š:1.6)"
            else:
                # å…¶ä»–åŠ¨ä½œï¼Œæå–å‰20ä¸ªå­—ç¬¦çš„æ ¸å¿ƒæè¿°
                if len(text) > 30:
                    if text.startswith("(") and ":" in text:
                        content = text.split(":")[0].strip("()")
                        weight = text.split(":")[1].strip("()")
                        compact_content = content[:20] + "..."
                        text = f"({compact_content}:{weight})"
                    else:
                        text = text[:20] + "..."
        else:
            # å…¶ä»–ç±»å‹ï¼Œæå–ç¬¬ä¸€ä¸ªå…³é”®è¯
            text = self.parser.extract_first_keyword(text)
        
        return text
    
    def _remove_duplicate_facing_camera(self, parts: List[Dict]) -> List[Dict]:
        """ç§»é™¤é‡å¤çš„facing cameraæè¿°ï¼Œåªä¿ç•™æƒé‡æœ€é«˜çš„ä¸€ä¸ª"""
        facing_camera_parts = []
        other_parts = []
        
        for part in parts:
            part_lower = part["text"].lower()
            # æ£€æµ‹æ˜¯å¦æ˜¯facing cameraç›¸å…³çš„æè¿°
            if any(kw in part_lower for kw in [
                "facing camera", "front view", "face forward", "character facing viewer", 
                "frontal view", "é¢å‘é•œå¤´", "æ­£é¢è§†è§’", "äººç‰©é¢å‘è§‚ä¼—"
            ]):
                facing_camera_parts.append(part)
            else:
                other_parts.append(part)
        
        # å¦‚æœæœ‰å¤šä¸ªfacing cameraæè¿°ï¼Œåªä¿ç•™æƒé‡æœ€é«˜çš„ä¸€ä¸ª
        if len(facing_camera_parts) > 1:
            # æå–æƒé‡å¹¶æ’åº
            import re
            for part in facing_camera_parts:
                weight_match = re.search(r':(\d+\.?\d*)', part["text"])
                if weight_match:
                    part["_weight"] = float(weight_match.group(1))
                else:
                    part["_weight"] = 1.0
            
            # æŒ‰æƒé‡é™åºæ’åºï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆæƒé‡æœ€é«˜çš„ï¼‰
            facing_camera_parts.sort(key=lambda x: x.get("_weight", 1.0), reverse=True)
            kept_part = facing_camera_parts[0]
            if len(facing_camera_parts) > 1:
                removed_count = len(facing_camera_parts) - 1
                print(f"  âœ“ ç§»é™¤ {removed_count} ä¸ªé‡å¤çš„ 'facing camera' æè¿°ï¼Œä¿ç•™æƒé‡æœ€é«˜çš„ ({kept_part.get('_weight', 1.0):.1f})")
            return [kept_part] + other_parts
        
        return parts

    def _remove_duplicate_and_similar(self, parts: List[Dict]) -> List[Dict]:
        """
        é€šç”¨çš„å»é‡å‡½æ•°ï¼šç§»é™¤é‡å¤å’Œè¯­ä¹‰ç›¸è¿‘çš„æè¿°
        
        æ£€æµ‹å¹¶åˆå¹¶ï¼š
        1. å®Œå…¨é‡å¤çš„æè¿°
        2. è¯­ä¹‰ç›¸è¿‘çš„æè¿°ï¼ˆå¦‚åŒä¹‰è¯ã€è¿‘ä¹‰è¯ï¼‰
        3. åŒ…å«ç›¸åŒæ ¸å¿ƒæ¦‚å¿µçš„æè¿°
        """
        import re
        
        # å®šä¹‰è¯­ä¹‰ç›¸è¿‘çš„è¯æ±‡ç»„ï¼ˆåŒä¹‰è¯ç»„ï¼‰
        similar_groups = [
            # å•äººç›¸å…³
            {
                "keywords": ["single person", "lone figure", "only one character", "one person only", 
                           "sole character", "single individual", "å•äºº", "ç‹¬è¡Œ", "åªæœ‰ä¸€ä¸ªè§’è‰²", 
                           "ä»…ä¸€äºº", "å”¯ä¸€è§’è‰²", "å•ç‹¬ä¸ªä½“"],
                "merged": "single person, only one character",
                "type": "constraint"
            },
            # æ­£é¢æœå‘ç›¸å…³
            {
                "keywords": ["facing camera", "front view", "face forward", "character facing viewer", 
                           "frontal view", "é¢å‘é•œå¤´", "æ­£é¢è§†è§’", "äººç‰©é¢å‘è§‚ä¼—", "æ­£é¢", "é¢å‘"],
                "merged": "facing camera, front view",
                "type": "camera"
            },
            # ä»™ä¾ é£æ ¼ç›¸å…³
            {
                "keywords": ["xianxia", "chinese fantasy", "ä»™ä¾ ", "ä¿®ä»™", "å¤é£", "immortal", "cultivator"],
                "merged": "xianxia fantasy",
                "type": "style"
            },
            # è¿œæ™¯/å…¨èº«ç›¸å…³
            {
                "keywords": ["wide shot", "full body", "full figure", "å…¨èº«", "è¿œæ™¯", "wide view", "full view"],
                "merged": "wide shot, full body",
                "type": "camera"
            },
            # ä¸­æ™¯/åŠèº«ç›¸å…³
            {
                "keywords": ["medium shot", "mid shot", "upper body", "half body", "ä¸­æ™¯", "åŠèº«", "ä¸ŠåŠèº«"],
                "merged": "medium shot, upper body",
                "type": "camera"
            },
            # ç‰¹å†™ç›¸å…³
            {
                "keywords": ["close-up", "closeup", "close up", "face close-up", "portrait shot", "headshot", 
                           "ç‰¹å†™", "è¿‘æ™¯", "é¢éƒ¨ç‰¹å†™"],
                "merged": "close-up, face close-up",
                "type": "camera"
            }
        ]
        
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†è¯­ä¹‰ç›¸è¿‘çš„ç»„
        processed_parts = []
        used_indices = set()
        
        for group in similar_groups:
            matching_parts = []
            for i, part in enumerate(parts):
                if i in used_indices:
                    continue
                part_lower = part["text"].lower()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¥ç»„çš„ä»»ä½•å…³é”®è¯
                if any(kw in part_lower for kw in group["keywords"]):
                    matching_parts.append((i, part))
            
            # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…çš„éƒ¨åˆ†ï¼Œåˆå¹¶å®ƒä»¬
            if len(matching_parts) > 1:
                # æå–æœ€é«˜æƒé‡
                max_weight = 1.0
                min_index = float('inf')
                for idx, part in matching_parts:
                    weight_match = re.search(r':(\d+\.?\d*)', part["text"])
                    if weight_match:
                        weight = float(weight_match.group(1))
                        if weight > max_weight:
                            max_weight = weight
                    if part["index"] < min_index:
                        min_index = part["index"]
                
                # åˆ›å»ºåˆå¹¶åçš„æè¿°
                merged_text = f"({group['merged']}:{max_weight:.1f})"
                
                # åˆ›å»ºæ–°çš„part
                merged_part = {
                    "text": merged_text,
                    "type": group["type"],
                    "analysis": matching_parts[0][1]["analysis"].copy(),
                    "index": min_index
                }
                merged_part["analysis"]["token_count"] = self.token_estimator.estimate(merged_text)
                
                processed_parts.append(merged_part)
                # æ ‡è®°è¿™äº›éƒ¨åˆ†å·²ä½¿ç”¨
                for idx, _ in matching_parts:
                    used_indices.add(idx)
                
                removed_count = len(matching_parts) - 1
                print(f"  âœ“ åˆå¹¶ {len(matching_parts)} ä¸ªè¯­ä¹‰ç›¸è¿‘çš„æè¿°ï¼ˆ{group['merged']}ï¼‰ï¼Œç§»é™¤ {removed_count} ä¸ªé‡å¤é¡¹")
            elif len(matching_parts) == 1:
                # åªæœ‰ä¸€ä¸ªåŒ¹é…ï¼Œç›´æ¥ä¿ç•™
                idx, part = matching_parts[0]
                processed_parts.append(part)
                used_indices.add(idx)
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†å…¶ä»–éƒ¨åˆ†ï¼ˆæœªåŒ¹é…åˆ°ä»»ä½•ç»„çš„ï¼‰
        for i, part in enumerate(parts):
            if i not in used_indices:
                processed_parts.append(part)
        
        # ç¬¬äºŒæ­¥åŠï¼šæ£€æµ‹é‡å¤çš„å…³é”®è¯ï¼ˆå¦‚"scroll"ã€"Three dazzling suns"ï¼‰
        # å…ˆæ£€æµ‹å¹¶åˆå¹¶åŒ…å«ç›¸åŒæ ¸å¿ƒå…³é”®è¯çš„éƒ¨åˆ†
        keyword_groups = {}  # å…³é”®è¯ -> [parts]
        for part in processed_parts:
            text_lower = part["text"].lower()
            # æå–æ ¸å¿ƒå…³é”®è¯ï¼ˆåè¯ã€ä¸»è¦ç‰©ä½“ç­‰ï¼‰
            words = re.findall(r'\b[a-z]+(?:\s+[a-z]+)*\b', text_lower)
            if words:
                # å–å‰2-3ä¸ªè¯ä½œä¸ºæ ¸å¿ƒå…³é”®è¯ï¼ˆå¦‚"golden scroll", "three dazzling suns"ï¼‰
                core_keyword = ' '.join(words[:3]) if len(words) >= 3 else words[0]
                # ä¹Ÿæ£€æŸ¥å•ä¸ªå…³é”®è¯ï¼ˆå¦‚"scroll"ï¼‰
                single_keyword = words[0] if words else None
                
                # æ£€æŸ¥æ˜¯å¦ä¸å…¶ä»–éƒ¨åˆ†å…±äº«æ ¸å¿ƒå…³é”®è¯
                found_group = False
                for keyword, group_parts in keyword_groups.items():
                    if core_keyword in keyword or keyword in core_keyword or \
                       (single_keyword and (single_keyword in keyword or keyword in single_keyword)):
                        keyword_groups[keyword].append(part)
                        found_group = True
                        break
                
                if not found_group:
                    # åˆ›å»ºæ–°çš„å…³é”®è¯ç»„
                    keyword_groups[core_keyword] = [part]
        
        # åˆå¹¶åŒ…å«ç›¸åŒå…³é”®è¯çš„éƒ¨åˆ†
        merged_parts = []
        for keyword, group_parts in keyword_groups.items():
            if len(group_parts) > 1:
                # å¤šä¸ªéƒ¨åˆ†åŒ…å«ç›¸åŒå…³é”®è¯ï¼Œåˆå¹¶å®ƒä»¬
                max_weight = 1.0
                min_index = float('inf')
                merged_text_parts = []
                
                for part in group_parts:
                    weight_match = re.search(r':(\d+\.?\d*)', part["text"])
                    if weight_match:
                        weight = float(weight_match.group(1))
                        if weight > max_weight:
                            max_weight = weight
                    if part["index"] < min_index:
                        min_index = part["index"]
                    
                    # æå–æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤æƒé‡ï¼‰
                    text_content = re.sub(r':\d+\.?\d*\)?$', '', part["text"])
                    text_content = re.sub(r'^\(|\)$', '', text_content).strip()
                    merged_text_parts.append(text_content)
                
                # åˆå¹¶æ–‡æœ¬ï¼Œå»é‡
                unique_parts = []
                seen_words = set()
                for text_part in merged_text_parts:
                    words_in_part = set(text_part.lower().split())
                    # å¦‚æœè¿™ä¸ªéƒ¨åˆ†åŒ…å«æ–°è¯æ±‡ï¼Œæ·»åŠ å®ƒ
                    if not words_in_part.issubset(seen_words):
                        unique_parts.append(text_part)
                        seen_words.update(words_in_part)
                
                # åˆ›å»ºåˆå¹¶åçš„æè¿°
                merged_content = ', '.join(unique_parts)
                merged_text = f"({merged_content}:{max_weight:.1f})"
                
                merged_part = {
                    "text": merged_text,
                    "type": group_parts[0]["type"],
                    "analysis": group_parts[0]["analysis"].copy(),
                    "index": min_index
                }
                merged_part["analysis"]["token_count"] = self.token_estimator.estimate(merged_text)
                merged_parts.append(merged_part)
                
                removed_count = len(group_parts) - 1
                print(f"  âœ“ åˆå¹¶ {len(group_parts)} ä¸ªåŒ…å«ç›¸åŒå…³é”®è¯çš„éƒ¨åˆ†ï¼ˆå…³é”®è¯: {keyword[:30]}...ï¼‰ï¼Œç§»é™¤ {removed_count} ä¸ªé‡å¤é¡¹")
            else:
                # åªæœ‰ä¸€ä¸ªéƒ¨åˆ†ï¼Œç›´æ¥ä¿ç•™
                merged_parts.append(group_parts[0])
        
        # ç¬¬ä¸‰æ­¥ï¼šæ£€æµ‹å®Œå…¨é‡å¤çš„æè¿°ï¼ˆç›¸åŒæˆ–å‡ ä¹ç›¸åŒçš„æ–‡æœ¬ï¼‰
        # ä½¿ç”¨merged_partsï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–processed_parts
        parts_for_final_check = merged_parts if merged_parts else processed_parts
        final_parts = []
        seen_texts = {}  # å­˜å‚¨è§„èŒƒåŒ–æ–‡æœ¬ -> (part, index_in_final_parts)
        
        for part in parts_for_final_check:
            # è§„èŒƒåŒ–æ–‡æœ¬ï¼ˆå»é™¤æƒé‡ï¼Œç”¨äºæ¯”è¾ƒï¼‰
            text_normalized = re.sub(r':\d+\.?\d*', '', part["text"]).lower().strip()
            text_normalized = re.sub(r'[()]', '', text_normalized)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è§è¿‡ç±»ä¼¼çš„æ–‡æœ¬
            is_duplicate = False
            duplicate_key = None
            
            for seen_text, (seen_part, seen_idx) in seen_texts.items():
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„è¯æ±‡é‡å åº¦ï¼‰
                seen_words = set(seen_text.split())
                current_words = set(text_normalized.split())
                
                if len(seen_words) > 0 and len(current_words) > 0:
                    overlap = len(seen_words & current_words)
                    similarity = overlap / max(len(seen_words), len(current_words))
                    
                    # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡80%ï¼Œè®¤ä¸ºæ˜¯é‡å¤
                    if similarity > 0.8:
                        # æ¯”è¾ƒæƒé‡ï¼Œä¿ç•™æƒé‡æ›´é«˜çš„é‚£ä¸ª
                        current_weight_match = re.search(r':(\d+\.?\d*)', part["text"])
                        seen_weight_match = re.search(r':(\d+\.?\d*)', seen_part["text"])
                        
                        current_weight = float(current_weight_match.group(1)) if current_weight_match else 1.0
                        seen_weight = float(seen_weight_match.group(1)) if seen_weight_match else 1.0
                        
                        if current_weight > seen_weight:
                            # å½“å‰éƒ¨åˆ†æƒé‡æ›´é«˜ï¼Œæ›¿æ¢æ—§çš„éƒ¨åˆ†
                            duplicate_key = seen_text
                            # ä»final_partsä¸­ç§»é™¤æ—§çš„éƒ¨åˆ†
                            final_parts[seen_idx] = part
                            seen_texts[seen_text] = (part, seen_idx)
                        else:
                            # å·²å­˜åœ¨çš„éƒ¨åˆ†æƒé‡æ›´é«˜ï¼Œä¿ç•™å®ƒ
                            pass
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                # æ·»åŠ æ–°éƒ¨åˆ†
                final_parts.append(part)
                seen_texts[text_normalized] = (part, len(final_parts) - 1)
        
        # è®¡ç®—ç§»é™¤æ•°é‡
        if len(final_parts) < len(parts_for_final_check):
            removed = len(parts_for_final_check) - len(final_parts)
            print(f"  âœ“ ç§»é™¤ {removed} ä¸ªå®Œå…¨é‡å¤çš„æè¿°")
        
        return final_parts
    
    def _remove_internal_duplicates(self, text: str) -> str:
        """
        ç§»é™¤å•ä¸ª part å†…éƒ¨çš„é‡å¤è¯æ±‡
        
        å¤„ç†æƒ…å†µï¼š
        1. åŒ…å«å…³ç³»ï¼šå¦‚ "scroll" å’Œ "golden scroll" -> åªä¿ç•™ "golden scroll"
        2. è¯­ä¹‰é‡å¤ï¼šå¦‚ "single person" å’Œ "only one character" -> åªä¿ç•™ä¸€ä¸ª
        """
        import re
        
        # æå–æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        weight_match = re.search(r':(\d+\.?\d*)\)?$', text)
        weight = weight_match.group(1) if weight_match else None
        has_paren = text.strip().startswith('(')
        
        # ç§»é™¤æƒé‡å’Œæ‹¬å·ï¼Œæå–å†…å®¹
        content = re.sub(r':\d+\.?\d*\)?$', '', text)
        content = re.sub(r'^\(|\)$', '', content).strip()
        
        # åˆ†å‰²æˆè¯æ±‡åˆ—è¡¨
        words = [w.strip() for w in content.split(',') if w.strip()]
        
        if len(words) <= 1:
            return text  # åªæœ‰ä¸€ä¸ªè¯ï¼Œä¸éœ€è¦å»é‡
        
        # å®šä¹‰è¯­ä¹‰é‡å¤ç»„ï¼ˆåŒä¹‰è¯ç»„ï¼‰
        semantic_groups = [
            # å•äººç›¸å…³ï¼šå¦‚æœåŒæ—¶å­˜åœ¨å¤šä¸ªï¼Œåªä¿ç•™æœ€ç®€æ´çš„ä¸€ä¸ª
            {
                "keywords": ["single person", "lone figure", "only one character", "one person only", 
                           "sole character", "single individual"],
                "keep": "single person, only one character",  # é»˜è®¤ä¿ç•™ç»„åˆ
                "single_keep": "single person"  # å¦‚æœåªéœ€è¦ä¸€ä¸ªï¼Œä¿ç•™è¿™ä¸ª
            },
            # æ­£é¢æœå‘ç›¸å…³
            {
                "keywords": ["facing camera", "front view", "face forward", "character facing viewer", 
                           "frontal view"],
                "keep": "facing camera, front view",
                "single_keep": "facing camera"
            },
            # ç‰¹å†™ç›¸å…³
            {
                "keywords": ["close-up", "closeup", "close up", "face close-up", "portrait shot", "headshot"],
                "keep": "close-up, face close-up",
                "single_keep": "close-up"
            }
        ]
        
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†è¯­ä¹‰é‡å¤ç»„
        processed_words = []
        used_indices = set()
        
        for group in semantic_groups:
            matching_indices = []
            matching_words = []
            for i, word in enumerate(words):
                if i in used_indices:
                    continue
                word_lower = word.lower()
                if any(kw in word_lower for kw in group["keywords"]):
                    matching_indices.append(i)
                    matching_words.append(word)
            
            if len(matching_indices) > 1:
                # å¦‚æœåŒ¹é…åˆ°å¤šä¸ªè¯­ä¹‰é‡å¤çš„è¯ï¼Œåªä¿ç•™ä¸€ä¸ªæœ€ç®€æ´çš„
                # ä¼˜å…ˆé€‰æ‹©æœ€çŸ­ä¸”æœ€å¸¸ç”¨çš„
                best_word = None
                best_length = float('inf')
                for word in matching_words:
                    if not word or not word.strip():  # è·³è¿‡ç©ºè¯
                        continue
                    word_lower = word.lower()
                    # ä¼˜å…ˆé€‰æ‹© "single person" æˆ– "only one character"ï¼ˆæœ€ç®€æ´ï¼‰
                    if "single person" in word_lower or "only one character" in word_lower:
                        if len(word) < best_length:
                            best_word = word
                            best_length = len(word)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ€ç®€æ´çš„ï¼Œé€‰æ‹©æœ€çŸ­çš„
                if best_word is None and matching_words:
                    # è¿‡æ»¤æ‰ç©ºè¯
                    valid_words = [w for w in matching_words if w and w.strip()]
                    if valid_words:
                        best_word = min(valid_words, key=len)
                
                # åªæœ‰å½“ best_word ä¸ä¸º None æ—¶æ‰æ·»åŠ 
                if best_word and best_word.strip():
                    processed_words.append(best_word)
                    for idx in matching_indices:
                        used_indices.add(idx)
            elif len(matching_indices) == 1:
                # åªæœ‰ä¸€ä¸ªåŒ¹é…ï¼Œç›´æ¥ä¿ç•™
                idx = matching_indices[0]
                word = words[idx]
                if word and word.strip():  # ç¡®ä¿ä¸æ˜¯ç©ºè¯
                    processed_words.append(word)
                    used_indices.add(idx)
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†åŒ…å«å…³ç³»ï¼ˆå¦‚æœä¸€ä¸ªè¯æ˜¯å¦ä¸€ä¸ªè¯çš„ä¸€éƒ¨åˆ†ï¼‰
        remaining_words = [words[i] for i in range(len(words)) if i not in used_indices]
        
        # æŒ‰é•¿åº¦æ’åºï¼ˆé•¿çš„åœ¨å‰ï¼‰ï¼Œè¿™æ ·çŸ­çš„è¯å¦‚æœè¢«åŒ…å«åœ¨é•¿è¯ä¸­ï¼Œä¼šè¢«æ£€æµ‹åˆ°
        remaining_words_sorted = sorted(remaining_words, key=len, reverse=True)
        final_words = []
        
        for word in remaining_words_sorted:
            if not word or not word.strip():  # è·³è¿‡ç©ºè¯
                continue
            word_lower = word.lower()
            # æ£€æŸ¥æ˜¯å¦è¢«å·²æ·»åŠ çš„è¯åŒ…å«
            is_contained = False
            for existing_word in final_words:
                if not existing_word or not existing_word.strip():  # è·³è¿‡ç©ºè¯
                    continue
                existing_lower = existing_word.lower()
                # æ£€æŸ¥ word æ˜¯å¦è¢« existing_word åŒ…å«ï¼ˆä½œä¸ºå®Œæ•´è¯ï¼Œä¸æ˜¯å­ä¸²ï¼‰
                # ä¾‹å¦‚ï¼š"scroll" åœ¨ "golden scroll" ä¸­
                if word_lower in existing_lower:
                    # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šç¡®ä¿æ˜¯å®Œæ•´çš„è¯ï¼Œè€Œä¸æ˜¯éƒ¨åˆ†åŒ¹é…
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„è¯
                    pattern = r'\b' + re.escape(word_lower) + r'\b'
                    if re.search(pattern, existing_lower):
                        is_contained = True
                        break
                # æˆ–è€… existing_word è¢« word åŒ…å«ï¼ˆword æ›´é•¿ï¼‰
                elif existing_lower in word_lower:
                    pattern = r'\b' + re.escape(existing_lower) + r'\b'
                    if re.search(pattern, word_lower):
                        # ç§»é™¤è¾ƒçŸ­çš„è¯ï¼Œä¿ç•™è¾ƒé•¿çš„è¯
                        final_words.remove(existing_word)
                        is_contained = False
                        break
            
            if not is_contained:
                final_words.append(word)
        
        # åˆå¹¶å¤„ç†åçš„è¯æ±‡ï¼ˆè¿‡æ»¤æ‰ None å’Œç©ºè¯ï¼‰
        valid_final_words = [w for w in final_words if w and w.strip()]
        processed_words.extend(valid_final_words)
        
        # è¿‡æ»¤æ‰ None å’Œç©ºè¯
        processed_words = [w for w in processed_words if w and w.strip()]
        
        # å¦‚æœè¯æ±‡æœ‰å˜åŒ–ï¼Œé‡å»ºæ–‡æœ¬
        if len(processed_words) != len(words) or set(processed_words) != set(words):
            # å»é‡å¹¶ä¿æŒé¡ºåº
            seen = set()
            unique_words = []
            for word in processed_words:
                if word and word.strip() and word not in seen:
                    seen.add(word)
                    unique_words.append(word)
            
            # é‡å»ºæ–‡æœ¬
            new_content = ', '.join(unique_words)
            if weight:
                new_text = f"({new_content}:{weight})" if has_paren else f"{new_content}:{weight}"
            else:
                new_text = f"({new_content})" if has_paren else new_content
            
            return new_text
        
        return text
    
    def enhance_prompt_part(self, part: str, part_type: str) -> str:
        """
        é€šç”¨çš„promptéƒ¨åˆ†å¢å¼ºæ–¹æ³•ï¼ˆå…¼å®¹å±‚ï¼‰
        
        âš¡ æ–°æ¶æ„ï¼šä½¿ç”¨ AST + è¯­ä¹‰å¢å¼ºå™¨ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²æ“ä½œ
        
        Args:
            part: promptéƒ¨åˆ†æ–‡æœ¬
            part_type: éƒ¨åˆ†ç±»å‹
            
        Returns:
            å¢å¼ºåçš„promptéƒ¨åˆ†æ–‡æœ¬
        """
        if not part:
            return part
        
        # âš¡ ä½¿ç”¨ AST æ¶æ„è¿›è¡Œå¢å¼º
        # 1. è§£æä¸º AST
        node = self.ast_builder.parse_part(part, index=0)
        node.type = part_type  # ä½¿ç”¨ä¼ å…¥çš„ç±»å‹
        
        # 2. åˆ›å»ºä¸´æ—¶ AST è¿›è¡Œå¢å¼º
        temp_ast = PromptAST([node])
        temp_ast = self.semantic_enhancer.enhance_ast(temp_ast)
        
        # 3. è¿”å›å¢å¼ºåçš„èŠ‚ç‚¹å­—ç¬¦ä¸²
        enhanced_node = temp_ast.nodes[0]
        return enhanced_node.to_string(include_weight=True)





#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„ Flux LoRA è®­ç»ƒè„šæœ¬
é’ˆå¯¹å½“å‰é—®é¢˜ä¼˜åŒ–ï¼šäººè„¸ä¸å¯¹ã€å½¢è±¡ä¸å¯¹
"""

import os
import torch
from pathlib import Path
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from diffusers import FluxPipeline
from peft import LoraConfig, get_peft_model
from accelerate import Accelerator
from tqdm import tqdm
import argparse


class HostDataset(Dataset):
    """ä¸»æŒäººè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_dir: str, tokenizer, tokenizer_2, size: int = 512):
        self.data_dir = Path(data_dir)
        self.tokenizer = tokenizer  # CLIP tokenizer
        self.tokenizer_2 = tokenizer_2  # T5 tokenizer
        self.size = size
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
        self.images = []
        image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.JPG', '.JPEG', '.PNG', '.WEBP'}
        
        for img_file in sorted(self.data_dir.iterdir()):
            if img_file.suffix in image_extensions:
                prompt = self._extract_prompt_from_filename(img_file.name)
                self.images.append({
                    'path': img_file,
                    'prompt': prompt
                })
        
        print(f"âœ… æ‰¾åˆ° {len(self.images)} å¼ è®­ç»ƒå›¾ç‰‡")
    
    def _extract_prompt_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æç¤ºè¯ï¼Œå¹¶ä¼˜åŒ–é¡ºåºï¼ˆæ ¸å¿ƒä¿¡æ¯åœ¨å‰ï¼‰"""
        if '_repeat_' in filename:
            parts = filename.split('_repeat_', 1)
            if len(parts) > 1:
                prompt_part = parts[1].split('_', 1)
                if len(prompt_part) > 1:
                    prompt = prompt_part[1]
                    prompt = prompt.rsplit('.', 1)[0]
                    
                    # ä¼˜åŒ–æç¤ºè¯é¡ºåºï¼šæ ¸å¿ƒä¿¡æ¯åœ¨å‰ï¼Œç»†èŠ‚åœ¨å
                    # è¿™æ ·å³ä½¿è¢« CLIP tokenizer æˆªæ–­ï¼ˆ77 tokensï¼‰ï¼Œæ ¸å¿ƒä¿¡æ¯ä¹Ÿèƒ½ä¿ç•™
                    prompt_parts = prompt.split("ï¼Œ")
                    core_keywords = []
                    detail_keywords = []
                    
                    # è¯†åˆ«æ ¸å¿ƒå…³é”®è¯
                    core_patterns = ["ç§‘æ™®ä¸»æŒäºº", "ç”·æ€§", "ä¸“ä¸šå½¢è±¡"]
                    for part in prompt_parts:
                        if any(pattern in part for pattern in core_patterns):
                            core_keywords.append(part)
                        else:
                            detail_keywords.append(part)
                    
                    # é‡æ–°æ’åºï¼šæ ¸å¿ƒä¿¡æ¯ + ç»†èŠ‚
                    if core_keywords:
                        optimized_prompt = "ï¼Œ".join(core_keywords + detail_keywords)
                        return optimized_prompt
                    else:
                        return prompt
        return "ç§‘æ™®ä¸»æŒäººï¼Œç”·æ€§ï¼Œä¸“ä¸šå½¢è±¡"
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        item = self.images[idx]
        
        # åŠ è½½å›¾ç‰‡
        image = Image.open(item['path']).convert('RGB')
        if image.size != (self.size, self.size):
            image = image.resize((self.size, self.size), Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸º tensor
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
        image_tensor = transform(image).float()
        
        # Tokenize æç¤ºè¯ï¼ˆFlux ä½¿ç”¨ CLIP + T5 åŒç¼–ç å™¨ï¼‰
        prompt = item['prompt']
        
        # CLIP tokenizer åªèƒ½å¤„ç† 77 tokensï¼Œéœ€è¦æˆªæ–­
        # å…ˆæ£€æŸ¥é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™æˆªæ–­
        text_inputs_1 = self.tokenizer(
            prompt,
            padding="max_length",
            max_length=77,  # CLIP tokenizer æœ€å¤§é•¿åº¦
            truncation=True,  # è‡ªåŠ¨æˆªæ–­
            return_tensors="pt",
            return_length=False  # ä¸è¿”å›é•¿åº¦ï¼Œé¿å…è­¦å‘Š
        )
        
        # T5 tokenizer æ”¯æŒæ›´é•¿åºåˆ—ï¼ˆ512 tokensï¼‰
        text_inputs_2 = self.tokenizer_2(
            prompt,
            padding="max_length",
            max_length=512,  # T5 æ”¯æŒæ›´é•¿åºåˆ—
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            'pixel_values': image_tensor,
            'input_ids_1': text_inputs_1.input_ids.squeeze(),
            'input_ids_2': text_inputs_2.input_ids.squeeze(),
            'prompt': prompt
        }


def train_flux_lora_improved(
    data_dir: str,
    output_dir: str,
    base_model_path: str,
    num_train_epochs: int = 20,  # å¢åŠ åˆ° 20 è½®
    train_batch_size: int = 1,
    gradient_accumulation_steps: int = 4,
    learning_rate: float = 5e-5,  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ä¿å®ˆ
    lora_rank: int = 16,  # é™ä½ rankï¼Œæ›´ä¿å®ˆï¼ˆæ•°æ®å°‘æ—¶ï¼‰
    lora_alpha: int = 16,
    save_steps: int = 500,
    resolution: int = 512,  # ä½¿ç”¨ 512 åˆ†è¾¨ç‡ï¼ŒèŠ‚çœæ˜¾å­˜ï¼ŒåŠ å¿«è®­ç»ƒ
    use_bf16: bool = True,
):
    """
    æ”¹è¿›çš„ Flux LoRA è®­ç»ƒï¼ˆé’ˆå¯¹å½“å‰é—®é¢˜ä¼˜åŒ–ï¼‰
    
    ä¼˜åŒ–ç‚¹ï¼š
    - å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ20 è½®ï¼‰
    - é™ä½å­¦ä¹ ç‡ï¼ˆ5e-5ï¼Œæ›´ä¿å®ˆï¼‰
    - é™ä½ LoRA rankï¼ˆ16ï¼Œæ•°æ®å°‘æ—¶æ›´ç¨³å®šï¼‰
    - ä½¿ç”¨ 512 åˆ†è¾¨ç‡ï¼ˆåŠ å¿«è®­ç»ƒï¼‰
    """
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ”¹è¿›ç‰ˆ Flux LoRA")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=gradient_accumulation_steps,
        mixed_precision="bf16" if use_bf16 else "fp16"
    )
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¼˜åŒ–æ˜¾å­˜ï¼‰
    print(f"\nğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    pipe = FluxPipeline.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,
        device_map="balanced"
    )
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    if hasattr(pipe.transformer, "enable_gradient_checkpointing"):
        pipe.transformer.enable_gradient_checkpointing()
        print("  âœ… å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    
    # 3. é…ç½® LoRAï¼ˆæ›´ä¿å®ˆçš„å‚æ•°ï¼‰
    print(f"\nğŸ”§ é…ç½® LoRA (rank={lora_rank}, alpha={lora_alpha})")
    
    # Flux transformer çš„æ³¨æ„åŠ›å±‚ï¼ˆDiT æ¶æ„ï¼‰
    target_modules = [
        "attn.to_k",
        "attn.to_q",
        "attn.to_v",
        "attn.to_out.0",
    ]
    
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.1,
    )
    
    # 4. åº”ç”¨ LoRA åˆ° transformer
    pipe.transformer = get_peft_model(pipe.transformer, lora_config)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆåœ¨åº”ç”¨ LoRA åï¼‰
    if hasattr(pipe.transformer, "enable_gradient_checkpointing"):
        pipe.transformer.enable_gradient_checkpointing()
        print("  âœ… LoRA æ¨¡å‹å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # 5. å‡†å¤‡æ•°æ®é›†
    print(f"\nğŸ“ å‡†å¤‡è®­ç»ƒæ•°æ®: {data_dir}")
    dataset = HostDataset(
        data_dir=data_dir,
        tokenizer=pipe.tokenizer,  # CLIP tokenizer
        tokenizer_2=pipe.tokenizer_2,  # T5 tokenizer
        size=resolution
    )
    
    if len(dataset) == 0:
        raise ValueError(f"æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼è¯·æ£€æŸ¥ç›®å½•: {data_dir}")
    
    dataloader = DataLoader(
        dataset,
        batch_size=train_batch_size,
        shuffle=True,
        num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    # 6. è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨ 8bit AdamW èŠ‚çœæ˜¾å­˜ï¼‰
    try:
        import bitsandbytes as bnb
        optimizer = bnb.optim.AdamW8bit(
            pipe.transformer.parameters(),
            lr=learning_rate
        )
        print("  â„¹ ä½¿ç”¨ 8bit AdamW ä¼˜åŒ–å™¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    except ImportError:
        optimizer = torch.optim.AdamW(
            pipe.transformer.parameters(),
            lr=learning_rate
        )
        print("  â„¹ ä½¿ç”¨æ ‡å‡† AdamW ä¼˜åŒ–å™¨")
    
    # 7. å‡†å¤‡è®­ç»ƒ
    pipe.transformer, optimizer, dataloader = accelerator.prepare(
        pipe.transformer, optimizer, dataloader
    )
    
    # 8. è®­ç»ƒå¾ªç¯
    num_update_steps_per_epoch = len(dataloader) // gradient_accumulation_steps
    max_train_steps = num_train_epochs * num_update_steps_per_epoch
    
    print(f"\nğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   è®­ç»ƒè½®æ•°: {num_train_epochs}")
    print(f"   æ€»æ­¥æ•°: {max_train_steps}")
    print(f"   æ‰¹æ¬¡å¤§å°: {train_batch_size}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   LoRA rank: {lora_rank}")
    print(f"   LoRA alpha: {lora_alpha}")
    print(f"   åˆ†è¾¨ç‡: {resolution}x{resolution}")
    print(f"   ç²¾åº¦: {'bf16' if use_bf16 else 'fp16'}")
    print(f"   GPU: H20 (97GB æ˜¾å­˜)")
    
    global_step = 0
    progress_bar = tqdm(range(max_train_steps), desc="è®­ç»ƒä¸­")
    
    pipe.transformer.train()
    
    # è®°å½•æŸå¤±
    losses = []
    
    for epoch in range(num_train_epochs):
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(pipe.transformer):
                device = next(pipe.transformer.parameters()).device
                dtype = torch.bfloat16 if use_bf16 else torch.float16
                
                # VAE ç¼–ç 
                pixel_values = batch['pixel_values'].to(device, dtype=dtype)
                with torch.no_grad():
                    latents = pipe.vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * pipe.vae.config.scaling_factor
                # æ¸…ç†æ˜¾å­˜
                del pixel_values
                torch.cuda.empty_cache()
                
                # Flow Matching å™ªå£°æ·»åŠ 
                noise = torch.randn_like(latents, device=device, dtype=dtype)
                timesteps = torch.rand(
                    (latents.shape[0],),
                    device=device,
                    dtype=dtype
                )  # [0, 1]
                
                # Flow Matching: x_t = (1-t)*x_0 + t*x_1
                t = timesteps.view(-1, 1, 1, 1)
                noisy_latents = (1 - t) * latents + t * noise
                
                # ç¼–ç æç¤ºè¯ï¼ˆä½¿ç”¨ pipe.encode_prompt æ–¹æ³•ï¼‰
                prompts = batch['prompt']
                with torch.no_grad():
                    prompt_embeds, pooled_prompt_embeds, text_ids = pipe.encode_prompt(
                        prompts,
                        num_images_per_prompt=1,
                        device=device
                    )
                    encoder_hidden_states = prompt_embeds
                # æ¸…ç†æ˜¾å­˜
                torch.cuda.empty_cache()
                
                # å‡†å¤‡ latent image IDsï¼ˆFlux éœ€è¦ï¼‰
                latent_image_ids = FluxPipeline._prepare_latent_image_ids(
                    noisy_latents.shape[0],
                    noisy_latents.shape[2] // 2,
                    noisy_latents.shape[3] // 2,
                    device,
                    dtype
                )
                
                # æ‰“åŒ… latentsï¼ˆFlux éœ€è¦ï¼‰
                packed_noisy_latents = FluxPipeline._pack_latents(
                    noisy_latents,
                    batch_size=noisy_latents.shape[0],
                    num_channels_latents=noisy_latents.shape[1],
                    height=noisy_latents.shape[2],
                    width=noisy_latents.shape[3],
                )
                
                # å¤„ç† guidanceï¼ˆå¦‚æœéœ€è¦ï¼‰
                if hasattr(pipe.transformer.config, 'guidance_embeds') and pipe.transformer.config.guidance_embeds:
                    guidance = torch.tensor([3.5], device=device).expand(noisy_latents.shape[0])
                else:
                    guidance = None
                
                # é¢„æµ‹ï¼ˆFlux transformer è°ƒç”¨ï¼‰
                model_pred = pipe.transformer(
                    hidden_states=packed_noisy_latents,
                    timestep=timesteps / 1000.0,  # Flux éœ€è¦é™¤ä»¥ 1000
                    guidance=guidance,
                    pooled_projections=pooled_prompt_embeds,
                    encoder_hidden_states=encoder_hidden_states,
                    txt_ids=text_ids,
                    img_ids=latent_image_ids,
                    return_dict=False,
                )[0]
                
                # è§£åŒ… latents
                vae_scale_factor = 2 ** (len(pipe.vae.config.block_out_channels) - 1)
                model_pred = FluxPipeline._unpack_latents(
                    model_pred,
                    height=noisy_latents.shape[2] * vae_scale_factor,
                    width=noisy_latents.shape[3] * vae_scale_factor,
                    vae_scale_factor=vae_scale_factor,
                )
                
                # è®¡ç®—æŸå¤±ï¼ˆFlow Matchingï¼šé€Ÿåº¦åœºï¼‰
                target_velocity = noise - latents
                loss = torch.nn.functional.mse_loss(model_pred.float(), target_velocity.float())
                
                # è®°å½•æŸå¤±å€¼ï¼ˆåœ¨æ¸…ç†å‰ï¼‰
                loss_value = loss.item()
                losses.append(loss_value)
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                
                # æ¸…ç†æ˜¾å­˜
                del model_pred, target_velocity, loss
                torch.cuda.empty_cache()
                
                # æ›´æ–°è¿›åº¦æ¡
                if step % 10 == 0:
                    avg_loss = sum(losses[-10:]) / min(10, len(losses))
                    progress_bar.set_postfix({
                        "loss": f"{loss_value:.4f}",
                        "avg_loss": f"{avg_loss:.4f}",
                        "epoch": f"{epoch+1}/{num_train_epochs}"
                    })
            
            global_step += 1
            progress_bar.update(1)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if global_step % save_steps == 0:
                if accelerator.is_main_process:
                    checkpoint_dir = Path(output_dir) / f"checkpoint-{global_step}"
                    checkpoint_dir.mkdir(parents=True, exist_ok=True)
                    pipe.transformer.save_pretrained(str(checkpoint_dir))
                    avg_loss = sum(losses[-100:]) / min(100, len(losses))
                    print(f"\nğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_dir} (å¹³å‡æŸå¤±: {avg_loss:.4f})")
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {output_dir}")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    if accelerator.is_main_process:
        pipe.transformer.save_pretrained(str(output_path))
        
        # ä¿å­˜ä¸º safetensors
        try:
            from safetensors.torch import save_file
            state_dict = {}
            for name, param in pipe.transformer.named_parameters():
                if 'lora' in name.lower():
                    state_dict[name] = param.data.cpu()
            if state_dict:
                safetensors_path = output_path / "pytorch_lora_weights.safetensors"
                save_file(state_dict, str(safetensors_path))
                print(f"âœ… å·²ä¿å­˜ safetensors: {safetensors_path}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ safetensors æ—¶å‡ºé”™: {e}")
        
        # æ‰“å°è®­ç»ƒç»Ÿè®¡
        final_avg_loss = sum(losses[-100:]) / min(100, len(losses))
        initial_avg_loss = sum(losses[:100]) / min(100, len(losses))
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"   åˆå§‹å¹³å‡æŸå¤±: {initial_avg_loss:.4f}")
        print(f"   æœ€ç»ˆå¹³å‡æŸå¤±: {final_avg_loss:.4f}")
        print(f"   æŸå¤±ä¸‹é™: {initial_avg_loss - final_avg_loss:.4f}")
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ”¹è¿›çš„ Flux LoRA è®­ç»ƒè„šæœ¬")
    parser.add_argument("--data-dir", type=str, default="train_data/host_person")
    parser.add_argument("--output-dir", type=str, default="models/lora/host_person_v2")
    parser.add_argument("--base-model", type=str, default="models/flux1-dev")
    parser.add_argument("--epochs", type=int, default=20, help="è®­ç»ƒè½®æ•°ï¼ˆå¢åŠ åˆ° 20ï¼‰")
    parser.add_argument("--batch-size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient-accumulation", type=int, default=4, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--learning-rate", type=float, default=5e-5, help="å­¦ä¹ ç‡ï¼ˆé™ä½åˆ° 5e-5ï¼‰")
    parser.add_argument("--lora-rank", type=int, default=16, help="LoRA rankï¼ˆé™ä½åˆ° 16ï¼Œæ›´ä¿å®ˆï¼‰")
    parser.add_argument("--lora-alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--save-steps", type=int, default=500, help="ä¿å­˜æ£€æŸ¥ç‚¹çš„æ­¥æ•°")
    parser.add_argument("--resolution", type=int, default=512, help="è®­ç»ƒåˆ†è¾¨ç‡ï¼ˆ512 æ›´å¿«ï¼‰")
    parser.add_argument("--use-bf16", action="store_true", default=True, help="ä½¿ç”¨ bf16")
    
    args = parser.parse_args()
    
    train_flux_lora_improved(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        base_model_path=args.base_model,
        num_train_epochs=args.epochs,
        train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        save_steps=args.save_steps,
        resolution=args.resolution,
        use_bf16=args.use_bf16,
    )


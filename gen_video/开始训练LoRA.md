# å¼€å§‹è®­ç»ƒç§‘æ™®ä¸»æŒäºº LoRA

## âœ… æ•°æ®å‡†å¤‡å®Œæˆ

- âœ… è®­ç»ƒå›¾ç‰‡ï¼š20 å¼ 
- âœ… å›¾ç‰‡å°ºå¯¸ï¼š1024x1024ï¼ˆå·²ç»Ÿä¸€ï¼‰
- âœ… æç¤ºè¯ï¼šå·²ä»æ–‡ä»¶åæå–

## ğŸš€ å¼€å§‹è®­ç»ƒ

### **æ–¹æ³• 1ï¼šä½¿ç”¨è®­ç»ƒè„šæœ¬ï¼ˆæ¨èï¼‰**

```bash
cd /vepfs-dev/shawn/vid/fanren/gen_video

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœä½¿ç”¨ï¼‰
source /vepfs-dev/shawn/venv/py312/bin/activate

# å¼€å§‹è®­ç»ƒï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
python train_host_lora.py \
    --data-dir train_data/host_person \
    --output-dir models/lora/host_person \
    --base-model models/flux1-dev \
    --epochs 10 \
    --batch-size 1 \
    --gradient-accumulation 4 \
    --learning-rate 1e-4 \
    --lora-rank 32 \
    --lora-alpha 16 \
    --save-steps 200
```

### **å‚æ•°è¯´æ˜ï¼š**

- `--data-dir`: è®­ç»ƒæ•°æ®ç›®å½•ï¼ˆé»˜è®¤: `train_data/host_person`ï¼‰
- `--output-dir`: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: `models/lora/host_person`ï¼‰
- `--base-model`: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: `models/flux1-dev`ï¼‰
- `--epochs`: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 10ï¼Œçº¦ 1000 æ­¥ï¼‰
- `--batch-size`: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 1ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
- `--gradient-accumulation`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆé»˜è®¤: 4ï¼‰
- `--learning-rate`: å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 1e-4ï¼‰
- `--lora-rank`: LoRA ç»´åº¦ï¼ˆé»˜è®¤: 32ï¼‰
- `--lora-alpha`: LoRA alphaï¼ˆé»˜è®¤: 16ï¼‰
- `--save-steps`: æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡ï¼ˆé»˜è®¤: 200ï¼‰

### **æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼š**

**24GB æ˜¾å­˜ï¼ˆA100ï¼‰ï¼š**
```bash
python train_host_lora.py \
    --batch-size 2 \
    --gradient-accumulation 2
```

**16GB æ˜¾å­˜ï¼š**
```bash
python train_host_lora.py \
    --batch-size 1 \
    --gradient-accumulation 4
```

**12GB æ˜¾å­˜ï¼š**
```bash
python train_host_lora.py \
    --batch-size 1 \
    --gradient-accumulation 8 \
    --lora-rank 16  # é™ä½ LoRA ç»´åº¦
```

---

## â±ï¸ è®­ç»ƒæ—¶é—´ä¼°ç®—

- **20 å¼ å›¾ç‰‡ï¼Œ10 è½®ï¼Œbatch_size=1ï¼Œgradient_accumulation=4**
- **æ€»æ­¥æ•°**: çº¦ 1000 æ­¥ï¼ˆ20 å¼  Ã— 10 è½® / 4 æ¢¯åº¦ç´¯ç§¯ï¼‰
- **é¢„è®¡æ—¶é—´**: 
  - A100 (24GB): çº¦ 1-2 å°æ—¶
  - RTX 3090 (24GB): çº¦ 2-3 å°æ—¶
  - RTX 3080 (10GB): çº¦ 3-4 å°æ—¶

---

## ğŸ“Š è®­ç»ƒè¿‡ç¨‹ç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- å½“å‰æ­¥æ•°
- æŸå¤±å€¼ï¼ˆlossï¼‰
- æ¯ 200 æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹

**æ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®ï¼š**
```
models/lora/host_person/
  checkpoint-200/
  checkpoint-400/
  checkpoint-600/
  ...
  checkpoint-1000/  (æœ€ç»ˆ)
```

---

## âœ… è®­ç»ƒå®Œæˆåçš„æ­¥éª¤

1. **æ£€æŸ¥è®­ç»ƒç»“æœ**
   ```bash
   ls -lh models/lora/host_person/
   ```

2. **æµ‹è¯• LoRA**
   - åœ¨ `model_manager.py` ä¸­é…ç½® `lora_path`
   - ä½¿ç”¨è§¦å‘è¯"ç§‘æ™®ä¸»æŒäºº"ç”Ÿæˆæµ‹è¯•å›¾åƒ

3. **å¦‚æœæ•ˆæœä¸ç†æƒ³**
   - å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ`--epochs 15`ï¼‰
   - è°ƒæ•´ LoRA æƒé‡ï¼ˆ`lora_alpha`ï¼‰
   - æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ˜¾å­˜ä¸è¶³**
   - é™ä½ `batch_size` åˆ° 1
   - å¢åŠ  `gradient_accumulation`
   - é™ä½ `lora_rank` åˆ° 16

2. **è®­ç»ƒä¸­æ–­**
   - æ£€æŸ¥ç‚¹ä¼šè‡ªåŠ¨ä¿å­˜
   - å¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆéœ€è¦ä¿®æ”¹è„šæœ¬ï¼‰

3. **æ•ˆæœä¸ç†æƒ³**
   - æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡
   - å¢åŠ è®­ç»ƒè½®æ•°
   - è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå°è¯• 5e-5 æˆ– 2e-4ï¼‰

---

## ğŸ”§ æ•…éšœæ’é™¤

### **é”™è¯¯ï¼šCUDA out of memory**
```bash
# è§£å†³æ–¹æ¡ˆï¼šé™ä½ batch_size æˆ–å¢åŠ  gradient_accumulation
python train_host_lora.py --batch-size 1 --gradient-accumulation 8
```

### **é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨**
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la models/flux1-dev/
```

### **é”™è¯¯ï¼šæ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®**
```bash
# æ£€æŸ¥æ•°æ®ç›®å½•
ls -la train_data/host_person/
```

---

## ğŸ“ å¿«é€Ÿå¯åŠ¨å‘½ä»¤

```bash
# ä¸€é”®å¯åŠ¨è®­ç»ƒï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
cd /vepfs-dev/shawn/vid/fanren/gen_video
source /vepfs-dev/shawn/venv/py312/bin/activate
python train_host_lora.py
```


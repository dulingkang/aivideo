#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Flux LoRA è®­ç»ƒè„šæœ¬ï¼ˆä½¿ç”¨ diffusers å®˜æ–¹æ–¹æ³•ï¼‰
åŸºäº diffusers å®˜æ–¹ Flux è®­ç»ƒç¤ºä¾‹
"""

import os
import torch
from pathlib import Path
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from diffusers import FluxPipeline, DDPMScheduler
from peft import LoraConfig, get_peft_model
from transformers import CLIPTokenizer
from accelerate import Accelerator
from tqdm import tqdm
import argparse


class HostDataset(Dataset):
    """ä¸»æŒäººè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_dir: str, tokenizer, size: int = 1024):
        self.data_dir = Path(data_dir)
        self.tokenizer = tokenizer
        self.size = size
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
        self.images = []
        image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.JPG', '.JPEG', '.PNG', '.WEBP'}
        
        for img_file in sorted(self.data_dir.iterdir()):
            if img_file.suffix in image_extensions:
                # ä»æ–‡ä»¶åæå–æç¤ºè¯
                prompt = self._extract_prompt_from_filename(img_file.name)
                self.images.append({
                    'path': img_file,
                    'prompt': prompt
                })
        
        print(f"âœ… æ‰¾åˆ° {len(self.images)} å¼ è®­ç»ƒå›¾ç‰‡")
    
    def _extract_prompt_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æç¤ºè¯"""
        if '_repeat_' in filename:
            parts = filename.split('_repeat_', 1)
            if len(parts) > 1:
                prompt_part = parts[1].split('_', 1)
                if len(prompt_part) > 1:
                    prompt = prompt_part[1]
                    prompt = prompt.rsplit('.', 1)[0]
                    return prompt
        return "ç§‘æ™®ä¸»æŒäººï¼Œä¸“ä¸šå½¢è±¡"
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        item = self.images[idx]
        
        # åŠ è½½å›¾ç‰‡
        image = Image.open(item['path']).convert('RGB')
        
        # è°ƒæ•´å¤§å°
        if image.size != (self.size, self.size):
            image = image.resize((self.size, self.size), Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸º tensor
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])
        image_tensor = transform(image).float()
        
        # Tokenize æç¤ºè¯ï¼ˆFlux ä½¿ç”¨ T5ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        prompt = item['prompt']
        # Flux ä½¿ç”¨åŒç¼–ç å™¨ï¼Œè¿™é‡Œå…ˆç”¨ç®€å•çš„ tokenizer
        text_inputs = self.tokenizer(
            prompt,
            padding="max_length",
            max_length=77,
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            'pixel_values': image_tensor,
            'input_ids': text_inputs.input_ids.squeeze(),
            'prompt': prompt
        }


def train_flux_lora(
    data_dir: str,
    output_dir: str,
    base_model_path: str,
    num_train_epochs: int = 10,
    train_batch_size: int = 1,
    gradient_accumulation_steps: int = 4,
    learning_rate: float = 1e-4,
    lora_rank: int = 32,
    lora_alpha: int = 16,
    save_steps: int = 200,
    resolution: int = 1024,
    use_bf16: bool = True,  # ä½¿ç”¨ bf16ï¼ˆH20 æ”¯æŒï¼‰
    use_flash_attention: bool = False,  # Flash Attentionï¼ˆå¯é€‰ï¼‰
):
    """
    è®­ç»ƒ Flux LoRAï¼ˆä½¿ç”¨ diffusers å®˜æ–¹æ–¹æ³•ï¼‰
    
    åŸºäº diffusers å®˜æ–¹ Flux è®­ç»ƒç¤ºä¾‹
    """
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ Flux LoRAï¼ˆä½¿ç”¨ diffusers å®˜æ–¹æ–¹æ³•ï¼‰")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=gradient_accumulation_steps,
        mixed_precision="bf16" if use_bf16 else "fp16"
    )
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    pipe = FluxPipeline.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,
        device_map="balanced"
    )
    
    # 3. é…ç½® LoRAï¼ˆFlux DiT æ¶æ„ï¼‰
    print(f"\nğŸ”§ é…ç½® LoRA (rank={lora_rank}, alpha={lora_alpha})")
    
    # Flux transformer çš„æ³¨æ„åŠ›å±‚
    # æ³¨æ„ï¼šFlux ä½¿ç”¨ DiT æ¶æ„ï¼Œç›®æ ‡æ¨¡å—ä¸ UNet ä¸åŒ
    target_modules = [
        "attn.to_k",
        "attn.to_q",
        "attn.to_v",
        "attn.to_out.0",
    ]
    
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.1,
    )
    
    # 4. åº”ç”¨ LoRA åˆ° transformer
    pipe.transformer = get_peft_model(pipe.transformer, lora_config)
    
    # 5. å‡†å¤‡æ•°æ®é›†
    print(f"\nğŸ“ å‡†å¤‡è®­ç»ƒæ•°æ®: {data_dir}")
    dataset = HostDataset(
        data_dir=data_dir,
        tokenizer=pipe.tokenizer,
        size=resolution
    )
    
    if len(dataset) == 0:
        raise ValueError(f"æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼è¯·æ£€æŸ¥ç›®å½•: {data_dir}")
    
    dataloader = DataLoader(
        dataset,
        batch_size=train_batch_size,
        shuffle=True,
        num_workers=2
    )
    
    # 6. è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        pipe.transformer.parameters(),
        lr=learning_rate
    )
    
    # 7. å‡†å¤‡è®­ç»ƒ
    pipe.transformer, optimizer, dataloader = accelerator.prepare(
        pipe.transformer, optimizer, dataloader
    )
    
    # 8. è®­ç»ƒå¾ªç¯
    num_update_steps_per_epoch = len(dataloader) // gradient_accumulation_steps
    max_train_steps = num_train_epochs * num_update_steps_per_epoch
    
    print(f"\nğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   è®­ç»ƒè½®æ•°: {num_train_epochs}")
    print(f"   æ€»æ­¥æ•°: {max_train_steps}")
    print(f"   æ‰¹æ¬¡å¤§å°: {train_batch_size}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   åˆ†è¾¨ç‡: {resolution}x{resolution}")
    print(f"   ç²¾åº¦: {'bf16' if use_bf16 else 'fp16'}")
    
    global_step = 0
    progress_bar = tqdm(range(max_train_steps), desc="è®­ç»ƒä¸­")
    
    pipe.transformer.train()
    
    for epoch in range(num_train_epochs):
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(pipe.transformer):
                device = next(pipe.transformer.parameters()).device
                
                # VAE ç¼–ç 
                pixel_values = batch['pixel_values'].to(device, dtype=torch.bfloat16 if use_bf16 else torch.float16)
                with torch.no_grad():
                    latents = pipe.vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * pipe.vae.config.scaling_factor
                
                # Flow Matching å™ªå£°æ·»åŠ 
                noise = torch.randn_like(latents, device=device, dtype=latents.dtype)
                timesteps = torch.rand(
                    (latents.shape[0],),
                    device=device,
                    dtype=latents.dtype
                )  # [0, 1]
                
                # Flow Matching: x_t = (1-t)*x_0 + t*x_1
                t = timesteps.view(-1, 1, 1, 1)
                noisy_latents = (1 - t) * latents + t * noise
                
                # ç¼–ç æç¤ºè¯ï¼ˆFlux åŒç¼–ç å™¨ï¼‰
                input_ids = batch['input_ids'].to(device)
                with torch.no_grad():
                    if hasattr(pipe, 'text_encoder_1') and hasattr(pipe, 'text_encoder_2'):
                        prompt_embeds_1 = pipe.text_encoder_1(input_ids)[0]
                        prompt_embeds_2 = pipe.text_encoder_2(input_ids)[0]
                        encoder_hidden_states = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)
                    else:
                        raise ValueError("æ— æ³•æ‰¾åˆ° Flux åŒç¼–ç å™¨")
                
                # é¢„æµ‹ï¼ˆä½¿ç”¨ pipe çš„æ ‡å‡†æ–¹æ³•ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨ pipe çš„æ ‡å‡†è°ƒç”¨æ–¹å¼
                # Flux transformer çš„è¾“å…¥æ ¼å¼ç”± pipe å†…éƒ¨å¤„ç†
                try:
                    # ä½¿ç”¨ pipe çš„æ ‡å‡†æ–¹æ³•è°ƒç”¨ transformer
                    # è¿™éœ€è¦æ­£ç¡®çš„è¾“å…¥æ ¼å¼
                    model_pred = pipe.transformer(
                        hidden_states=noisy_latents,
                        timestep=timesteps,
                        encoder_hidden_states=encoder_hidden_states,
                    ).sample
                except RuntimeError as e:
                    if "shapes cannot be multiplied" in str(e) or "mat1 and mat2" in str(e):
                        print(f"\nâŒ Flux transformer è¾“å…¥æ ¼å¼é—®é¢˜")
                        print(f"ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ diffusers å®˜æ–¹è®­ç»ƒè„šæœ¬")
                        print(f"   å‚è€ƒ: https://github.com/huggingface/diffusers/tree/main/examples/flux")
                        raise RuntimeError(
                            f"Flux transformer è¾“å…¥æ ¼å¼å¤æ‚ï¼Œå»ºè®®ä½¿ç”¨å®˜æ–¹è®­ç»ƒè„šæœ¬ã€‚"
                            f"é”™è¯¯: {e}"
                        ) from e
                    raise
                
                # è®¡ç®—æŸå¤±ï¼ˆFlow Matchingï¼šé€Ÿåº¦åœºï¼‰
                target_velocity = noise - latents
                loss = torch.nn.functional.mse_loss(model_pred.float(), target_velocity.float())
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                
                # æ›´æ–°è¿›åº¦æ¡
                if step % 10 == 0:
                    progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
            
            global_step += 1
            progress_bar.update(1)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if global_step % save_steps == 0:
                if accelerator.is_main_process:
                    checkpoint_dir = Path(output_dir) / f"checkpoint-{global_step}"
                    checkpoint_dir.mkdir(parents=True, exist_ok=True)
                    pipe.transformer.save_pretrained(str(checkpoint_dir))
                    print(f"\nğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_dir}")
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {output_dir}")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    if accelerator.is_main_process:
        pipe.transformer.save_pretrained(str(output_path))
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è®­ç»ƒ Flux LoRAï¼ˆdiffusers å®˜æ–¹æ–¹æ³•ï¼‰")
    parser.add_argument("--data-dir", type=str, default="train_data/host_person")
    parser.add_argument("--output-dir", type=str, default="models/lora/host_person")
    parser.add_argument("--base-model", type=str, default="models/flux1-dev")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch-size", type=int, default=1)
    parser.add_argument("--gradient-accumulation", type=int, default=4)
    parser.add_argument("--learning-rate", type=float, default=1e-4)
    parser.add_argument("--lora-rank", type=int, default=32)
    parser.add_argument("--lora-alpha", type=int, default=16)
    parser.add_argument("--save-steps", type=int, default=200)
    parser.add_argument("--resolution", type=int, default=1024)
    parser.add_argument("--use-bf16", action="store_true", help="ä½¿ç”¨ bf16ï¼ˆH20 æ”¯æŒï¼‰")
    
    args = parser.parse_args()
    
    train_flux_lora(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        base_model_path=args.base_model,
        num_train_epochs=args.epochs,
        train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        save_steps=args.save_steps,
        resolution=args.resolution,
        use_bf16=args.use_bf16,
    )


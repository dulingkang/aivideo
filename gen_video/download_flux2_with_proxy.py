#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FLUX.2-dev æ¨¡å‹ä¸‹è½½è„šæœ¬ï¼ˆä½¿ç”¨ proxychains4 ä»£ç†ï¼Œè§£å†³100%å¡ä½é—®é¢˜ï¼‰
"""

import os
import sys
import time
import subprocess
from pathlib import Path
from huggingface_hub import snapshot_download, HfApi
import signal

# å…¨å±€å˜é‡
download_interrupted = False

def signal_handler(sig, frame):
    """å¤„ç†ä¸­æ–­ä¿¡å·"""
    global download_interrupted
    print("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
    print("   â„¹ å·²ä¸‹è½½çš„æ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹è½½")
    download_interrupted = True
    sys.exit(0)

def check_proxychains4():
    """æ£€æŸ¥ proxychains4 æ˜¯å¦å¯ç”¨"""
    try:
        result = subprocess.run(
            ["which", "proxychains4"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            return result.stdout.strip()
        return None
    except Exception:
        return None

def check_existing_files(model_dir: Path):
    """æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶"""
    if not model_dir.exists():
        return 0, []
    
    safetensors_files = list(model_dir.rglob("*.safetensors"))
    bin_files = list(model_dir.rglob("*.bin"))
    pt_files = list(model_dir.rglob("*.pt"))
    all_files = safetensors_files + bin_files + pt_files
    
    total_size = sum(f.stat().st_size for f in all_files if f.is_file())
    size_gb = total_size / (1024 ** 3)
    
    return size_gb, all_files

def download_with_proxy(model_id: str, local_dir: Path, max_retries: int = 3):
    """ä½¿ç”¨ proxychains4 ä¸‹è½½æ¨¡å‹"""
    global download_interrupted
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    print("=" * 60)
    print("ğŸ“¥ FLUX.2-dev æ¨¡å‹ä¸‹è½½ï¼ˆä½¿ç”¨ proxychains4 ä»£ç†ï¼‰")
    print("=" * 60)
    print(f"æ¨¡å‹ID: {model_id}")
    print(f"ä¿å­˜ç›®å½•: {local_dir}")
    print()
    
    # æ£€æŸ¥ proxychains4
    proxychains_path = check_proxychains4()
    if not proxychains_path:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° proxychains4")
        print("   è¯·å®‰è£…: sudo apt install proxychains4")
        print("   æˆ–ç¡®ä¿ proxychains4 åœ¨ PATH ä¸­")
        return False
    
    print(f"âœ… æ‰¾åˆ° proxychains4: {proxychains_path}")
    print()
    
    # æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡ä»¶
    existing_size, existing_files = check_existing_files(local_dir)
    if existing_size > 0:
        print(f"âœ… å‘ç°å·²ä¸‹è½½çš„æ–‡ä»¶: {existing_size:.2f} GB ({len(existing_files)} ä¸ªæ–‡ä»¶)")
        print("   â„¹ å°†è‡ªåŠ¨ç»­ä¼ ï¼Œä¸ä¼šé‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ–‡ä»¶")
        print()
    
    # åˆ›å»ºç›®å½•
    local_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä¼˜åŒ–ä¸‹è½½
    env = os.environ.copy()
    env["HF_HUB_ENABLE_HF_TRANSFER"] = "1"  # å¯ç”¨ hf_transfer åŠ é€Ÿ
    env["HF_HUB_DOWNLOAD_TIMEOUT"] = "3600"  # è®¾ç½®è¶…æ—¶ï¼ˆ1å°æ—¶ï¼‰
    
    # ä¸‹è½½é…ç½®
    download_kwargs = {
        "repo_id": model_id,
        "local_dir": str(local_dir),
        "local_dir_use_symlinks": False,
        "resume_download": True,
        "max_workers": 2,  # å‡å°‘å¹¶å‘
    }
    
    # é‡è¯•ä¸‹è½½
    for attempt in range(1, max_retries + 1):
        try:
            if attempt > 1:
                print(f"â³ é‡è¯•ä¸‹è½½ ({attempt}/{max_retries})...")
                time.sleep(5)
            else:
                print("â³ å¼€å§‹ä¸‹è½½ï¼ˆä½¿ç”¨ proxychains4 ä»£ç†ï¼‰...")
                print("   ğŸ’¡ æç¤º: æŒ‰ Ctrl+C å¯ä»¥å®‰å…¨ä¸­æ–­ï¼Œå·²ä¸‹è½½çš„æ–‡ä»¶ä¼šä¿ç•™")
                print("   ğŸ’¡ å¦‚æœä¸‹è½½åˆ°100%åå¡ä½ï¼Œå¯èƒ½æ˜¯éªŒè¯é˜¶æ®µï¼Œè¯·è€å¿ƒç­‰å¾…æˆ–æŒ‰ Ctrl+C ä¸­æ–­")
                print()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ proxychains4 ç¯å¢ƒä¸­
            # proxychains4 ä¼šè®¾ç½® LD_PRELOADï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥è¿™ä¸ª
            is_proxychains = "PROXYCHAINS_CONF_FILE" in os.environ or \
                           any("proxychains" in str(v).lower() for v in os.environ.values())
            
            if not is_proxychains and "HTTP_PROXY" not in env and "HTTPS_PROXY" not in env:
                print("   âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° proxychains4 ç¯å¢ƒæˆ–ä»£ç†è®¾ç½®")
                print("   ğŸ’¡ å»ºè®®: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œ:")
                print(f"      proxychains4 -q python {sys.argv[0]}")
                print()
                print("   æˆ–è€…è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡:")
                print("      export HTTP_PROXY=your_proxy")
                print("      export HTTPS_PROXY=your_proxy")
                print()
                response = input("   æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
                if response.lower() != 'y':
                    return False
            
            # è®¾ç½®ä¸‹è½½è¶…æ—¶å’Œé‡è¯•å‚æ•°ï¼Œé¿å…100%åå¡ä½
            # é€šè¿‡è®¾ç½®è¾ƒå°çš„ chunk_size å’Œè¶…æ—¶æ¥é¿å…å¡ä½
            print("   â³ å¼€å§‹ä¸‹è½½...")
            print("   ğŸ’¡ å¦‚æœä¸‹è½½åˆ°100%åå¡ä½ï¼Œå¯èƒ½æ˜¯éªŒè¯é˜¶æ®µï¼Œè¯·ç­‰å¾…æˆ–æŒ‰ Ctrl+C ä¸­æ–­")
            print()
            
            # ç›´æ¥ä½¿ç”¨ snapshot_downloadï¼ˆå‡è®¾å·²ç»åœ¨ proxychains4 ç¯å¢ƒä¸­è¿è¡Œï¼‰
            # è®¾ç½®è¶…æ—¶é¿å…æ— é™ç­‰å¾…
            snapshot_download(**download_kwargs)
            
            if download_interrupted:
                print("\nâš ï¸  ä¸‹è½½è¢«ä¸­æ–­ï¼Œä½†å·²ä¸‹è½½çš„æ–‡ä»¶å·²ä¿å­˜")
                return False
            
            print()
            print("âœ… ä¸‹è½½å®Œæˆï¼")
            return True
            
        except KeyboardInterrupt:
            print("\nâš ï¸  ä¸‹è½½è¢«ç”¨æˆ·ä¸­æ–­")
            print("   â„¹ å·²ä¸‹è½½çš„æ–‡ä»¶å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹è½½")
            return False
            
        except Exception as e:
            error_msg = str(e)
            print(f"\nâŒ ä¸‹è½½å¤±è´¥: {error_msg}")
            
            if any(keyword in error_msg.lower() for keyword in ["timeout", "connection", "network", "socket"]):
                if attempt < max_retries:
                    print(f"   â¸ï¸  ç½‘ç»œé”™è¯¯ï¼Œ5ç§’åé‡è¯•...")
                    time.sleep(5)
                    continue
                else:
                    print(f"   âŒ å·²é‡è¯• {max_retries} æ¬¡ï¼Œä»ç„¶å¤±è´¥")
                    print("   ğŸ’¡ å»ºè®®:")
                    print("      1. æ£€æŸ¥ proxychains4 é…ç½®: /etc/proxychains4.conf")
                    print("      2. ç¡®ä¿ä»£ç†æœåŠ¡æ­£åœ¨è¿è¡Œ")
                    print("      3. ä½¿ç”¨å‘½ä»¤: proxychains4 -q python download_flux2_with_proxy.py")
                    return False
            else:
                if attempt < max_retries:
                    print(f"   â¸ï¸  5ç§’åé‡è¯•...")
                    time.sleep(5)
                    continue
                else:
                    raise
    
    return False

def verify_download(model_dir: Path):
    """éªŒè¯ä¸‹è½½æ˜¯å¦å®Œæ•´"""
    print()
    print("=" * 60)
    print("ğŸ” éªŒè¯ä¸‹è½½å®Œæ•´æ€§")
    print("=" * 60)
    
    required_files = [
        "model_index.json",
        "transformer/diffusion_pytorch_model-00001-of-00003.safetensors",
        "vae/diffusion_pytorch_model.safetensors",
    ]
    
    all_exist = True
    for req_file in required_files:
        file_path = model_dir / req_file
        if file_path.exists():
            size = file_path.stat().st_size / (1024 ** 3)
            print(f"âœ… {req_file} ({size:.2f} GB)")
        else:
            print(f"âŒ {req_file} (ç¼ºå¤±)")
            all_exist = False
    
    total_size = sum(
        f.stat().st_size 
        for f in model_dir.rglob("*") 
        if f.is_file()
    ) / (1024 ** 3)
    
    print()
    print(f"ğŸ“Š æ€»å¤§å°: {total_size:.2f} GB")
    
    if all_exist and total_size > 50:
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæ•´ï¼Œå¯ä»¥ä½¿ç”¨")
        return True
    elif total_size > 50:
        print("âš ï¸  æ¨¡å‹æ–‡ä»¶è¾ƒå¤§ï¼Œä½†å¯èƒ½ç¼ºå°‘éƒ¨åˆ†æ–‡ä»¶")
        return False
    else:
        print("âŒ æ¨¡å‹ä¸‹è½½ä¸å®Œæ•´")
        return False

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¸‹è½½ FLUX.2-dev æ¨¡å‹ï¼ˆä½¿ç”¨ proxychains4 ä»£ç†ï¼‰")
    parser.add_argument(
        "--model-dir",
        type=str,
        default="/vepfs-dev/shawn/vid/fanren/gen_video/models/flux2-dev",
        help="æ¨¡å‹ä¿å­˜ç›®å½•"
    )
    parser.add_argument(
        "--max-retries",
        type=int,
        default=5,
        help="æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰"
    )
    
    args = parser.parse_args()
    
    model_id = "black-forest-labs/FLUX.2-dev"
    local_dir = Path(args.model_dir)
    
    # ä¸‹è½½æ¨¡å‹
    success = download_with_proxy(
        model_id=model_id,
        local_dir=local_dir,
        max_retries=args.max_retries
    )
    
    # éªŒè¯ä¸‹è½½
    if success:
        verify_download(local_dir)
    else:
        print()
        print("=" * 60)
        print("ğŸ’¡ ä¸‹è½½æœªå®Œæˆï¼Œä½†å·²ä¸‹è½½çš„æ–‡ä»¶å·²ä¿å­˜")
        print("=" * 60)
        print("é‡æ–°è¿è¡Œæ­¤è„šæœ¬å¯ä»¥ç»§ç»­ä¸‹è½½ï¼ˆè‡ªåŠ¨æ–­ç‚¹ç»­ä¼ ï¼‰")
        print()
        print("âš ï¸  é‡è¦: è¯·ä½¿ç”¨ proxychains4 è¿è¡Œ:")
        print(f"   proxychains4 -q python {sys.argv[0]} --model-dir {local_dir}")
        print()
        
        existing_size, existing_files = check_existing_files(local_dir)
        if existing_size > 0:
            print(f"å½“å‰å·²ä¸‹è½½: {existing_size:.2f} GB ({len(existing_files)} ä¸ªæ–‡ä»¶)")

if __name__ == "__main__":
    main()


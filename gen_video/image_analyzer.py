#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾åƒåˆ†æå·¥å…·
å¯¹æ¯”ç”Ÿæˆçš„promptå’Œå®é™…å›¾ç‰‡ï¼Œæ‰¾å‡ºå¯ä»¥ä¼˜åŒ–çš„åœ°æ–¹
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from PIL import Image
import torch


class ImageAnalyzer:
    """å›¾åƒåˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.clip_model = None
        self.clip_processor = None
        self.clip_tokenizer = None
        self._load_clip_model()
    
    def _load_clip_model(self):
        """åŠ è½½CLIPæ¨¡å‹ç”¨äºå›¾åƒåˆ†æ"""
        try:
            from transformers import CLIPProcessor, CLIPModel
            import torch
            
            print("  åŠ è½½CLIPæ¨¡å‹ç”¨äºå›¾åƒåˆ†æ...")
            model_name = "openai/clip-vit-large-patch14"
            self.clip_model = CLIPModel.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")
            self.clip_processor = CLIPProcessor.from_pretrained(model_name)
            print("  âœ“ CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"  âš  CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("  â„¹ å°†ä½¿ç”¨åŸºç¡€å›¾åƒåˆ†æï¼ˆä¸ä¾èµ–CLIPï¼‰")
            self.clip_model = None
    
    def analyze_image(
        self,
        image_path: str,
        prompt: str,
        scene: Optional[Dict[str, Any]] = None,
        actual_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æå›¾åƒï¼Œå¯¹æ¯”promptå’Œå®é™…å†…å®¹
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            prompt: ç”Ÿæˆçš„prompt
            scene: åœºæ™¯JSONæ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            {
                "prompt_analysis": Dict,  # Promptåˆ†æç»“æœ
                "image_analysis": Dict,  # å›¾åƒåˆ†æç»“æœ
                "comparison": Dict,  # å¯¹æ¯”åˆ†æç»“æœ
                "suggestions": List[str],  # ä¼˜åŒ–å»ºè®®
            }
        """
        print(f"\nåˆ†æå›¾åƒ: {os.path.basename(image_path)}")
        
        # ä½¿ç”¨å®é™…promptï¼ˆå¦‚æœæä¾›ï¼‰
        prompt_to_analyze = actual_prompt if actual_prompt else prompt
        print(f"Prompt: {prompt_to_analyze[:100]}...")
        
        # 1. åˆ†æPrompt
        prompt_analysis = self._analyze_prompt(prompt_to_analyze, scene)
        
        # 2. åˆ†æå›¾åƒ
        image_analysis = self._analyze_image_content(image_path, prompt_to_analyze)
        
        # 3. å¯¹æ¯”åˆ†æ
        comparison = self._compare_prompt_and_image(prompt_analysis, image_analysis)
        
        # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        suggestions = self._generate_suggestions(prompt_analysis, image_analysis, comparison, scene)
        
        return {
            "prompt_analysis": prompt_analysis,
            "image_analysis": image_analysis,
            "comparison": comparison,
            "suggestions": suggestions,
        }
    
    def _analyze_prompt(self, prompt: str, scene: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """åˆ†æPromptå†…å®¹"""
        prompt_lower = prompt.lower()
        
        # æå–å…³é”®å…ƒç´ 
        # æ³¨æ„ï¼šå¦‚æœpromptä¸­æœ‰"no person"æˆ–"no character"ï¼Œåˆ™ä¸åº”è¯¥è®¤ä¸ºhas_character=True
        has_exclusion = any(kw in prompt_lower for kw in ["no person", "no character", "no human", "no people", "æ— äººç‰©", "æ— è§’è‰²", "æ— äºº"])
        elements = {
            "has_character": not has_exclusion and any(kw in prompt_lower for kw in ["character", "person", "han li", "éŸ©ç«‹", "äººç‰©", "è§’è‰²"]),
            "has_object": any(kw in prompt_lower for kw in ["object", "scroll", "item", "ç‰©ä½“", "ç‰©å“", "å·è½´"]),
            "has_environment": any(kw in prompt_lower for kw in ["sky", "desert", "ground", "gravel", "sand", "å¤©ç©º", "æ²™æ¼ ", "åœ°é¢", "ç ‚ç ¾", "æ²™ç ¾"]),
            "viewpoint": self._extract_viewpoint(prompt_lower),
            "shot_type": self._extract_shot_type(prompt_lower),
            "action_type": self._extract_action_type(prompt_lower),
            "facing_direction": self._extract_facing_direction(prompt_lower),
        }
        
        # ä»sceneä¸­æå–æœŸæœ›å†…å®¹
        expected = {}
        if scene:
            visual = scene.get("visual", {})
            if isinstance(visual, dict):
                expected["character_pose"] = visual.get("character_pose", "")
                expected["composition"] = visual.get("composition", "")
                expected["camera"] = scene.get("camera", "")
                expected["action"] = scene.get("action", "")
        
        return {
            "elements": elements,
            "expected": expected,
            "prompt_text": prompt,
        }
    
    def _analyze_image_content(self, image_path: str, prompt: str) -> Dict[str, Any]:
        """åˆ†æå›¾åƒå†…å®¹"""
        if not os.path.exists(image_path):
            return {"error": "å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨"}
        
        image = Image.open(image_path)
        width, height = image.size
        
        # åŸºç¡€å›¾åƒåˆ†æ
        analysis = {
            "dimensions": {"width": width, "height": height},
            "aspect_ratio": width / height,
        }
        
        # ä½¿ç”¨CLIPåˆ†æå›¾åƒå†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.clip_model is not None:
            clip_analysis = self._analyze_with_clip(image, prompt)
            analysis.update(clip_analysis)
        else:
            # åŸºç¡€åˆ†æï¼ˆä¸ä¾èµ–CLIPï¼‰
            analysis["detected_elements"] = self._basic_image_analysis(image)
        
        return analysis
    
    def _analyze_with_clip(self, image: Image.Image, prompt: str) -> Dict[str, Any]:
        """ä½¿ç”¨CLIPæ¨¡å‹åˆ†æå›¾åƒ"""
        try:
            # å®šä¹‰æ£€æµ‹é¡¹
            check_items = [
                "character facing camera",
                "character from behind",
                "character side view",
                "close-up shot",
                "wide shot",
                "medium shot",
                "golden scroll",
                "desert background",
                "sky background",
                "character with action",
                "static character",
            ]
            
            # ä½¿ç”¨CLIPè®¡ç®—ç›¸ä¼¼åº¦
            inputs = self.clip_processor(
                text=check_items,
                images=image,
                return_tensors="pt",
                padding=True
            ).to(self.clip_model.device)
            
            with torch.no_grad():
                outputs = self.clip_model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)
            
            # æå–é«˜ç›¸ä¼¼åº¦çš„é¡¹
            detected = {}
            for i, item in enumerate(check_items):
                score = probs[0][i].item()
                if score > 0.1:  # é˜ˆå€¼
                    detected[item] = score
            
            return {
                "detected_elements": detected,
                "primary_element": max(detected.items(), key=lambda x: x[1])[0] if detected else None,
            }
        except Exception as e:
            print(f"  âš  CLIPåˆ†æå¤±è´¥: {e}")
            return {"detected_elements": {}}
    
    def _basic_image_analysis(self, image: Image.Image) -> Dict[str, float]:
        """åŸºç¡€å›¾åƒåˆ†æï¼ˆä¸ä¾èµ–CLIPï¼‰"""
        # ç®€å•çš„å›¾åƒåˆ†æ
        # è¿™é‡Œå¯ä»¥æ·»åŠ åŸºäºåƒç´ çš„åˆ†æï¼Œå¦‚æ£€æµ‹ä¸»è¦é¢œè‰²ã€äº®åº¦ç­‰
        return {
            "analysis_method": "basic",
            "note": "éœ€è¦CLIPæ¨¡å‹è¿›è¡Œè¯¦ç»†åˆ†æ"
        }
    
    def _extract_viewpoint(self, prompt_lower: str) -> str:
        """ä»promptä¸­æå–è§†è§’"""
        if any(kw in prompt_lower for kw in ["facing camera", "front view", "æ­£é¢", "é¢å‘é•œå¤´"]):
            return "front"
        elif any(kw in prompt_lower for kw in ["back view", "from behind", "èƒŒå½±", "èƒŒå"]):
            return "back"
        elif any(kw in prompt_lower for kw in ["side view", "profile", "ä¾§é¢", "ä¾§èº«"]):
            return "side"
        elif any(kw in prompt_lower for kw in ["top-down", "aerial", "ä¿¯è§†", "é¸Ÿç°"]):
            return "top"
        else:
            return "unknown"
    
    def _extract_shot_type(self, prompt_lower: str) -> str:
        """ä»promptä¸­æå–é•œå¤´ç±»å‹"""
        if any(kw in prompt_lower for kw in ["close-up", "extreme close-up", "ç‰¹å†™", "è¿‘æ™¯"]):
            return "close-up"
        elif any(kw in prompt_lower for kw in ["wide shot", "distant", "è¿œæ™¯", "è¿œè·ç¦»"]):
            return "wide"
        elif any(kw in prompt_lower for kw in ["medium shot", "ä¸­æ™¯", "åŠèº«"]):
            return "medium"
        else:
            return "unknown"
    
    def _extract_action_type(self, prompt_lower: str) -> str:
        """ä»promptä¸­æå–åŠ¨ä½œç±»å‹"""
        if any(kw in prompt_lower for kw in ["attack", "fight", "run", "jump", "æ”»å‡»", "æˆ˜æ–—", "å¥”è·‘", "è·³è·ƒ"]):
            return "dynamic"
        elif any(kw in prompt_lower for kw in ["tilt", "turn", "move", "ä¾§", "è½¬", "ç§»åŠ¨"]):
            return "moderate"
        elif any(kw in prompt_lower for kw in ["still", "motionless", "é™æ­¢", "ä¸åŠ¨"]):
            return "static"
        else:
            return "unknown"
    
    def _extract_facing_direction(self, prompt_lower: str) -> str:
        """ä»promptä¸­æå–æœå‘"""
        if any(kw in prompt_lower for kw in ["facing camera", "front view", "æ­£é¢", "é¢å‘é•œå¤´"]):
            return "front"
        elif any(kw in prompt_lower for kw in ["back view", "from behind", "èƒŒå½±", "èƒŒå"]):
            return "back"
        else:
            return "unknown"
    
    def _compare_prompt_and_image(
        self,
        prompt_analysis: Dict[str, Any],
        image_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å¯¹æ¯”Promptå’Œå›¾åƒ"""
        comparison = {
            "matches": [],
            "mismatches": [],
            "missing": [],
            "extra": [],
        }
        
        prompt_elements = prompt_analysis.get("elements", {})
        image_elements = image_analysis.get("detected_elements", {})
        
        # æ£€æŸ¥è§†è§’åŒ¹é…
        expected_viewpoint = prompt_elements.get("viewpoint")
        if expected_viewpoint != "unknown":
            if "character facing camera" in image_elements and expected_viewpoint == "front":
                comparison["matches"].append("è§†è§’ï¼šæ­£é¢")
            elif "character from behind" in image_elements and expected_viewpoint == "back":
                comparison["matches"].append("è§†è§’ï¼šèƒŒé¢")
            elif expected_viewpoint == "front" and "character from behind" in image_elements:
                comparison["mismatches"].append("è§†è§’ä¸åŒ¹é…ï¼šæœŸæœ›æ­£é¢ï¼Œå®é™…èƒŒé¢")
            elif expected_viewpoint == "back" and "character facing camera" in image_elements:
                comparison["mismatches"].append("è§†è§’ä¸åŒ¹é…ï¼šæœŸæœ›èƒŒé¢ï¼Œå®é™…æ­£é¢")
        
        # æ£€æŸ¥é•œå¤´ç±»å‹åŒ¹é…
        expected_shot = prompt_elements.get("shot_type")
        if expected_shot != "unknown":
            if "close-up shot" in image_elements and expected_shot == "close-up":
                comparison["matches"].append("é•œå¤´ç±»å‹ï¼šç‰¹å†™")
            elif "wide shot" in image_elements and expected_shot == "wide":
                comparison["matches"].append("é•œå¤´ç±»å‹ï¼šè¿œæ™¯")
            elif "medium shot" in image_elements and expected_shot == "medium":
                comparison["matches"].append("é•œå¤´ç±»å‹ï¼šä¸­æ™¯")
        
        # æ£€æŸ¥è§’è‰²å­˜åœ¨
        if prompt_elements.get("has_character"):
            # æ£€æŸ¥å„ç§è§’è‰²è§†è§’ï¼šæ­£é¢ã€èƒŒé¢ã€ä¾§é¢ã€åŠ¨ä½œã€é™æ€
            character_detected = (
                "character facing camera" in image_elements or 
                "character from behind" in image_elements or
                "character side view" in image_elements or
                "character with action" in image_elements or
                "static character" in image_elements
            )
            if character_detected:
                comparison["matches"].append("è§’è‰²å­˜åœ¨")
            else:
                comparison["missing"].append("è§’è‰²æœªæ£€æµ‹åˆ°")
        
        # æ£€æŸ¥ç‰©ä½“å­˜åœ¨
        if prompt_elements.get("has_object"):
            if "golden scroll" in image_elements:
                comparison["matches"].append("ç‰©ä½“å­˜åœ¨ï¼šå·è½´")
            else:
                comparison["missing"].append("ç‰©ä½“æœªæ£€æµ‹åˆ°ï¼šå·è½´")
        
        return comparison
    
    def _generate_suggestions(
        self,
        prompt_analysis: Dict[str, Any],
        image_analysis: Dict[str, Any],
        comparison: Dict[str, Any],
        scene: Optional[Dict[str, Any]] = None
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºä¸åŒ¹é…é¡¹ç”Ÿæˆå»ºè®®
        for mismatch in comparison.get("mismatches", []):
            if "è§†è§’ä¸åŒ¹é…" in mismatch:
                if "æœŸæœ›æ­£é¢ï¼Œå®é™…èƒŒé¢" in mismatch:
                    suggestions.append("âš  è§†è§’é—®é¢˜ï¼šæœŸæœ›æ­£é¢ä½†ç”Ÿæˆäº†èƒŒé¢ã€‚å»ºè®®åœ¨promptä¸­å¢åŠ  '(facing camera, front view:1.8)' å¹¶æ·»åŠ è´Ÿé¢æç¤º 'back view, from behind'")
                elif "æœŸæœ›èƒŒé¢ï¼Œå®é™…æ­£é¢" in mismatch:
                    suggestions.append("â„¹ è§†è§’é—®é¢˜ï¼šæœŸæœ›èƒŒé¢ä½†ç”Ÿæˆäº†æ­£é¢ã€‚å¦‚æœç¡®å®éœ€è¦èƒŒé¢ï¼Œå»ºè®®åœ¨promptä¸­æ˜ç¡®æ·»åŠ  'back view, from behind'")
        
        # åŸºäºç¼ºå¤±é¡¹ç”Ÿæˆå»ºè®®
        for missing in comparison.get("missing", []):
            if "è§’è‰²æœªæ£€æµ‹åˆ°" in missing:
                suggestions.append("âš  è§’è‰²ç¼ºå¤±ï¼špromptä¸­æè¿°äº†è§’è‰²ä½†å›¾åƒä¸­æœªæ£€æµ‹åˆ°ã€‚å»ºè®®æ£€æŸ¥promptä¸­è§’è‰²æè¿°çš„æƒé‡å’Œä½ç½®")
            elif "ç‰©ä½“æœªæ£€æµ‹åˆ°" in missing:
                suggestions.append("âš  ç‰©ä½“ç¼ºå¤±ï¼špromptä¸­æè¿°äº†ç‰©ä½“ä½†å›¾åƒä¸­æœªæ£€æµ‹åˆ°ã€‚å»ºè®®åœ¨promptæœ€å‰é¢æ·»åŠ ç‰©ä½“æè¿°ï¼Œä½¿ç”¨é«˜æƒé‡ï¼ˆå¦‚2.0ï¼‰")
        
        # åŸºäºé•œå¤´ç±»å‹ç”Ÿæˆå»ºè®®
        prompt_elements = prompt_analysis.get("elements", {})
        expected_shot = prompt_elements.get("shot_type")
        if expected_shot == "close-up" and "close-up shot" not in image_analysis.get("detected_elements", {}):
            suggestions.append("âš  é•œå¤´è·ç¦»é—®é¢˜ï¼šæœŸæœ›ç‰¹å†™ä½†å®é™…å¯èƒ½æ˜¯ä¸­æ™¯æˆ–è¿œæ™¯ã€‚å»ºè®®åœ¨promptä¸­æ˜ç¡®æ·»åŠ  'extreme close-up' æˆ– 'close-up' å¹¶æé«˜æƒé‡")
        elif expected_shot == "wide" and "wide shot" not in image_analysis.get("detected_elements", {}):
            suggestions.append("âš  é•œå¤´è·ç¦»é—®é¢˜ï¼šæœŸæœ›è¿œæ™¯ä½†å®é™…å¯èƒ½æ˜¯ä¸­æ™¯æˆ–ç‰¹å†™ã€‚å»ºè®®åœ¨promptä¸­æ˜ç¡®æ·»åŠ  'wide shot' æˆ– 'distant view' å¹¶æé«˜æƒé‡")
        
        # åŸºäºåŠ¨ä½œç±»å‹ç”Ÿæˆå»ºè®®
        action_type = prompt_elements.get("action_type")
        if action_type == "dynamic" and "character with action" not in image_analysis.get("detected_elements", {}):
            suggestions.append("â„¹ åŠ¨ä½œé—®é¢˜ï¼šæœŸæœ›åŠ¨æ€åŠ¨ä½œä½†å¯èƒ½ä¸å¤Ÿæ˜æ˜¾ã€‚å»ºè®®åœ¨promptä¸­æ˜ç¡®æè¿°åŠ¨ä½œç»†èŠ‚ï¼Œå¦‚ 'attacking', 'running' ç­‰")
        
        # åŸºäºåœºæ™¯æ•°æ®ç”Ÿæˆå»ºè®®
        if scene:
            visual = scene.get("visual", {})
            if isinstance(visual, dict):
                character_pose = visual.get("character_pose", "")
                if character_pose and "facing camera" in character_pose.lower():
                    # å¦‚æœæœŸæœ›æ­£é¢ä½†å®é™…æ˜¯èƒŒé¢
                    if "character from behind" in image_analysis.get("detected_elements", {}):
                        suggestions.append("ğŸ”´ å…³é”®é—®é¢˜ï¼šcharacter_poseä¸­æŒ‡å®šäº†'facing camera'ä½†ç”Ÿæˆäº†èƒŒé¢ã€‚å»ºè®®ï¼š1) å¢åŠ æ­£é¢æœå‘æƒé‡è‡³1.8ï¼›2) æ·»åŠ è´Ÿé¢æç¤º'back view, from behind'ï¼›3) æ£€æŸ¥promptä¸­æ˜¯å¦æœ‰å†²çªçš„æè¿°")
        
        # åŸºäºé•œå¤´è·ç¦»ç”Ÿæˆå»ºè®®
        image_detected = image_analysis.get("detected_elements", {})
        if "wide shot" in image_detected and prompt_elements.get("shot_type") == "close-up":
            suggestions.append("ğŸ”´ å…³é”®é—®é¢˜ï¼šæœŸæœ›ç‰¹å†™ä½†ç”Ÿæˆäº†è¿œæ™¯ã€‚å»ºè®®ï¼š1) åœ¨promptæœ€å‰é¢æ·»åŠ  '(extreme close-up:2.0)'ï¼›2) æ·»åŠ è´Ÿé¢æç¤º 'wide shot, distant view'ï¼›3) æ£€æŸ¥cameraå­—æ®µæ˜¯å¦æ­£ç¡®")
        elif "close-up shot" in image_detected and prompt_elements.get("shot_type") == "wide":
            suggestions.append("ğŸ”´ å…³é”®é—®é¢˜ï¼šæœŸæœ›è¿œæ™¯ä½†ç”Ÿæˆäº†ç‰¹å†™ã€‚å»ºè®®ï¼š1) åœ¨promptä¸­æ·»åŠ  '(wide shot, distant view:1.8)'ï¼›2) æ·»åŠ è´Ÿé¢æç¤º 'close-up, extreme close-up'")
        
        return suggestions
    
    def analyze_batch(
        self,
        scenes: List[Dict[str, Any]],
        image_dir: str,
        output_file: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æå›¾åƒ
        
        Args:
            scenes: åœºæ™¯åˆ—è¡¨ï¼ˆåŒ…å«promptå’Œimage_pathï¼‰
            image_dir: å›¾åƒç›®å½•
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        results = []
        
        for i, scene in enumerate(scenes):
            image_path = scene.get("image_path")
            if not image_path:
                continue
            
            # æ„å»ºå®Œæ•´è·¯å¾„
            if not os.path.isabs(image_path):
                full_path = os.path.join(image_dir, image_path)
            else:
                full_path = image_path
            
            if not os.path.exists(full_path):
                print(f"  âš  åœºæ™¯ {i+1}: å›¾åƒä¸å­˜åœ¨: {full_path}")
                continue
            
            # è·å–promptï¼ˆä»sceneæˆ–éœ€è¦é‡æ–°ç”Ÿæˆï¼‰
            prompt = scene.get("prompt") or scene.get("description") or ""
            
            # åˆ†æå›¾åƒ
            try:
                result = self.analyze_image(full_path, prompt, scene)
                result["scene_id"] = scene.get("id", i)
                result["image_path"] = full_path
                results.append(result)
                print(f"  âœ“ åœºæ™¯ {i+1} åˆ†æå®Œæˆ")
            except Exception as e:
                print(f"  âœ— åœºæ™¯ {i+1} åˆ†æå¤±è´¥: {e}")
        
        # ä¿å­˜ç»“æœ
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nâœ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return results
    
    def generate_report(self, results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("å›¾åƒåˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        
        for result in results:
            scene_id = result.get("scene_id", "?")
            image_path = result.get("image_path", "?")
            
            report_lines.append(f"\nåœºæ™¯ {scene_id}: {os.path.basename(image_path)}")
            report_lines.append("-" * 80)
            
            # å¯¹æ¯”ç»“æœ
            comparison = result.get("comparison", {})
            if comparison.get("matches"):
                report_lines.append("âœ“ åŒ¹é…é¡¹:")
                for match in comparison["matches"]:
                    report_lines.append(f"  - {match}")
            
            if comparison.get("mismatches"):
                report_lines.append("âœ— ä¸åŒ¹é…é¡¹:")
                for mismatch in comparison["mismatches"]:
                    report_lines.append(f"  - {mismatch}")
            
            if comparison.get("missing"):
                report_lines.append("âš  ç¼ºå¤±é¡¹:")
                for missing in comparison["missing"]:
                    report_lines.append(f"  - {missing}")
            
            # ä¼˜åŒ–å»ºè®®
            suggestions = result.get("suggestions", [])
            if suggestions:
                report_lines.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for suggestion in suggestions:
                    report_lines.append(f"  {suggestion}")
        
        return "\n".join(report_lines)


# AI视频生成系统配置文件

# 路径配置
paths:
  # 输入目录
  input_dir: 灵界/img2/jpgsrc
  # 输出目录
  output_dir: outputs
  # 临时文件目录
  temp_dir: temp
  # 模型权重目录
  model_dir: checkpoints
  # 自动生成图像输出目录（每个项目会再加子目录）
  image_output: outputs/images

# 视频生成配置
video:
  # 视频方向配置（横屏/竖屏）
  # 可选值：horizontal（横屏，16:9，适合B站/YouTube）、vertical（竖屏，9:16，适合抖音/快手/视频号）
  aspect_ratio: vertical  # 当前使用竖屏模式（抖音/快手/视频号）
  # 注意：更改此配置后，视频和图像分辨率会自动匹配
  # horizontal: 视频 512x384，图像 1536x864（16:9横屏）
  # vertical: 视频 512x768，图像 768x1152（9:16竖屏）
  
  # 使用模型：svd, svd-xt, svd-image-to-video, animatediff, hunyuanvideo
  # 注意：animatediff在diffusers中不稳定（绿色竖条问题），建议使用svd-xt
  # hunyuanvideo: 高质量视频生成，适合高端场景（需要18-24GB显存）
  # 小说推文使用 hunyuanvideo（图生视频，更稳定）
  model_type: hunyuanvideo  # 小说推文使用 hunyuanvideo
  # 模型路径（从HuggingFace下载后放在model_dir中）
  # 注意：svd-image-to-video 模型实际在 svd 目录中（svd_xt_image_decoder.safetensors）
  model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/svd
  
  # HunyuanVideo配置（高端视频生成）
  hunyuanvideo:
    # 使用版本：true=1.5版本（推荐，更轻量质量更好），false=原版
    # 注意：代码会自动从model_index.json检测版本，这里可以保持默认
    use_v15: true  # 使用1.5版本（推荐）
    # 模型路径（如果本地已有，使用本地路径；否则会从HuggingFace下载）
    # 1.5版本（推荐）：
    #   - tencent/HunyuanVideo-1.5（官方版本，推荐）
    #   - hunyuanvideo-community/HunyuanVideo-1.5-480p_i2v（diffusers格式，图生视频）
    # 原版：hunyuanvideo-community/HunyuanVideo-I2V（图生视频）
    # 推荐使用diffusers格式模型（完整pipeline，包含所有组件）
    # 分辨率选择：
    #   - 480p模型：适合640×480或更低分辨率，速度快，显存占用少
    #   - 720p模型：适合960×720或1280×720，质量更高，细节更清晰（推荐用于1280×768）
    # 使用本地已下载的模型
    # ✅ 当前使用：蒸馏版本（可以用更少步数达到同样效果）
    model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/HunyuanVideo-1.5-Diffusers-480p_i2v_step_distilled  # 蒸馏版，速度快，步数少
    # 或使用HuggingFace自动下载: hunyuanvideo-community/HunyuanVideo-1.5-480p_i2v_step_distilled
    # 其他可选模型：
    #   - hunyuanvideo-community/HunyuanVideo-1.5-480p_i2v（480p图生视频，标准版）
    #   - hunyuanvideo-community/HunyuanVideo-1.5-720p_i2v（720p图生视频，高质量）
    #   - hunyuanvideo-community/HunyuanVideo-1.5-480p_t2v（480p文生视频）
    #   - hunyuanvideo-community/HunyuanVideo-1.5-480p_i2v_step_distilled（蒸馏版，推荐）✅ 当前使用
    transformer_subfolder: null  # diffusers格式模型不需要指定transformer_subfolder
    num_frames: 24  # 帧数（降低帧数以减少生成时的显存占用，attention计算需要大量显存，先降低到15帧测试）
    # 注意：HunyuanVideo 1.5实际生成的帧数可能少于请求的帧数（约45-60帧）
    # 系统会自动使用插帧补充到目标帧数，确保视频时长达到预期
    # 建议：
    #   - 120帧：目标5秒（实际生成45帧+插帧=120帧）
    #   - 80帧：目标3.3秒（实际生成45帧+插帧=80帧）
    #   - 60帧：目标2.5秒（实际生成45帧+插帧=60帧）
    fps: 24
    # 分辨率（根据 aspect_ratio 配置自动设置）
    # 竖屏模式（vertical）：512x768（9:16，适合抖音/快手/视频号）
    # 横屏模式（horizontal）：512x384（16:9，适合B站/YouTube）
    width: 512  # 竖屏宽度（如果改为横屏，应改为512）
    height: 768  # 竖屏高度（如果改为横屏，应改为384）
    num_inference_steps: 25  # 推理步数（从20提高到25，增强运动质量和幅度）
    # 步数建议（蒸馏版本）：
    #   - 15步：快速测试，质量不足，运动不明显（不推荐）❌
    #   - 20步：质量可能不足，运动可能不够自然（不推荐）❌
    #   - 25步：推荐用于生产（质量好，速度快，运动自然）✅ 当前使用（提高动作幅度）
    #   - 30步：高质量（如果需要更高质量和更自然的运动，动作幅度更大）
    # 注意：蒸馏版本虽然可以用更少步数，但至少需要25步才能保证质量和运动自然度
    # 标准版本建议：30-40步
    force_cpu_offload: true  # 强制启用CPU offload以降低显存占用
    guidance_scale: 8.5  # 引导尺度（从7.5提高到8.5，增强运动幅度和强度）
    # 注意：guidance_scale 影响运动强度，提高此值可以增加动作幅度
    # 建议范围：
    #   - 7.5-8.0：标准运动（原值）
    #   - 8.5-9.0：增强运动（当前使用，提高动作幅度）✅
    #   - 9.0-10.0：强烈运动（可能影响质量，谨慎使用）
    # 注意：guidance_scale过高可能导致过度运动或质量下降，建议在8.0-9.0之间
    # 色彩调整参数（用于修复过暗、色彩过浓的问题）
    # 注意：修复了双重色彩调整问题，现在所有帧统一只调整一次
    # 如果色彩仍然过浓，可以进一步降低saturation_factor（如0.7-0.8）
    # 如果色彩过淡，可以适当提高saturation_factor（如0.85-0.95）
    # 如果不需要色彩调整，可以设置为1.0（保持原始输出）
    # 色彩调整参数（暂时禁用，先看原始输出质量）
    # 如果原始输出色彩正常，可以保持1.0（不调整）
    # 如果原始输出过浓，可以降低saturation_factor（如0.9）
    # 如果原始输出过暗，可以增加brightness_factor（如1.05）
    saturation_factor: 1.0  # 暂时禁用饱和度调整，查看原始输出
    brightness_factor: 1.0  # 暂时禁用亮度调整，查看原始输出
    contrast_factor: 1.0  # 不调整对比度，保持原始对比度
    # Prompt配置（用于引导视频生成，虽然图生视频主要基于图片，但prompt可以帮助控制运动、风格等）
    # 重要：HunyuanVideo需要详细的prompt，描述图片内容和期望的运动方式
    # 参考官方示例格式：
    # "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. 
    # The fluffy-furred feline gazes directly at the camera with a relaxed expression. 
    # Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, 
    # and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, 
    # as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's 
    # intricate details and the refreshing atmosphere of the seaside."
    prompt: ""  # 视频生成提示词，详细描述图片内容、主体、背景、细节和期望的运动方式（留空则自动从场景信息构建）
    # 如果留空，系统会自动从scene信息构建详细prompt，包括：
    #   - 风格描述（style）
    #   - 主体描述（description/prompt）
    #   - 视觉构图（visual.composition）
    #   - 运动强度（motion_intensity: dynamic/moderate/gentle）
    #   - 镜头运动（camera_motion.type: pan/zoom/dolly/static）
    # 示例（手动指定）：
    #   - "Cinematic style, a peaceful mountain landscape with snow-capped peaks. The alpine meadow features wildflowers swaying gently in the breeze. Clear blue sky with white clouds forms the background. The scene captures the serene atmosphere of nature. Smooth camera movement, gentle motion, natural movement, stable camera. High quality, cinematic, detailed."
    negative_prompt: "low quality, blurry, distorted, deformed, bad anatomy, bad hands, text, watermark, static, frozen, no motion, still image"  # 负面提示词，避免不想要的元素
    # 注意：移除了"flickering, jittery, unstable, sudden movement, abrupt changes"
    # 这些词是为了减少SDXL跳动而添加的，但可能抑制了HunyuanVideo的运动生成
    # 改为强调"static, frozen, no motion, still image"，鼓励运动而避免跳动
    # 显存限制（类似JAX的显存限制，防止一次性分配完显存）
    # 注意：如果图像生成器已经占用了显存，需要降低此值以留出足够空间给HunyuanVideo
    # 由于图像生成器可能占用大量显存（约19GB），需要降低到20%以确保视频生成有足够空间
    max_memory_fraction: 0.6  # 20%，确保图像生成器卸载后有足够空间给HunyuanVideo
    # 注意：如果其他进程占用较多显存，可以降低此值（如0.25或0.2）
    # 但如果显存充足，可以提高此值（如0.4）以获得更好的性能
    # 建议值：
    #   - 0.2 (20%): 非常保守，适合有其他进程占用显存时
    #   - 0.3 (30%): 推荐，当图像生成器已占用显存时使用（当前使用）
    #   - 0.4 (40%): 如果图像生成器已完全卸载，可以使用更高的限制
    #   - 0.5 (50%): 如果GPU显存充足且没有其他进程
    attention_slice_size: 1  # attention切片大小，1=最省显存但最慢，"max"=平衡显存和速度
    # 风格配置（可在前端和脚本中覆盖）
    # 支持多种风格类型，每种风格包含关键词和描述
    style_templates:
      # 科普/教育风格（政府/教育赛道）
      scientific:
        keywords: ["realistic", "scientific", "educational", "professional", "cinematic"]
        description: "realistic scientific visualization style"
        negative_keywords: ["cartoon", "anime", "fantasy", "illustration"]
      # 产品广告风格（电商赛道）
      commercial:
        keywords: ["professional", "commercial", "product photography", "clean", "modern", "high quality"]
        description: "professional commercial product photography style"
        negative_keywords: ["low quality", "blurry", "amateur"]
      # 戏剧/情感风格（短剧推文）
      dramatic:
        keywords: ["cinematic", "dramatic", "emotional", "realistic", "film noir", "atmospheric"]
        description: "cinematic dramatic storytelling style"
        negative_keywords: ["cartoon", "anime", "unrealistic"]
      # 写实风格（通用）
      realistic:
        keywords: ["realistic", "photorealistic", "professional", "high quality", "detailed"]
        description: "realistic professional photography style"
        negative_keywords: ["cartoon", "anime", "illustration", "unrealistic"]
      # 仙侠风格（保留兼容性，但建议使用realistic）
      xianxia:
        keywords: ["Chinese animation style", "ancient Chinese fantasy", "xianxia", "cinematic"]
        description: "Chinese animation fantasy style"
        negative_keywords: ["modern", "realistic", "photorealistic"]
    # 默认风格（如果脚本中未指定style，使用此风格）
    default_style: realistic  # 可选: scientific, commercial, dramatic, realistic, xianxia

  # ==========================================
  # M6 视频身份保持：身份验证配置（HunyuanVideo 生成后验证 + 失败重试）
  # ==========================================
  identity_verification:
    # 是否启用身份验证（上层脚本也可覆盖）
    enabled: true
    # 相似度阈值：低于此值判定失败并可能触发重试
    similarity_threshold: 0.65
    # 丢弃阈值：低于此值直接丢弃，不再重试
    similarity_discard: 0.60
    # 是否允许“丢弃级别”的视频继续参与后续重试（用于 hard case 兜底）
    retry_on_discard: true
    # 最多允许触发几次“丢弃后仍继续重试”（避免无限烧算力）
    discard_retry_max: 1
    # 漂移阈值：单帧相似度低于此值视为漂移帧（用于 drift_ratio 统计）
    drift_threshold: 0.50
    # 最大漂移帧比例（超过则失败）
    max_drift_ratio: 0.05
    # 最低人脸检测率（低于则失败）
    min_face_detect_ratio: 0.75
    # 最低相似度下限（用于过滤极端崩脸帧）
    min_similarity_floor: 0.10
    # 最大重试次数
    max_retries: 3
    # 重试策略开关
    retry_reduce_motion: true
    retry_adjust_prompt: true
    # 验证采样：每隔多少帧采样一次（越小越准，越大越快）
    sample_interval: 5
    # 最多分析多少帧（限制耗时）
    max_frames: 30
    # 强制分析最后 N 帧（避免采样间隔错过“尾段崩脸”）
    include_last_n_frames: 3
    # 重试随机种子（确保每次重试采样不同，避免“重试=同一条视频”）
    seed_base: 42
    seed_step: 1
    # 不同镜头类型的阈值容忍度（会从 similarity_threshold 中减去该值，值越大越“宽松”）
    shot_type_tolerance:
      wide: 0.10
      medium: 0.05
      medium_close: 0.04
      close: 0.03
      extreme_close: 0.02
    # 分层调参（失败后按严重度/失败类型升级参数）
    layered_tuning:
      # 极低相似度（常见于尾段脸崩/脸花）阈值：低于此值按“严重”处理
      catastrophic_min_similarity: 0.15
      # 漂移比例偏高阈值：超过则按“漂移型失败”处理
      high_drift_ratio: 0.12
      # 人脸检测率偏低阈值：低于则按“未检出/遮挡”处理
      low_face_detect_ratio: 0.80
      # 失败后步数升级策略
      steps_increase_small: 4
      steps_increase_large: 8
      # 极端崩脸/高漂移场景的额外升级
      steps_increase_catastrophic: 12
      steps_max: 45
      # 动态场景降级链：dynamic -> moderate -> gentle
      downgrade_motion_on_retry: true

  # ==========================================
  # ID-Animator（实验/研究路径）：身份保持视频生成（SD1.5 + AnimateDiff）
  # ==========================================
  id_animator:
    enabled: false
    model_dir: models
    # Face Adapter（下载脚本默认文件名：id_animator.pth）
    animator_path: models/ID-Animator/id_animator.pth
    # Motion Module（下载脚本通常会落盘 diffusion_pytorch_model.safetensors 或 mm_sd_v15_v2.safetensors）
    motion_module_path: models/AnimateDiff/diffusion_pytorch_model.safetensors
    base_model: runwayml/stable-diffusion-v1-5
    # 生成参数（默认用于测试）
    id_strength: 0.7
    num_frames: 16
    fps: 8
    guidance_scale: 7.5
    num_inference_steps: 25
  
  # CogVideoX配置（量产产线，快速批量生成）
  cogvideox:
    # 模型路径（本地路径或HuggingFace模型ID）
    # 推荐模型：THUDM/CogVideoX-5b-I2V（图生视频）
    model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/CogVideoX-5b
    # 或使用HuggingFace自动下载: THUDM/CogVideoX-5b-I2V
    # 帧数（CogVideoX-5B 固定：49帧，对应6秒@8fps）
    # 注意：CogVideoX实际生成帧数 = num_frames + 1（因为第一帧是条件帧）
    # 重要：CogVideoX-5B 仅支持 720x480 分辨率，不支持其他分辨率（含微调）
    num_frames: 49  # CogVideoX-5B 固定49帧（6秒@8fps，官方文档要求）
    fps: 8  # CogVideoX-5B 固定帧率：8fps（官方文档要求）
    # 分辨率（CogVideoX-5B 仅支持 720x480，不支持其他分辨率）
    width: 720  # CogVideoX-5B 固定分辨率：720（官方文档要求）
    height: 480  # CogVideoX-5B 固定分辨率：480（官方文档要求）
    # 推理步数（CogVideoX-5B 推荐50步）
    num_inference_steps: 50  # CogVideoX-5B 推荐50步（官方文档推荐）
    guidance_scale: 6.0  # 引导尺度（CogVideoX推荐6.0）
    # 是否启用动态CFG（use_dynamic_cfg，推荐启用）
    use_dynamic_cfg: true  # 启用动态CFG，质量更好
    # 显存优化（根据官方文档，推荐启用CPU offload和tiling）
    enable_model_cpu_offload: true  # 启用CPU offload（官方推荐，确保兼容性）
    enable_tiling: true  # 启用VAE tiling以降低显存占用（对速度影响较小）
    # Prompt配置（CogVideoX对prompt要求不如HunyuanVideo严格，但详细prompt效果更好）
    prompt: ""  # 视频生成提示词（留空则自动从场景信息构建）
    negative_prompt: "low quality, blurry, distorted, deformed, bad anatomy, bad hands, text, watermark, flickering, jittery, unstable, sudden movement, abrupt changes"  # 负面提示词
    # 显存限制（速度优化：如果禁用CPU offload，可以提高显存限制）
    max_memory_fraction: 0.8  # 从0.3提高到0.6，允许使用更多显存以提升速度（禁用CPU offload时）
  
  # 模型路由配置（自动选择模型）
  model_routing:
    # 场景类型到模型的映射
    scene_type_mapping:
      government: hunyuanvideo  # 政府/科普 → 高端产线
      enterprise: hunyuanvideo  # 企业广告 → 高端产线
      scientific: hunyuanvideo  # 科普教育 → 高端产线
      premium: hunyuanvideo  # 高端场景 → 高端产线
      novel: cogvideox  # 小说推文 → 量产产线
      drama: cogvideox  # 短剧 → 量产产线
      daily: cogvideox  # 日常运营 → 量产产线
      social: cogvideox  # 社交媒体 → 量产产线
      general: cogvideox  # 通用场景 → 量产产线（默认）
    # 用户等级到模型的映射
    user_tier_mapping:
      free: cogvideox  # 免费用户 → 量产产线
      basic: cogvideox  # 基础用户 → 量产产线
      professional: cogvideox  # 专业用户 → 量产产线
      enterprise: hunyuanvideo  # 企业用户 → 高端产线
      premium: hunyuanvideo  # 高级用户 → 高端产线
    # 显存阈值（GB）
    memory_threshold:
      hunyuanvideo: 20  # HunyuanVideo需要至少20GB可用显存
      cogvideox: 12  # CogVideoX需要至少12GB可用显存

# Prompt Engine配置（专业级Prompt工程系统）
# 包含6个核心模块：Prompt Rewriter、Scene Decomposer、Style Controller、
# Camera Engine、Negative Prompt Generator、Prompt QA
prompt_engine:
  # 是否启用Prompt Engine（推荐启用，可提升视频质量30%-70%）
  enabled: true
  # 是否使用LLM进行Prompt重写（需要LLM API，暂时禁用）
  use_llm_rewriter: false
  # 是否使用智能场景分析器（推荐启用，自动识别场景需求）
  use_scene_analyzer: true
  # 场景分析器模式：local（本地规则，快速免费）或 llm（LLM分析，更智能但需要API）
  scene_analyzer_mode: "llm" # local | llm | hybrid
  # LLM API配置（如果启用LLM重写器或LLM场景分析）
  llm_api:
    type: "openai"  # 或 "anthropic", "custom", "qwen", "doubao"
    api_key: "sk-2OyCN2OOgrk8GA6hSASHaMeZBmbh5R0ZNPQ99x122bXWMhuU"
    model: "gpt-4o-mini"
    base_url: "https://api.chatanywhere.tech/v1"

  # 生成帧数
  # 关键优化：num_frames 必须 = duration * fps，否则会导致运动不连贯
  # 例如：duration=5s, fps=24 → num_frames=120
  # 24 帧太少，会导致雾气跳动、动作不连贯、人脸模糊
  num_frames: 120  # 从75提高到120（= duration * fps = 5 * 24），极大改善流畅度和人脸清晰度
  # 帧率（影响运动流畅度：fps太低会导致运动跳跃、不自然）
  # 优化：提高到 24 fps 以获得更流畅的动画效果（标准视频帧率）
  # 注意：提高帧率会增加生成时间和文件大小，但显著改善流畅度
  fps: 24  # 从 15 提高到 24，标准视频帧率，显著改善流畅度和人脸清晰度
  # 视频分辨率（SVD 要求必须是 64 的倍数，推荐 1024x576 或 1280x768）
  # 优化：提高到 1280x768 以获得更好的人脸清晰度（后续可通过 Real-ESRGAN 超分到 1080P）
  width: 1280  # 从 1024 提高到 1280，提升人脸清晰度
  height: 768  # 从 576 提高到 768，提升人脸清晰度
  # 生成参数
  # 推理步数（增加步数可提高稳定性和质量，减少人物位置漂移和画面抖动）
  # 注意：对运动自然度无影响，主要用于人物和背景稳定性
  # 建议：40-50 步即可，60 步对运动自然度无帮助
  # 对于动作场景，系统会自动增加到45步以提高稳定性
  num_inference_steps: 40  # 基础步数，动作场景会自动增加
  min_inference_steps_for_static: 0  # 静态场景（lying_still）的最小推理步数（设置为 0 禁用自动增加，推理步数对微动影响不大）
  guidance_scale: 6.0
  # 注意：svd-xt 和 svd-image-to-video 的参数范围不同
  # svd-xt: motion_bucket_id <= 2, noise_aug_strength <= 0.0005（更适合静态场景）
  # svd-image-to-video: motion_bucket_id 2-3, noise_aug_strength 0.015-0.025
  # 运动参数（影响雾气等环境元素的运动自然度）
  # motion_bucket_id: 控制运动幅度和连贯性
  #   - 1.5: 最自然的雾气飘动（推荐）
  #   - 2: 可能导致过度运动、反向拉回、背景抖动
  #   注意：对于雾气、轻风、灵气飘动，motion_bucket_id 不应超过 2
  motion_bucket_id: 1.5  # 从2降到1.5，最自然的雾气飘动，避免过度运动和反向拉回
  # noise_aug_strength: 影响运动的平滑度和随机性
  #   - 推荐范围：0.0001 ~ 0.00025（最自然的雾气）
  #   - 0.0004 太高，会导致：雾气飘动过大、反向拉回、背景"中风式抖动"
  noise_aug_strength: 0.00025  # 从0.0004降到0.00025，最自然的雾气飘动，避免过度运动和抖动
  # decode_chunk_size: 控制解码时的块大小，影响帧间连贯性
  #   - 推荐范围：6 ~ 8（SVD-XT 最稳定）
  #   - 超过 8 会引发：某些帧视觉跳跃、局部动作反向、亮度不稳定
  decode_chunk_size: 8  # 从10降到8，SVD-XT最稳定范围，避免补帧抖动和动作反向
  # 视频时长（秒）
  duration: 5.0
  # 内存优化：限制最大时长和帧数，避免OOM
  max_duration: 16.0  # 最大视频时长（秒），超过此值会被截断
  max_frames: 384  # 最大帧数（约16秒@24fps），超过此值会被截断
  
  # 视频插帧配置（提升视频流畅度）
  # 原理：先生成关键帧（减少帧数，提高速度），然后使用插帧到目标帧数
  # 优势：生成速度提升 30%+，流畅度提升 50%+
  # 支持多种插帧方法（按优先级）：
  # 1. 官方 RIFE 实现（需要手动安装：git clone https://github.com/hzwer/arXiv2020-RIFE.git）
  # 2. OpenCV 光流插帧（自动使用，无需额外安装）
  # 3. 简单线性插值（降级方案，自动使用）
  rife:
    enabled: true  # 是否启用插帧
    interpolation_scale: 2.0  # 插帧倍数（例如：生成60帧，插到120帧）
    # 注意：系统会自动选择可用的插帧方法，无需手动配置

# 图像生成配置
# 支持多种模式：sdxl（旧方案）、instantid（新方案 1080P）、多模型组合（推荐）
image:
  # 生成引擎：sdxl, instantid, auto（自动选择，推荐）
  engine: auto  # 可选：sdxl, instantid, auto（自动根据任务类型选择）
  device: cuda
  mixed_precision: fp16  # fp16 或 fp8（需要 PyTorch 2.5+ 和 CUDA 12.1+）
  enable_cpu_offload: false
  enable_vae_tiling: true
  overwrite_existing: false
  force_regenerate: false
  # 图像分辨率（根据 video.aspect_ratio 配置自动匹配视频格式）
  # 注意：分辨率必须是8的倍数（SDXL/Flux要求）
  # 竖屏模式（vertical）：768x1152（9:16，匹配视频512x768）
  # 横屏模式（horizontal）：1536x864（16:9，匹配视频512x384）
  width: 768  # 竖屏宽度（如果改为横屏，应改为1536）
  height: 1152  # 竖屏高度（如果改为横屏，应改为864）
  # LoRA（角色定制）配置：InstantID / SDXL 均可复用
  # 组合B（InstantID + LoRA 平衡）：推荐配置
  lora:
    enabled: false  # ⚡ 核心修复：禁用角色LoRA，避免与InstantID冲突，使用人物Prompt模板代替
    weights_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/lora/hanli/pytorch_lora_weights.safetensors
    adapter_name: hanli
    # ⚠️ 重要：角色LoRA与InstantID冲突，导致人物不稳定
    # 正确方案：使用人物Prompt模板（HanLi.prompt）代替LoRA
    alpha: 0.5  # 已禁用，此值不再生效
    force_cpu: false
    # 注意：组合B 禁用额外的 IP-Adapter（InstantID 自己的 IP-Adapter 是必需的，用于面部嵌入）
    # 使用 LoRA 时 IP-Adapter 权重乘数（避免与 LoRA 冲突）
    # 组合B：此参数不适用（禁用额外的 IP-Adapter）
    ip_adapter_scale_multiplier: 0.6  # 保留兼容性，但组合B不使用额外的 IP-Adapter
    # 风格 LoRA（可选）
    style_lora:
      enabled: false  # ⚡ 核心修复：禁用风格LoRA，风格只在Scene层通过Prompt注入，避免与InstantID冲突
      weights_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/lora/anime_style/pytorch_lora_weights.safetensors
      adapter_name: anime_style
      # ⚠️ 重要：风格LoRA会干扰人物稳定性，正确方案是在Scene层通过Prompt控制风格
      alpha: 0.65  # 已禁用，此值不再生效
      force_cpu: false
  
  # 镜头控制配置
  camera:
    # 是否允许特写场景（close-up）
    # true: 允许特写，不自动转换为中景
    # false: 自动将特写转换为中景（默认，更稳定，避免身体变形）
    allow_close_up: false  # 默认false，如需特写可设为true
  
  # 场景参考图像选择配置（从原始素材中自动选择匹配的参考图像）
  scene_reference:
    # 是否启用场景参考图像选择
    enabled: true  # 设置为true启用，从processed目录中自动选择匹配的参考图像
    # FAISS索引路径（用于向量检索）
    index_path: processed/global_index.faiss
    # 索引元数据路径
    metadata_path: processed/index_metadata.json
    # keyframes基础目录
    keyframes_base: processed
    # 返回top k个最相关的参考图像
    top_k: 3  # 选择3个最相关的参考图像
    # 检索方法：vector（向量检索）、keyword（关键词检索）、hybrid（混合检索，推荐）
    method: hybrid  # 推荐使用hybrid，结合语义和关键词匹配
    # IP-Adapter权重（使用场景参考图像时的权重）
    weight: 0.56  # 场景参考图像权重，0.6表示中等强度
    # 是否使用场景参考图像作为img2img输入（推荐启用，效果更好）
    use_as_img2img: true  # 启用后，场景参考图像将作为img2img的init_image，更好地保持结构和风格
  
  # InstantID 配置（1080P 方案）
  instantid:
    # 基础模型：使用标准的 SDXL 模型（InstantID 支持）
    # 推荐模型（按优先级）：
    # 1. stabilityai/stable-diffusion-xl-base-1.0 (标准 SDXL，稳定可靠)
    # 2. RunDiffusion/Juggernaut-XL-v9 (如果可用)
    # 注意: Juggernaut-XL-v9-anime 可能不存在，使用标准 SDXL 即可
    base_model: stabilityai/stable-diffusion-xl-base-1.0
    # 模型路径（如果本地已有完整的 SDXL 模型，使用本地路径）
    # 注意: 如果本地路径存在且完整，会优先使用本地模型，避免网络下载
    model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/sdxl-base
    # InstantID ControlNet 路径
    controlnet_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/instantid/ControlNet
    # IP-Adapter 路径
    ip_adapter_path: /vepfs-dev/shawn/vid/fanren/InstantID/checkpoints
    # 面部参考图像路径（用于特征缓存）
    # 建议使用正脸照片（如 character_face.png），效果更好
    # 如果是目录，会优先查找 character_face.png、face.png 等正脸照片
    # 韩立的参考图：hanli_mid.jpg（已更新为新参考图）
    face_image_path: /vepfs-dev/shawn/vid/fanren/gen_video/reference_image/hanli_mid.jpg
    # 分辨率：竖屏模式（9:16，匹配视频512x768）
    # 注意：分辨率必须是 8 的倍数（SDXL 要求）
    width: 768  # 竖屏宽度（9:16，匹配视频512x768）
    height: 1152  # 竖屏高度（9:16，匹配视频512x768）
    # 推理步数（提高步数可改善生成质量，减少拉伸和变形）
    # 建议范围：50-70，步数越多质量越好但速度越慢
    # 如果使用 DPM++ 采样器，可以用更少步数（40-50 步）达到相同质量
    # ⚡ 优化：从40提高到50，改善人像生成质量（特别是复杂场景和特写）
    num_inference_steps: 50  # 从40提高到50，改善人像生成质量和细节
    # 引导尺度（控制文本提示的影响强度）
    # 建议范围：7.0-8.5，过高可能导致过度引导和僵硬
    guidance_scale: 7.5  # 从8.5降到7.5，减少过度引导，使生成更自然
    # CFG Rescale（减少过度饱和，使颜色更自然）
    # 主流做法：使用 CFG Rescale 避免高 CFG 值导致的颜色过度饱和
    # 推荐值：0.5-0.8，0.7 是平衡点
    guidance_rescale: 0.7  # 新增：CFG rescale factor，使颜色更自然
    # 采样器配置（主流优化：使用 DPM++ 获得更好质量）
    # 可选值：
    #   - "EulerDiscreteScheduler" (默认，速度快但质量一般)
    #   - "DPMSolverMultistepScheduler" (推荐，质量最好，业界标准)
    scheduler: "DPMSolverMultistepScheduler"  # 新增：使用 DPM++ 采样器，质量提升 15-20%
    # DPM++ 采样器参数（可选，使用默认值即可）
    scheduler_config:
      algorithm_type: "dpmsolver++"  # DPM++ 算法
      solver_order: 2  # 求解器阶数（2 或 3，2 更稳定）
      lower_order_final: true  # 最后几步使用低阶，更稳定
      use_karras_sigmas: true  # 使用 Karras noise schedule，质量更好
    # 面部特征缓存（30 张图只算 1 次）
    enable_face_cache: true
    # 量化类型：fp8, fp16, fp32
    # 注意: fp8 需要 PyTorch 2.5+ 和 CUDA 12.1+，且需要硬件支持
    # 如果遇到 "couldn't find storage object Float8_e4m3fnStorage" 错误，请使用 fp16
    quantization: fp16  # 稳定可靠，广泛支持
    # 面部权重（控制角色一致性强度，影响角色相似度和服饰准确性）
    # InstantID的人脸特征嵌入权重，直接影响人脸相似度
    # 组合B（InstantID + LoRA 平衡）：推荐 0.75-0.95
    # 如果生成的人像不像韩立，需要提高此值以增强人脸相似度
    # 用户反馈生成的人像不像韩立，进一步提高到 0.95 以确保高相似度
    face_emb_scale: 0.82  # ⚡ 关键修复：根据专业分析，对于远景场景（脸部占比<5%），权重应该降低（0.65-0.72），而不是提高。但对于正常场景，使用0.82（推荐区间0.78-0.88的中值）
    # 面部关键点图像缩放基准（控制面部关键点的控制强度，影响人物比例和拉伸）
    # 1.0 表示原始尺寸，>1 放大（可能导致拉伸），<1 缩小（减少控制，更自然）
    # 组合B（InstantID + LoRA 平衡）：推荐 0.65-0.70
    face_kps_scale: 0.70  # 组合B优化：保持0.70，与LoRA平衡，避免畸变
    face_kps_offset_y: 0  # 可微调关键点在竖向的位置（像素）
  
  # 多模型组合配置（推荐方案）
  # 针对科普视频流水线 + 政府单场景的最优组合
  model_selection:
    # 人物生成（主持人）- 使用 Flux.1 + FaceID（最稳）
    character:
      engine: flux-instantid  # Flux.1 + InstantID/FaceID
      # Flux.1 模型配置（用于主持人脸）
      flux1_model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/flux1-dev
      flux1_base_model: black-forest-labs/FLUX.1-dev
      # InstantID 配置（复用现有配置）
      instantid_path: /vepfs-dev/shawn/vid/fanren/InstantID/checkpoints
      controlnet_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/instantid/ControlNet
      face_image_path: /vepfs-dev/shawn/vid/fanren/gen_video/reference_image/kupu_gege.png
      # LoRA 配置（用于固定人设）
      lora:
        enabled: true
        weights_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/lora/kepu_gege/pytorch_lora_weights.safetensors
        adapter_name: kepu_gege
        alpha: 1.3  # 提高LoRA权重以增强人脸相似度（仅使用LoRA时建议1.2-1.5）
      # 生成参数
      width: 1536
      height: 864
      num_inference_steps: 28  # 优化：从40降到28，速度提升约30%，质量几乎无损失（Flux模型在28步已足够）
      guidance_scale: 7.5
      quantization: fp16
    
    # 场景生成（科普背景）
    scene:
      # 优先级列表（按顺序尝试）
      engines:
        - flux2              # 优先：科学背景图（冲击力强）、太空/粒子/量子类（效果爆炸）
        - flux1              # 备选：实验室/医学（更干净自然）
        - hunyuan-dit        # 备选：中文场景
        - kolors             # 备选：真实感场景（真人质感强，中文 prompt 理解优秀）
        - realistic-vision   # 备选：真实感场景（如果 Kolors 不可用）
        - sd3-turbo         # 批量：高速备选图
      
      # Flux.2 配置（科学背景图、太空/粒子/量子类，冲击力强）
      # 注意：当前本地模型目录是 flux2-dev，但实际可能是 FLUX.1-schnell
      # 如果确实使用 FLUX.2，模型ID应该是：black-forest-labs/FLUX.2-dev 或 black-forest-labs/FLUX.2-schnell
      # 确认方法：访问 https://huggingface.co/black-forest-labs 查看可用的 FLUX.2 模型
      # ⚡ 性能优化：根据Flux特性优化（步数已优化到28，CFG降低到3.5-4.0，Flux不吃高CFG）
      flux2:
        model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/flux2-dev
        base_model: black-forest-labs/FLUX.1-schnell  # 当前使用 FLUX.1-schnell，如要使用 FLUX.2 请更新为对应的模型ID
        width: 768  # 竖屏宽度（9:16，匹配视频512x768）
        height: 1152  # 竖屏高度（9:16，匹配视频512x768）
        num_inference_steps: 20  # 优化：从28降到20，速度提升约30%，质量几乎无损失（Flux在20步已足够）
        guidance_scale: 3.5  # 优化：从7.0降到3.5，Flux不吃高CFG，速度和质量都更好
        quantization: bfloat16  # 已优化：使用bfloat16获得更好质量和速度
        device_map: cuda
        # 加速选项（在代码中启用）
        enable_model_cpu_offload: true  # CPU offload减少显存占用
        enable_attention_slicing: true  # Attention slicing加速推理
      
      # Flux.1 配置（实验室/医学场景，更干净自然）
      # ⚡ 性能优化：根据Flux特性优化（步数已优化到20，CFG降低到3.5-4.0）
      flux1:
        model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/flux1-dev
        base_model: black-forest-labs/FLUX.1-dev
        width: 768  # 竖屏宽度（9:16，匹配视频512x768）
        height: 1152  # 竖屏高度（9:16，匹配视频512x768）
        num_inference_steps: 20  # 优化：从28降到20，速度提升约30%，质量几乎无损失（Flux极速模式）
        guidance_scale: 3.5  # 优化：从7.0降到3.5，Flux不吃高CFG，速度和质量都更好
        quantization: bfloat16  # 已优化：使用bfloat16获得更好质量和速度
        device_map: cuda
        # 加速选项（在代码中启用）
        enable_model_cpu_offload: true  # CPU offload减少显存占用
        enable_attention_slicing: true  # Attention slicing加速推理
      
      # Hunyuan-DiT 配置（中文理解最强）
      hunyuan_dit:
        model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/hunyuan-dit
        base_model: Tencent-Hunyuan/HunyuanDiT
        width: 1536
        height: 864
        num_inference_steps: 50
        guidance_scale: 7.5
        # 中文提示词优化
        chinese_optimization: true
        quantization: fp16
      
      # Kolors 配置（真实感和稳定性，快手可图团队开发）
      # 使用 Kolors-IP-Adapter-FaceID-Plus 版本（可直接用 diffusers 加载）
      # HuggingFace: https://huggingface.co/Kwai-Kolors/Kolors-IP-Adapter-FaceID-Plus
      # 特点: 真人质感强、肤色真实、五官清晰、光影自然、色彩稳定，不会脏、中文 prompt 理解优秀
      kolors:
        model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/kolors
        base_model: Kwai-Kolors/Kolors-IP-Adapter-FaceID-Plus
        width: 1536
        height: 864
        num_inference_steps: 40
        guidance_scale: 7.0
        # 真实感增强
        realism_boost: true
        # 量化类型：bfloat16（推荐，质量好）或 float16
        quantization: bfloat16  # 使用 bfloat16 以获得更好的质量
        # 设备映射：cuda（GPU）或 cpu
        device_map: cuda
      
      # Realistic Vision 配置（备选方案，如果 Kolors 不可用）
      realistic_vision:
        model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/realistic-vision
        base_model: SG161222/Realistic_Vision_V5.1_noVAE
        width: 1536
        height: 864
        num_inference_steps: 40
        guidance_scale: 7.0
        quantization: fp16
      
      # SD3.5 Large Turbo 配置（极速批量生成）
      sd3_turbo:
        model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/sd3-turbo
        base_model: calcuis/sd3.5-large-turbo  # 使用 SD3.5 Large Turbo
        width: 1024
        height: 1024
        num_inference_steps: 4  # Turbo 模式，步数少但速度快
        guidance_scale: 1.0    # Turbo 模式，低引导
        # 批量生成模式
        batch_mode: true
        batch_size: 4  # 一次生成4张备选图
        quantization: fp16
  
  # SDXL 配置（旧方案，保留兼容）
  sdxl:
    model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/sdxl-base
    model_name: sdxl-base
    width: 1280
    height: 720
    use_img2img: true  # 启用img2img，用于场景连贯性（相邻场景使用前一个场景作为参考）
    use_ip_adapter: true
    img2img_strength: 0.3  # 降低默认值，避免参考图过度覆盖（人物场景会自动降到0.28）
    reference_image_dir: 
      /vepfs-dev/shawn/vid/fanren/gen_video/hanli_reference
    reference_image_pattern: '*.png'
    face_reference_dir: 
      /vepfs-dev/shawn/vid/fanren/gen_video/hanli_reference
    num_inference_steps: 40  # 统一为 40 步（与 InstantID 保持一致，如果使用 DPM++ 采样器）
    guidance_scale: 8.5
  ip_adapter:
      model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/ip-adapter
      subfolder: sdxl_models
      weight_name: ip-adapter_sdxl.bin
      scale: 0.7
      clip_image_size: 224
      face_image_size: 160
      face_crop_ratio: 0.9  # ⚡ 稳定性优化：从0.75提高到0.9，固定参考脸角度，提升稳定性
      face_reference_only: true
  
  # ==========================================
  # 增强模式配置（新方案 - PuLID + 解耦融合 + Execution Planner V3）
  # ==========================================
  # 是否启用增强模式（使用新的角色一致性方案）
  # 启用后，当 scene 参数存在时，会自动使用 EnhancedImageGenerator
  # 否则使用传统的 ImageGenerator
  enhanced_mode:
    enabled: true  # 是否启用增强模式（默认 true，可以设置为 false 回退到旧方案）
    # 注意：即使 enabled=false，pulid 和 decoupled_fusion 的配置仍然保留
    # 以便将来可以单独启用某个功能
  
  # ==========================================
  # PuLID-FLUX 配置（新方案 - 身份保持与环境融合更好）
  # ==========================================
  # 参考豆包/可灵的参考强度控制和多参考图方案
  pulid:
    enabled: true
    # 模型路径
    model_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/pulid/pulid_flux_v0.9.1.safetensors
    flux_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/flux1-dev
    # InsightFace 路径 (人脸检测)
    antelopev2_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/antelopev2
    eva_clip_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/clip/EVA02_CLIP_L_336_psz14_s6B.pt
    # 分辨率（竖屏 9:16）
    width: 768
    height: 1152
    # 生成参数
    num_inference_steps: 28  # Flux 在 28 步已足够
    guidance_scale: 3.5      # Flux 不需要高 CFG
    quantization: bfloat16
    # 参考强度控制 (0-100, 类似可灵)
    # 这是默认值，实际值由 Execution Planner V3 根据场景动态计算
    default_reference_strength: 60  # 默认中等强度
    # 参考强度映射（镜头类型 -> 参考强度）
    # 注意：这些值会被 Execution Planner V3 动态调整
    reference_strength_map:
      extreme_wide: 20   # 超远景: 环境优先
      wide: 30           # 远景
      full: 45           # 全身
      american: 55       # 7/8身
      medium: 60         # 中景
      medium_close: 70   # 中近景
      close: 80          # 特写
      extreme_close: 90  # 超特写
  
  # ==========================================
  # 解耦融合配置（SAM2 + YOLO）
  # ==========================================
  # 解决"人脸一致性 vs 环境丰富度"矛盾的核心方案
  decoupled_fusion:
    enabled: true
    # SAM2 模型路径
    sam2_path: /vepfs-dev/shawn/vid/fanren/gen_video/models/sam2
    # 人物检测方法: auto, yolo, sam2
    detection_method: auto  # auto 会根据场景自动选择
    # 融合参数
    feather_radius: 10      # 边缘羽化半径（像素）
    mask_padding: 20        # mask 扩展边缘（用于自然过渡）
    # 质量验证
    verify_face_similarity: true
    similarity_threshold: 0.7  # 人脸相似度阈值
    # 自动重试
    auto_retry: true
    max_retries: 3
    retry_strength_adjustment: 10  # 每次重试增加的参考强度
  
  # ==========================================
  # Execution Planner V3 配置
  # ==========================================
  # 智能场景路由，根据镜头类型自动选择引擎和参数
  execution_planner:
    version: 3
    # 是否启用 V3 功能
    enabled: true
    # 默认引擎选择策略
    engine_selection:
      # 特写/中近景: 使用 InstantID (锁脸更强)
      close_shots: instantid
      # 中景: 使用 PuLID (平衡)
      medium_shots: pulid
      # 远景: 使用 Flux (环境优先)
      wide_shots: flux
    # 是否自动使用解耦生成
    auto_decoupled: true
    # 解耦生成触发条件
    decoupled_threshold:
      max_reference_strength: 50  # 参考强度低于此值时考虑解耦
      shot_types: ["extreme_wide", "wide", "full"]  # 这些镜头类型考虑解耦
  
  # ==========================================
  # 角色档案系统（多参考图）
  # ==========================================
  # 参考可灵 Element Library 的多参考图方案
  character_profiles:
    enabled: true
    # 角色档案基础目录
    profiles_dir: /vepfs-dev/shawn/vid/fanren/gen_video/character_profiles
    # 默认角色
    default_character: hanli
    # 角色配置
    characters:
      hanli:
        id: hanli
        name: 韩立
        profile_dir: hanli
        # 多角度参考图
        references:
          front: front.png
          three_quarter: three_quarter.png
          profile: side.png
        # 表情库
        expressions:
          neutral: neutral.png
          happy: happy.png
          sad: sad.png
          angry: angry.png
        # 元数据
        metadata:
          gender: male
          age_range: "20-25"
          style_tags: ["ancient_chinese", "cultivator", "handsome"]
          hair: "black, long, tied"
          eyes: "deep, brown"
  
  # 通用提示词配置（两种引擎共用）
  # 注意：这些是默认提示词，实际使用时应该根据场景描述动态生成
  # character_prompt: 角色描述（根据场景中的角色信息动态生成）
  # environment_prompt: 环境描述（根据场景描述动态生成）
  # base_style_prompt: 基础风格提示词（可根据配置选择不同风格）
  character_prompt: ""  # 留空，由代码根据场景动态生成
  environment_prompt: ""  # 留空，由代码根据场景动态生成
  # 基础风格提示词（凡人修仙传仙侠动漫风格）
  # 使用中国仙侠动漫风格，类似《凡人修仙传》动画的风格
  base_style_prompt: "Chinese xianxia anime style, 3D rendered anime, detailed character, cinematic lighting, 4k, sharp focus, traditional Chinese fantasy aesthetic, immortal cultivator style"  # 凡人修仙传仙侠动漫风格
  # 图像生成负面提示词（针对仙侠动漫场景优化）
  # 注意：移除了forest, mountains, night, indoor, horse等仙侠场景可能需要元素
  negative_prompt: low quality, blurry, noise, overexposed, underexposed, deformed, distorted, mutated hands, deformed hands, extra fingers, missing fingers, fused fingers, bad anatomy, photorealistic, hyperrealistic, realistic, real photo, photograph, western style, european style, modern style, watermark, modern city, text, logo, compression artifacts, oversized character, oversized subject, huge figure, camera shake, unstable camera, floating, drifting, unstable motion, inconsistent action, discontinuous action, jitter, flicker, unstable frame, fast movement, rapid action, sudden movement, broken frame, frame jump, inconsistent background, changing background, background jump, scene jump, style drift, style inconsistency, unstable environment, changing scene, motorcycle, bicycle, car, vehicle, modern transportation, modern technology, electronic products, appliances, modern architecture, skyscraper, modern architectural style, modern clothing, modern decoration, modern fashion, phone, computer, TV, camera, modern weapons, guns, firearms, modern equipment, (multiple people:1.5), (two people:1.5), (crowd:1.5), (group of people:1.5), (extra person:1.5), (duplicate character:1.5), (second person:1.5), (additional figure:1.5), (cloned person:1.5), (duplicate person:1.5), (identical person:1.5), (twin character:1.5), (repeated character:1.5), (second identical figure:1.5), (duplicate figure:1.5), (mirrored person:1.5), (copy of person:1.5), (same person twice:1.5)
  ascii_only_prompt: true

# 配音配置
tts:
  # TTS引擎：chattts, cosyvoice, openvoice, coqui
  engine: cosyvoice  # 新方案使用 CosyVoice-2.0-0.5B
  # 量化类型：int8, fp16, fp32（int8 显存 3.4GB → 1.1GB）
  quantization: fp32
  force_regenerate: false
  # 采样率
  sample_rate: 48000
  # 位深度
  bit_depth: 24
  # 流式生成（首包 150ms，边生成边播）
  stream: false
  # 自动情感预测（内置 6 种风格）
  auto_emotion: true
  # 全局 TTS 随机种子（用于 CosyVoice 和 ChatTTS，确保生成结果可复现）
  # 如果设置为 null 或不设置，每次生成结果会不同（随机）
  seed: 777
  # 语速配置（用于计算视频时长和字幕同步）
  # 根据实际测试，朗读速度为 3.55 字/秒（中文字符），8 字符/秒（总字符，包括标点）
  speech_rate:
    chars_per_second: 8.0  # 总字符（包括标点）的朗读速度：8 字符/秒
    words_per_second: 3.55  # 中文字符的朗读速度：3.55 字/秒
    # 使用哪个参数计算时长：chars（总字符数）或 words（中文字符数）
    # 推荐使用 chars，因为更准确（包含标点符号的停顿）
    use_chars: true
  
  # CosyVoice 配置（新方案）
  cosyvoice:
    # 模型名称：CosyVoice2-0.5B（推荐，性能最好，根据 GitHub README）
    # 其他可选：CosyVoice-300M, CosyVoice-300M-SFT, CosyVoice-300M-Instruct
    # 注意: 模型名称需要与下载脚本中的映射一致
    model_name: CosyVoice2-0.5B
    # 模型路径（如果本地已有，通常下载到 CosyVoice/pretrained_models/ 目录）
    model_path: /vepfs-dev/shawn/vid/fanren/CosyVoice/pretrained_models/CosyVoice2-0.5B
    # 是否使用 CosyVoice2（True 使用 CosyVoice2，False 使用 CosyVoice）
    use_cosyvoice2: true
    # 默认情感风格（如果 auto_emotion=false）
    # CosyVoice2: 使用 zero_shot 模式时需要 prompt_speech
    # CosyVoice: 使用 SFT 模式，speaker 可选：中文女、中文男等
    default_emotion: 中文  # CosyVoice2 使用 zero_shot，CosyVoice 使用 SFT speaker
    
    # 双环境配置（解决 transformers 版本冲突）
    # 如果 use_subprocess=true，CosyVoice 将在独立环境中运行（避免 transformers 版本冲突）
    use_subprocess: true  # 是否使用子进程模式（推荐：true，避免版本冲突）
    subprocess_python: /vepfs-dev/shawn/venv/cosyvoice/bin/python  # CosyVoice 环境的 Python 路径
    subprocess_script: /vepfs-dev/shawn/vid/fanren/gen_video/tools/cosyvoice_subprocess_wrapper.py  # 子进程包装器脚本路径
    
    # CosyVoice2 zero_shot 模式配置（按照官方用法）
    # prompt_speech: 用于 zero_shot 的参考音频文件路径（16kHz WAV 格式）
    # 排查问题：暂时切换回之前正常工作的配置进行对比测试
    # 如果这个配置正常，说明问题在于 haoran 音频或 prompt_text 不匹配
    prompt_speech: /vepfs-dev/shawn/vid/fanren/gen_video/assets/prompts/zero_shot_prompt_clean.wav
    # prompt_text: 用于 zero_shot 的参考文本（对应参考音频的文本内容）
    # 注意：这个文本应该与 prompt_speech 音频文件中的实际内容精确匹配
    # 重要：prompt_text 必须与音频中的实际内容完全一致，包括标点符号，否则会导致声音质量差
    # 这是之前正常工作的配置（给凡人修仙传配音时使用的）
    # 关键发现：prompt_text 必须与 prompt_speech 音频内容完全匹配
    # 音频识别结果：'大家好，我是云卷仙音，今天我们要继续讲述凡人修仙转的故事，在这个故事中，韩立经历了无数的挑' (42字符)
    # 使用完整的音频内容作为 prompt_text，确保完全匹配
    prompt_text: "大家好，我是云卷仙音，今天我们要继续讲述凡人修仙转的故事，在这个故事中，韩立经历了无数的挑"  # 匹配完整音频内容（42字符，确保与音频完全匹配）
    # text_frontend: 是否启用文本前端处理（True=启用，False=禁用）
    # 官方 README 提示：如果要复现演示页面结果，应该使用 text_frontend=False
    # 但官方 API 默认值是 text_frontend=True
    # 测试：使用 text_frontend=False 看是否能解决生成时长异常问题
    text_frontend: false  # 使用 False 以匹配官方演示页面结果
    # mode: CosyVoice2 使用模式（zero_shot, instruct2, cross_lingual）
    # zero_shot: 需要 prompt_text 和 prompt_speech 精确匹配（prompt_text 必须与音频内容完全一致）
    # instruct2: 可以使用指令文本，更灵活，不要求精确匹配
    mode: zero_shot  # 使用 zero_shot 模式，需要 prompt_text 与音频内容完全匹配
    # instruct2 模式的指令文本（当 mode=instruct2 时使用）
    # 描述声音风格和特点，不需要与音频内容匹配
    instruction: "用专业、清晰、亲切的科普解说风格，语速适中，吐字清晰"  # 用于 instruct2 模式
  
  # ChatTTS 配置（旧方案，保留兼容）
  chattts:
    temperature: 0.3
    speed: 1.0
    pitch: 1.0
    oral_ability: 1 
    style_prompt: |
      女性声音，温柔知性，情绪平稳，语调亲切自然，解说语气，吐字清晰，语速适中，句末轻柔收尾，女声旁白
    negative_prompt: |
      男性声音，男声，过度夸张、机械感、语速过快、咬字含糊
    seed: 777

# 字幕配置
subtitle:
  # Whisper模型大小：tiny, base, small, medium, large-v2, large-v3
  # 注意: large-v3 需要大量显存，如果遇到 std::bad_alloc 错误，请使用 medium 或 base
  # medium 模型在中文识别上表现也很好，且内存占用更少
  model_size: medium  # 从 large-v3 改为 medium 以节省内存
  # 如果遇到 std::bad_alloc 错误（不是内存不足，而是 CUDA/C++ 问题），可以强制使用 CPU
  # 设置方法: export WHISPERX_FORCE_CPU=1 或在 config.yaml 中设置 force_cpu: true
  force_cpu: true  # 设为 true 强制使用 CPU（避免 CUDA 问题，推荐）
  # 注意: 如果使用 medium 模型，model_dir 应该指向 medium 模型目录
  # 如果目录不存在，会自动下载到默认位置
  model_dir: models/faster-whisper-medium  # 更新为 medium 模型目录
  local_files_only: false
  # 启用对齐功能（如果内存不足，可以设为 false）
  align: false  # 暂时禁用以节省内存
  # 对齐模型（中文专用）
  align_model: ZhangCheng/whisperx-align-zh
  # VAD 模型（语音活动检测）
  vad_model: pyannote/segmentation-3.0
  # 批量大小（medium 模型建议使用较小的 batch）
  # 注意: 如果遇到内存不足错误，可以降低到 8 或 4
  batch_size: 8  # 进一步降低以减少内存占用
  use_script_segments: false  # 设为 false 以使用 WhisperX 实际识别音频，而不是占位字幕
  # 语言代码
  language: zh
  # 计算类型：float16（CUDA）或 int8（更节省内存）
  # 注意: int8 可以大幅减少内存占用，建议优先使用
  compute_type: int8  # 使用 int8 以节省内存
  # 输出格式：srt, vtt, ass
  format: srt
  # 是否将繁体中文转换为简体中文（默认启用）
  convert_to_simplified: true
  # 原稿字幕配置：使用原始文本替换 WhisperX 识别结果
  script:
    use_original_text: true  # 开启后，将使用原始脚本文本作为字幕内容
    path: ""  # 可选：脚本文件路径（留空则使用 narration/segments）
    format: auto  # auto / json / text
    field: full_narration  # 当格式为 json 时，读取的字段
    fields: []  # 可选：额外字段列表（支持 scenes[].narration 语法）
    include_scene_narration: false  # 若为 true，会追加 scenes[*].narration
    delimiters: "。！？!?；;>\n"  # 拆分脚本文本的分隔符
  # 字体配置（用于硬编码字幕）
  font:
    family: SimHei
    size: 36  # 1080P 下增大字体
    color: white
    outline_color: black
    outline_width: 2

# 视频合成配置
composition:
  # 输出视频分辨率（1080P 方案）
  output_width: 1920
  output_height: 1080
  # 输出格式
  output_format: mp4
  # 视频编码器
  # 优化：使用 libx264（H.264）编码器，兼容性好，质量高
  video_codec: libx264
  # 视频编码预设（影响编码速度和质量）
  # 可选值：ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow
  # 优化：使用 medium 预设，平衡编码速度和质量
  video_preset: medium  # 从默认值改为 medium，提升编码质量
  # 视频编码质量参数（CRF，Constant Rate Factor）
  # 范围：0-51，值越小质量越好（推荐：18-23）
  # 优化：使用 20，在质量和文件大小之间取得平衡
  video_crf: 18  # 从 20 降到 18，提升视频质量，改善人脸清晰度
  # 音频编码器
  audio_codec: aac
  # 比特率
  # 优化：提高视频比特率以提升质量（从 8000k 提高到 10000k）
  video_bitrate: 10000k  # 从 8000k 提高到 10000k，提升视频质量
  audio_bitrate: 192k
  upscale:
    enabled: true  # 启用 FFmpeg 简单放大（在 Real-ESRGAN 之前进行初步放大）
    width: 1920
    height: 1080
    flags: lanczos  # 使用 lanczos 算法，质量较好
  sharpen:
    enabled: true
    luma_msize_x: 7
    luma_msize_y: 7
    luma_amount: 0.85
    chroma_msize_x: 5
    chroma_msize_y: 5
    chroma_amount: 0.0
  bgm:
    enabled: true
    volume: 0.32  # 背景音乐音量（0.0-1.0）
    intro_duration: 15.0  # BGM 开头播放时长（秒），之后淡出
    fade_out: 2.0  # BGM 淡出时长（秒）
    path: assets/bgm/lingjie_bgm.mp3  # 兼容旧配置，作为兜底音轨
    fade_in: 800
    fade_out: 1200
    crossfade: 300
    tracks:
      default:
        path: background_music/Whispers of the Bamboo Forest.mp3
        fade_in: 1200
        fade_out: 1600
      alternate:
        path: background_music/Whispering Bamboo Dreams.mp3
        match_moods:
          - calm
          - tranquil
          - cool
          - composed
          - lingering_power
      start:
        path: background_music/start.mp3
        fade_in: 1800
        fade_out: 1500
        apply_to: opening
      tense:
        path: background_music/Celestial Shadows nurse.mp3
        fade_in: 300
        fade_out: 600
        match_moods:
          - tense
          - crisis
          - explosive
          - shocking
          - pressure
      ending:
        path: background_music/ending.mp3
        fade_in: 400
        fade_out: 400
        apply_to: ending
  # 音频音量
  audio_volume: 1.35

# GPU配置
  postprocess:
    enabled: true
    # Real-ESRGAN 超分模型配置
    # 根据 分析chatgpt.md 建议，使用 RealESRGAN（x2）进行超分
    # 
    # 模型选择（针对3D渲染动漫）：
    # 方案1: RealESRGAN_x2plus.pth (推荐，速度最快，适合1080P视频)
    #   - x2模型，速度比x4快约4倍（0.3帧/秒 → 1.2帧/秒）
    #   - 适合从576P超分到1080P（需要2倍放大）
    #   - 质量略低于x4，但对解说视频足够
    #   - 下载地址: https://github.com/xinntao/Real-ESRGAN/releases
    # 
    # 方案2: RealESRGAN_x4plus_anime_6B.pth (备选，质量更好但较慢)
    #   - 针对动漫优化，保留动漫风格
    #   - 适合3D渲染动漫
    #   - 模型更小(6B)，但速度较慢（约0.3帧/秒）
    # 
    # 方案3: RealESRGAN_x4plus.pth (备选，如果方案1效果不理想)
    #   - 通用模型，对3D纹理细节处理可能更好
    #   - 可能过度锐化，失去动漫感
    #   - 需要实际测试对比效果
    # 
    # 切换方法：修改下面的 model_path 和 model_scale 即可
    # 例如：model_path: models/realesrgan/RealESRGAN_x2plus.pth, model_scale: 2
    model_path: gen_video/models/realesrgan/RealESRGAN_x2plus.pth
    model_scale: 2  # 模型本身的放大倍数（x4 模型用4，x2 模型用2）
    outscale: 2   # 最终输出放大倍数（2.0 = x2 放大，符合文档建议）
    # 目标分辨率（可选，如果设置，超分后会缩放到此分辨率）
    # 格式: "宽度x高度"，例如 "1920x1080"
    # 如果设置为 null 或空，则使用 outscale 的倍数输出
    target_resolution: "1920x1080"  # 超分后缩放到 1920x1080
    tile: 0  # 瓦片大小（设置为 400 可以加速处理，减少显存占用；0 表示不使用瓦片，可能更慢）
    full_precision: false
    codec: mp4v
    suffix: _upx2
    preserve_audio: true
    # 并行处理线程数（用于预读取帧，提高IO效率）
    # 注意：RealESRGANer不是线程安全的，实际处理仍在主线程
    # 但预读取可以优化IO，建议设置为2-4
    # 设置为1表示单线程模式（最稳定）
    num_workers: 1  # 预读取线程数，可以提高IO效率（但RealESRGANer限制，实际提升有限）
gpu:
  # 设备ID
  device_id: 0
  # 混合精度
  mixed_precision: fp16
  # 显存优化
  memory_efficient: true
  # CPU offload（将模型部分卸载到CPU，减少GPU内存但降低速度）
  # 设为 false 时，模型完全在GPU上，速度更快但占用更多显存（约10-15GB）
  # 设为 true 时，模型部分在CPU上，显存占用更少（约2-3GB）但速度较慢
  enable_cpu_offload: false  # 建议设为 false 以获得最佳性能

# 批量处理配置
batch:
  # 批量大小
  batch_size: 1
  # 并行处理数量（视频生成并行数，显存充足时可设置为2）
  # 注意：并行处理会增加显存占用，建议根据显存大小调整
  # 如果显存不足，建议设置为1（顺序处理）
  num_workers: 1  # 显存充足时可设置为2，同时处理2个视频




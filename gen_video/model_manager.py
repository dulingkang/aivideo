#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹åè°ƒè°ƒç”¨ç³»ç»Ÿ
ç»Ÿä¸€æ¥å£ï¼Œè‡ªåŠ¨è·¯ç”±ï¼ŒæŒ‰ä»»åŠ¡é€‰æ‹©æœ€ä¼˜æ¨¡å‹
"""

from pathlib import Path
from typing import Optional, Dict, Any
from PIL import Image
import torch
import yaml

from pipelines.flux_pipeline import FluxPipeline
from pipelines.kolors_pipeline import KolorsPipeline
from pipelines.sd3_pipeline import SD3TurboPipeline
from pipelines.hunyuan_pipeline import HunyuanPipeline
from pipelines.flux_instantid_pipeline import FluxInstantIDPipeline


class ModelManager:
    """
    å¤šæ¨¡å‹ç®¡ç†å™¨
    ç»Ÿä¸€æ¥å£ï¼Œè‡ªåŠ¨è·¯ç”±ï¼ŒæŒ‰ä»»åŠ¡é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    """
    
    def __init__(self, models_root: Optional[str] = None, lazy_load: bool = True, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ– ModelManager
        
        Args:
            models_root: æ¨¡å‹æ ¹ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰é¡¹ç›®çš„ models ç›®å½•
            lazy_load: æ˜¯å¦å»¶è¿ŸåŠ è½½ï¼ˆåªåœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹ï¼‰
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè¯»å– LoRA alpha ç­‰é…ç½®ï¼‰
        """
        if models_root is None:
            models_root = Path(__file__).parent / "models"
        else:
            models_root = Path(models_root)
        
        self.models_root = models_root
        self.lazy_load = lazy_load
        
        # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆç”¨äºè¯»å– LoRA alpha ç­‰é…ç½®ï¼‰
        self.config = {}
        if config_path is None:
            config_path = Path(__file__).parent / "config.yaml"
        else:
            config_path = Path(config_path)
        
        if config_path.exists():
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f) or {}
        
        # æ¨¡å‹è·¯å¾„é…ç½®
        self.model_paths = {
            "flux1": str(models_root / "flux1-dev"),
            "flux2": str(models_root / "flux2-dev"),
            "flux1-instantid": str(models_root / "flux1-dev"),  # Flux.1 + InstantID
            "kolors": str(models_root / "kolors-base"),
            "hunyuan": str(models_root / "hunyuan-dit" / "t2i"),
            "sd3": str(models_root / "sd3-turbo"),
        }
        
        # InstantID è·¯å¾„é…ç½®
        self.instantid_paths = {
            "instantid": str(models_root / "instantid"),
            "controlnet": str(models_root / "instantid" / "ControlNet"),
            "ip_adapter": str(models_root / "instantid" / "ip-adapter"),  # InstantID åŸç‰ˆï¼ˆSDXL ç”¨ï¼‰
            "ip_adapter_flux": str(models_root / "instantid" / "ip-adapter-flux"),  # Flux ä¸“ç”¨ç‰ˆæœ¬
        }
        
        # äººè„¸å‚è€ƒå›¾ç‰‡ç›®å½•
        self.face_references_dir = models_root / "face_references"
        self.face_references_dir.mkdir(parents=True, exist_ok=True)
        
        # Pipeline ç¼“å­˜ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.pipelines: Dict[str, Any] = {}
        
        # LoRA é…ç½®ï¼ˆå¯é€‰ï¼‰
        # ä¼˜å…ˆä½¿ç”¨æ”¹è¿›ç‰ˆ LoRAï¼ˆv2ï¼‰ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åŸç‰ˆ
        lora_root_v2 = models_root / "lora" / "host_person_v2"
        lora_root_v1 = models_root / "lora" / "host_person"
        
        # æ£€æŸ¥å“ªä¸ªç‰ˆæœ¬å­˜åœ¨ï¼ˆæœ€ç»ˆæ¨¡å‹å·²æ›´æ–°ä¸º checkpoint-500ï¼‰
        final_model = lora_root_v2 / "pytorch_lora_weights.safetensors"
        v1_model = lora_root_v1 / "pytorch_lora_weights.safetensors"
        
        if final_model.exists():
            lora_path = str(final_model)
            lora_version = "v2ï¼ˆæœ€ç»ˆç‰ˆï¼ŒåŸºäº checkpoint-500ï¼‰"
        elif v1_model.exists():
            lora_path = str(v1_model)
            lora_version = "v1ï¼ˆåŸç‰ˆï¼‰"
        else:
            lora_path = None
            lora_version = "æœªæ‰¾åˆ°"
        
        # ä»é…ç½®æ–‡ä»¶è¯»å– LoRA alphaï¼ˆä¼˜å…ˆä½¿ç”¨ model_selection.character.lora.alphaï¼‰
        default_alpha = 0.6  # é»˜è®¤å€¼
        if self.config:
            image_config = self.config.get('image', {})
            model_selection = image_config.get('model_selection', {})
            character_config = model_selection.get('character', {})
            lora_config = character_config.get('lora', {})
            if 'alpha' in lora_config:
                default_alpha = float(lora_config['alpha'])
                print(f"  â„¹ ä»é…ç½®æ–‡ä»¶è¯»å– LoRA alpha: {default_alpha}")
        
        self.lora_configs: Dict[str, Dict[str, Any]] = {
            "host_face": {
                "lora_path": lora_path,  # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„ LoRA ç‰ˆæœ¬
                "lora_alpha": default_alpha  # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼ˆé»˜è®¤ 0.6ï¼‰
            },
            "character_face": {
                "lora_path": lora_path,  # å¤ç”¨ä¸»æŒäºº LoRA
                "lora_alpha": default_alpha  # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼ˆé»˜è®¤ 0.6ï¼‰
            }
        }
        
        if lora_path:
            print(f"  â„¹ ä½¿ç”¨ LoRA: {lora_version}")
        
        # åŠ è½½è§’è‰²æè¿°æ–‡ä»¶ï¼ˆç”¨äºé…åˆ LoRA å›ºå®šå½¢è±¡ï¼‰
        self.character_profiles = self._load_character_profiles()
        
        # ä»»åŠ¡è·¯ç”±è¡¨
        self.routing_table = {
            # äººè„¸ç›¸å…³ï¼ˆæ”¯æŒ InstantIDï¼‰
            "host_face": "flux1",  # ç§‘æ™®ä¸»æŒäººè„¸ï¼ˆé»˜è®¤ä½¿ç”¨ Flux.1ï¼Œå¦‚æœæä¾› face_image åˆ™ä½¿ç”¨ InstantIDï¼‰
            "host_face_instantid": "flux1-instantid",  # ç§‘æ™®ä¸»æŒäººè„¸ + InstantIDï¼ˆå›ºå®šäººè„¸ï¼‰
            "character_face": "flux1",  # è§’è‰²äººè„¸ï¼ˆé»˜è®¤ä½¿ç”¨ Flux.1ï¼‰
            "character_face_instantid": "flux1-instantid",  # è§’è‰²äººè„¸ + InstantID
            "realistic_face": "flux1",  # çœŸå®æ„Ÿäººè„¸ï¼ˆé»˜è®¤ä½¿ç”¨ Flux.1ï¼‰
            "realistic_face_instantid": "flux1-instantid",  # çœŸå®æ„Ÿäººè„¸ + InstantID
            
            # ç§‘å­¦èƒŒæ™¯
            "science_background": "flux2",  # ç§‘å­¦èƒŒæ™¯å›¾ï¼ˆå†²å‡»åŠ›å¼ºï¼‰
            "quantum_particle": "flux2",  # é‡å­/ç²’å­
            "space_cosmos": "flux2",  # å¤ªç©º/å®‡å®™
            
            # å®éªŒå®¤/åŒ»å­¦
            "lab_scene": "flux1",  # å®éªŒå®¤åœºæ™¯ï¼ˆæ›´å¹²å‡€è‡ªç„¶ï¼‰
            "medical_scene": "flux1",  # åŒ»å­¦åœºæ™¯
            
            # å®˜æ–¹é£æ ¼
            "official_style": "hunyuan",  # å®˜æ–¹æ„Ÿç§‘æ•™å®£ä¼ å›¾
            "chinese_scene": "hunyuan",  # ä¸­æ–‡åœºæ™¯
            "education_style": "hunyuan",  # æ•™è‚²é£æ ¼
            
            # å¿«é€Ÿç”Ÿæˆ
            "fast_background": "sd3",  # å¿«é€ŸèƒŒæ™¯
            "batch_generation": "sd3",  # æ‰¹é‡ç”Ÿæˆ
            "variations": "sd3",  # å¤‡é€‰å›¾
        }
        
        # å¦‚æœä¸éœ€è¦å»¶è¿ŸåŠ è½½ï¼Œé¢„åŠ è½½æ‰€æœ‰æ¨¡å‹
        if not lazy_load:
            self._load_all_pipelines()
    
    def _get_pipeline(self, model_name: str):
        """è·å– Pipelineï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if model_name in self.pipelines:
            return self.pipelines[model_name]
        
        model_path = self.model_paths.get(model_name)
        if not model_path:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹: {model_name}")
        
        if not Path(model_path).exists():
            raise RuntimeError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        # æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºå¯¹åº”çš„ Pipeline
        if model_name == "flux1":
            pipeline = FluxPipeline(model_path, model_type="flux1")
        elif model_name == "flux2":
            pipeline = FluxPipeline(model_path, model_type="flux2")
        elif model_name == "flux1-instantid":
            # ä½¿ç”¨ Flux + InstantID Pipeline
            # ä¼˜å…ˆä½¿ç”¨ Flux ä¸“ç”¨çš„ IP-Adapter
            instantid_path = self.instantid_paths.get("ip_adapter_flux")
            if not Path(instantid_path).exists():
                # å¦‚æœ Flux ç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨åŸç‰ˆï¼ˆè™½ç„¶ä¸å…¼å®¹ï¼Œä½†è‡³å°‘å¯ä»¥æç¤ºï¼‰
                instantid_path = self.instantid_paths.get("ip_adapter")
            controlnet_path = self.instantid_paths.get("controlnet")
            
            # é»˜è®¤ç¦ç”¨ IP-Adapterï¼Œä½¿ç”¨çº¯ Flux + LoRA æ¨¡å¼ï¼ˆæ•ˆæœæ›´å¥½ï¼‰
            # å¦‚æœç”¨æˆ·éœ€è¦ IP-Adapterï¼Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®å¯ç”¨
            use_ip_adapter = False  # é»˜è®¤ç¦ç”¨ï¼Œå› ä¸º Flux IP-Adapter æ•ˆæœä¸å¦‚ LoRA
            
            pipeline = FluxInstantIDPipeline(
                model_path=model_path,
                instantid_path=instantid_path,
                controlnet_path=controlnet_path,
                model_type="flux1",
                use_ip_adapter=use_ip_adapter
            )
        elif model_name == "kolors":
            # Kolors tokenizer æœ‰ä¸¥é‡ bugï¼Œæš‚æ—¶ç¦ç”¨ï¼Œè‡ªåŠ¨ä½¿ç”¨ Flux.1 æ›¿ä»£
            print("  âš ï¸  è­¦å‘Š: Kolors tokenizer å­˜åœ¨ä¸¥é‡ bugï¼ˆå³ä½¿å¾ˆçŸ­æç¤ºè¯ä¹Ÿä¼šæº¢å‡ºï¼‰")
            print("  â„¹ï¸  è‡ªåŠ¨åˆ‡æ¢åˆ° Flux.1ï¼ˆæ•ˆæœç±»ä¼¼ï¼Œæ›´ç¨³å®šï¼‰")
            # ä½¿ç”¨ Flux.1 æ›¿ä»£
            flux1_path = self.model_paths.get("flux1")
            if flux1_path and Path(flux1_path).exists():
                pipeline = FluxPipeline(flux1_path, model_type="flux1")
                # æ›´æ–°ç¼“å­˜é”®ï¼Œä½¿ç”¨ flux1 è€Œä¸æ˜¯ kolors
                model_name = "flux1"
            else:
                raise RuntimeError(
                    "Kolors tokenizer æœ‰ bug ä¸” Flux.1 ä¸å¯ç”¨ã€‚"
                    "å»ºè®®ï¼šä½¿ç”¨å…¶ä»–æ¨¡å‹æˆ–ç­‰å¾… Kolors ä¿®å¤"
                )
        elif model_name == "hunyuan":
            pipeline = HunyuanPipeline(model_path)
        elif model_name == "sd3":
            pipeline = SD3TurboPipeline(model_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        # å»¶è¿ŸåŠ è½½ï¼šåªåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åŠ è½½
        if not self.lazy_load:
            pipeline.load()
        
        # å¦‚æœ model_name è¢«ä¿®æ”¹ï¼ˆå¦‚ kolors -> flux1ï¼‰ï¼Œä½¿ç”¨æ–°çš„åç§°ç¼“å­˜
        # ä½†ä¹Ÿè¦ä¿ç•™åŸå§‹è¯·æ±‚çš„æ˜ å°„ï¼Œä»¥ä¾¿åç»­æŸ¥æ‰¾
        self.pipelines[model_name] = pipeline
        return pipeline
    
    def route(self, task: str) -> str:
        """
        æ ¹æ®ä»»åŠ¡è·¯ç”±åˆ°å¯¹åº”çš„æ¨¡å‹
        
        Args:
            task: ä»»åŠ¡ç±»å‹
            
        Returns:
            æ¨¡å‹åç§°
        """
        model_name = self.routing_table.get(task)
        if model_name is None:
            # é»˜è®¤ä½¿ç”¨ Flux.2
            print(f"âš ï¸  æœªçŸ¥ä»»åŠ¡ '{task}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ flux2")
            return "flux2"
        return model_name
    
    def _load_face_image(self, face_image_name: Optional[str] = None, task: Optional[str] = None) -> Optional[Image.Image]:
        """
        ä» face_references ç›®å½•åŠ è½½äººè„¸å›¾ç‰‡
        
        Args:
            face_image_name: äººè„¸å›¾ç‰‡æ–‡ä»¶åï¼ˆå¦‚ "host_face.png"ï¼‰
            task: ä»»åŠ¡ç±»å‹ï¼ˆç”¨äºè‡ªåŠ¨æŸ¥æ‰¾ï¼‰
            
        Returns:
            PIL Image æˆ– None
        """
        if face_image_name:
            # ç›´æ¥åŠ è½½æŒ‡å®šæ–‡ä»¶
            face_path = self.face_references_dir / face_image_name
            if face_path.exists():
                try:
                    return Image.open(face_path)
                except Exception as e:
                    print(f"  âš ï¸  æ— æ³•åŠ è½½äººè„¸å›¾ç‰‡ {face_image_name}: {e}")
                    return None
            else:
                print(f"  âš ï¸  äººè„¸å›¾ç‰‡ä¸å­˜åœ¨: {face_path}")
                return None
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨æŸ¥æ‰¾
        if task:
            # ä»»åŠ¡ç±»å‹åˆ°æ–‡ä»¶åçš„æ˜ å°„
            task_to_filename = {
                "host_face": "host_face.png",
                "character_face": "character_face.png",
                "realistic_face": "realistic_face.png",
            }
            
            filename = task_to_filename.get(task)
            if filename:
                face_path = self.face_references_dir / filename
                if face_path.exists():
                    try:
                        print(f"  âœ… è‡ªåŠ¨åŠ è½½äººè„¸å›¾ç‰‡: {filename}")
                        return Image.open(face_path)
                    except Exception as e:
                        print(f"  âš ï¸  æ— æ³•åŠ è½½äººè„¸å›¾ç‰‡ {filename}: {e}")
        
        return None
    
    def generate(
        self,
        task: str,
        prompt: str,
        negative_prompt: Optional[str] = None,
        width: int = 1024,
        height: int = 1024,
        num_inference_steps: Optional[int] = None,
        guidance_scale: Optional[float] = None,
        seed: Optional[int] = None,
        face_image: Optional[Image.Image] = None,
        face_image_name: Optional[str] = None,
        face_strength: float = 0.8,
        **kwargs
    ) -> Image.Image:
        """
        ç”Ÿæˆå›¾åƒï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            task: ä»»åŠ¡ç±»å‹ï¼ˆè‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”æ¨¡å‹ï¼‰
            prompt: æç¤ºè¯
            negative_prompt: è´Ÿé¢æç¤ºè¯
            width: å›¾åƒå®½åº¦
            height: å›¾åƒé«˜åº¦
            num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆNone æ—¶ä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼ï¼‰
            guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆNone æ—¶ä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼ï¼‰
            seed: éšæœºç§å­
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            PIL Image
        """
        # åŠ è½½äººè„¸å›¾ç‰‡ï¼ˆå¦‚æœæä¾›äº†æ–‡ä»¶åæˆ–éœ€è¦è‡ªåŠ¨æŸ¥æ‰¾ï¼‰
        if face_image is None:
            loaded_face_image = self._load_face_image(face_image_name, task)
            if loaded_face_image:
                face_image = loaded_face_image
        
        # å¦‚æœæä¾›äº†äººè„¸å›¾åƒï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° InstantID æ¨¡å¼
        if face_image is not None and task in ["host_face", "character_face", "realistic_face"]:
            task = f"{task}_instantid"
            print(f"  ğŸ¯ æ£€æµ‹åˆ°äººè„¸å›¾åƒï¼Œåˆ‡æ¢åˆ° InstantID æ¨¡å¼: {task}")
        
        # è·¯ç”±åˆ°å¯¹åº”æ¨¡å‹
        model_name = self.route(task)
        
        # æ£€æŸ¥ pipeline æ˜¯å¦å·²ç¼“å­˜ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
        if model_name in self.pipelines:
            pipeline = self.pipelines[model_name]
            print(f"  ğŸ” è°ƒè¯•: ä½¿ç”¨ç¼“å­˜çš„ pipeline {model_name} (loaded={pipeline.loaded})")
        else:
            pipeline = self._get_pipeline(model_name)
            print(f"  ğŸ” è°ƒè¯•: åˆ›å»ºæ–°çš„ pipeline {model_name} (loaded={pipeline.loaded})")
        
        # å»¶è¿ŸåŠ è½½
        if self.lazy_load:
            if not pipeline.loaded:
                print(f"  ğŸ” è°ƒè¯•: å‡†å¤‡åŠ è½½ pipeline {model_name} (loaded={pipeline.loaded})")
                pipeline.load()
                # éªŒè¯ loaded çŠ¶æ€æ˜¯å¦æ­£ç¡®è®¾ç½®
                if hasattr(pipeline, 'loaded'):
                    print(f"  ğŸ” è°ƒè¯•: pipeline {model_name} åŠ è½½å®Œæˆ (loaded={pipeline.loaded})")
                else:
                    print(f"  âš ï¸  è­¦å‘Š: pipeline {model_name} æ²¡æœ‰ loaded å±æ€§")
            else:
                print(f"  â­ï¸  Pipeline {model_name} å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½ (loaded={pipeline.loaded})")
        elif not pipeline.loaded:
            # éå»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œä½†pipelineè¿˜æœªåŠ è½½ï¼Œéœ€è¦åŠ è½½
            print(f"  âš ï¸  è­¦å‘Š: éå»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œä½†pipelineæœªåŠ è½½ï¼Œå¼ºåˆ¶åŠ è½½...")
            pipeline.load()
        
        # ä¼˜åŒ–æç¤ºè¯ï¼ˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼‰
        optimized_prompt, optimized_negative = self._optimize_prompt(task, prompt, negative_prompt)
        
        # å¦‚æœæ˜¯ host_face ä»»åŠ¡ï¼Œæ·»åŠ ç§‘å­¦ä¸»æŒäººçš„è§’è‰²æè¿°ï¼ˆé…åˆ LoRA ä½¿ç”¨ï¼‰
        base_task = task.replace("_instantid", "")
        if base_task == "host_face" and "host_person" in self.character_profiles:
            # å…ˆæ„å»ºè§’è‰²æè¿°
            optimized_prompt = self._build_character_prompt("host_person", optimized_prompt)
            print(f"  âœ… å·²æ·»åŠ ç§‘å­¦ä¸»æŒäººè§’è‰²æè¿°ï¼ˆé…åˆ LoRA ä½¿ç”¨ï¼‰")
            
            # åœ¨è§’è‰²æè¿°åï¼Œå¼ºåˆ¶åœ¨æœ€å‰é¢æ·»åŠ çœŸå®æ„Ÿå…³é”®è¯ï¼ˆç¡®ä¿çœŸå®é£æ ¼ï¼Œé¿å…åŠ¨æ¼«ï¼‰
            # è¿™äº›å…³é”®è¯å¿…é¡»åœ¨æœ€å‰é¢ï¼Œæƒé‡æœ€é«˜
            real_style_keywords = "photorealistic, realistic, high quality, detailed, professional photography"
            if real_style_keywords.lower() not in optimized_prompt.lower():
                optimized_prompt = f"{real_style_keywords}, {optimized_prompt}"
                print(f"  â„¹ æ·»åŠ çœŸå®æ„Ÿå…³é”®è¯ï¼ˆæœ€å‰é¢ï¼‰ï¼š{real_style_keywords}ï¼ˆé¿å…åŠ¨æ¼«é£æ ¼ï¼‰")
            
            # ç¡®ä¿æœ‰ä¸­å›½/äºšæ´²äººç‰¹å¾ï¼ˆåœ¨çœŸå®æ„Ÿå…³é”®è¯åï¼‰
            if "chinese" not in optimized_prompt.lower() and "asian" not in optimized_prompt.lower() and "ä¸­å›½" not in optimized_prompt and "äºšæ´²" not in optimized_prompt:
                # åœ¨çœŸå®æ„Ÿå…³é”®è¯åæ’å…¥
                optimized_prompt = optimized_prompt.replace(real_style_keywords, f"{real_style_keywords}, Chinese, Asian", 1)
                print(f"  â„¹ æ·»åŠ ä¸­å›½/äºšæ´²äººç‰¹å¾ï¼šChinese, Asianï¼ˆç¡®ä¿ä¸­å›½äººå½¢è±¡ï¼‰")
            
            # ä»…ä½¿ç”¨LoRAæ—¶ï¼Œæ·»åŠ æ›´ç²¾ç¡®çš„äººè„¸ç‰¹å¾æè¿°ä»¥å¢å¼ºæ•ˆæœ
            if face_image is not None and "instantid" in task.lower():
                # å¦‚æœFaceAnalyzerä¸å¯ç”¨ï¼Œä»…ä½¿ç”¨LoRAï¼Œéœ€è¦æ›´å¼ºçš„æç¤ºè¯
                # åœ¨çœŸå®æ„Ÿå…³é”®è¯åæ’å…¥
                face_keywords = "detailed facial features, accurate face"
                if face_keywords.lower() not in optimized_prompt.lower():
                    optimized_prompt = optimized_prompt.replace(real_style_keywords, f"{real_style_keywords}, {face_keywords}", 1)
                    print(f"  â„¹ ä»…ä½¿ç”¨LoRAæ¨¡å¼ï¼Œæ·»åŠ ç²¾ç¡®äººè„¸ç‰¹å¾æè¿°ä»¥å¢å¼ºæ•ˆæœ")
        
        # ä½¿ç”¨æ¨¡å‹é»˜è®¤å‚æ•°ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼ˆimage.model_selection.characterï¼‰
        if num_inference_steps is None:
            # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
            if self.config:
                image_config = self.config.get('image', {})
                model_selection = image_config.get('model_selection', {})
                character_config = model_selection.get('character', {})
                if 'num_inference_steps' in character_config:
                    num_inference_steps = int(character_config['num_inference_steps'])
                    print(f"  â„¹ ä»é…ç½®æ–‡ä»¶è¯»å–æ¨ç†æ­¥æ•°: {num_inference_steps}")
            
            # å¦‚æœé…ç½®æ–‡ä»¶æ²¡æœ‰ï¼Œä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼
            if num_inference_steps is None:
                if model_name == "sd3":
                    num_inference_steps = 8  # SD3 Turbo é»˜è®¤æ­¥æ•°å°‘
                elif model_name in ["flux1", "flux2", "flux1-instantid"]:
                    num_inference_steps = 28  # Flux ä¼˜åŒ–ï¼š28æ­¥å·²è¶³å¤Ÿï¼Œé€Ÿåº¦æå‡çº¦30%
                elif model_name == "kolors":
                    num_inference_steps = 22  # Kolors é»˜è®¤æ­¥æ•°
                else:
                    num_inference_steps = 50  # å…¶ä»–æ¨¡å‹é»˜è®¤
        
        if guidance_scale is None:
            # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
            if self.config:
                image_config = self.config.get('image', {})
                model_selection = image_config.get('model_selection', {})
                character_config = model_selection.get('character', {})
                if 'guidance_scale' in character_config:
                    guidance_scale = float(character_config['guidance_scale'])
                    print(f"  â„¹ ä»é…ç½®æ–‡ä»¶è¯»å–å¼•å¯¼å¼ºåº¦: {guidance_scale}")
            
            # å¦‚æœé…ç½®æ–‡ä»¶æ²¡æœ‰ï¼Œä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼
            if guidance_scale is None:
                if model_name == "sd3":
                    guidance_scale = 1.0  # SD3 Turbo ä½å¼•å¯¼
                elif model_name in ["flux1", "flux2", "flux1-instantid"]:
                    guidance_scale = 3.5  # Flux é»˜è®¤å¼•å¯¼
                else:
                    guidance_scale = 7.5  # å…¶ä»–æ¨¡å‹é»˜è®¤
        
        print(f"ğŸ¨ ä½¿ç”¨æ¨¡å‹: {model_name} (ä»»åŠ¡: {task})")
        if optimized_prompt != prompt:
            print(f"  â„¹ ä¼˜åŒ–åçš„æç¤ºè¯: {optimized_prompt[:100]}...")
        if optimized_negative != (negative_prompt or ""):
            print(f"  â„¹ ä¼˜åŒ–åçš„è´Ÿé¢æç¤ºè¯: {optimized_negative[:100]}...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ è½½ LoRA
        # æ³¨æ„ï¼šhost_face_instantid å’Œ character_face_instantid ä¹Ÿåº”è¯¥ä½¿ç”¨å¯¹åº”çš„ LoRA
        lora_kwargs = {}
        # è·å–åŸºç¡€ä»»åŠ¡åï¼ˆå»æ‰ _instantid åç¼€ï¼‰
        base_task = task.replace("_instantid", "")
        if base_task in self.lora_configs:
            lora_cfg = self.lora_configs[base_task]
            if lora_cfg.get("lora_path"):
                lora_kwargs["lora_path"] = lora_cfg["lora_path"]
                lora_kwargs["lora_alpha"] = lora_cfg.get("lora_alpha", 1.0)
                print(f"  âœ… å·²é…ç½® LoRA: {Path(lora_cfg['lora_path']).name} (alpha={lora_cfg.get('lora_alpha', 1.0)})")
        elif task in self.lora_configs:
            # å…¼å®¹æ—§çš„ä»»åŠ¡å
            lora_cfg = self.lora_configs[task]
            if lora_cfg.get("lora_path"):
                lora_kwargs["lora_path"] = lora_cfg["lora_path"]
                lora_kwargs["lora_alpha"] = lora_cfg.get("lora_alpha", 1.0)
                print(f"  âœ… å·²é…ç½® LoRA: {Path(lora_cfg['lora_path']).name} (alpha={lora_cfg.get('lora_alpha', 1.0)})")
        
        # ç”Ÿæˆå›¾åƒ
        generate_kwargs = {
            "prompt": optimized_prompt,
            "negative_prompt": optimized_negative,
            "width": width,
            "height": height,
            "num_inference_steps": num_inference_steps,
            "guidance_scale": guidance_scale,
            "seed": seed,
            **lora_kwargs,
        }
        
        # å¦‚æœä½¿ç”¨ InstantIDï¼Œæ·»åŠ äººè„¸ç›¸å…³å‚æ•°
        if "instantid" in model_name and face_image is not None:
            generate_kwargs["face_image"] = face_image
            generate_kwargs["face_strength"] = face_strength
        
        generate_kwargs.update(kwargs)
        
        image = pipeline.generate(**generate_kwargs)
        
        return image
    
    def _optimize_prompt(self, task: str, prompt: str, negative_prompt: Optional[str] = None) -> tuple[str, str]:
        """
        ä¼˜åŒ–æç¤ºè¯ï¼ˆæ ¹æ®ä»»åŠ¡ç±»å‹æ™ºèƒ½æ·»åŠ çº¦æŸï¼Œä¸å†™æ­»ï¼‰
        
        Args:
            task: ä»»åŠ¡ç±»å‹
            prompt: åŸå§‹æç¤ºè¯
            negative_prompt: åŸå§‹è´Ÿé¢æç¤ºè¯
            
        Returns:
            (ä¼˜åŒ–åçš„æç¤ºè¯, ä¼˜åŒ–åçš„è´Ÿé¢æç¤ºè¯)
        """
        optimized_prompt = prompt
        optimized_negative = negative_prompt or ""
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ InstantID ä»»åŠ¡
        is_instantid_task = task.endswith("_instantid")
        base_task = task.replace("_instantid", "") if is_instantid_task else task
        
        # é’ˆå¯¹ host_face ä»»åŠ¡ï¼ˆç§‘æ™®ä¸»æŒäººè„¸ï¼‰çš„æ™ºèƒ½ä¼˜åŒ–
        if base_task in ["host_face", "character_face", "realistic_face"] or task in ["host_face", "character_face", "realistic_face"]:
            prompt_lower = prompt.lower()
            
            # æ£€æŸ¥æ˜¯å¦æ˜ç¡®æåˆ°äººç‰©ç›¸å…³å…³é”®è¯
            has_person_keywords = any(kw in prompt_lower for kw in [
                "äºº", "äººç‰©", "è§’è‰²", "person", "character", "people", "man", "woman",
                "ä¸»æŒäºº", "host", "presenter", "ç§‘æ™®ä¸»æŒäºº", "face", "portrait"
            ])
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®æåˆ°äººç‰©ï¼Œæ·»åŠ "äººç‰©"å…³é”®è¯
            if not has_person_keywords:
                optimized_prompt = f"äººç‰©ï¼Œ{prompt}"
                print(f"  ğŸ’¡ æç¤ºè¯ä¼˜åŒ–: æ·»åŠ 'äººç‰©'å…³é”®è¯ï¼Œç¡®ä¿ç”Ÿæˆäººç‰©å›¾åƒ")
            
            # æ£€æŸ¥æ˜¯å¦æ˜ç¡®æŒ‡å®šäº†æ€§åˆ«
            has_male = any(kw in prompt_lower for kw in ["ç”·", "male", "man", "gentleman", "å…ˆç”Ÿ", "ç”·å£«"])
            has_female = any(kw in prompt_lower for kw in ["å¥³", "female", "woman", "lady", "å¥³å£«", "å°å§", "å¥³å­©"])
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯"ç§‘æ™®ä¸»æŒäºº"åœºæ™¯ï¼ˆåªæœ‰æ˜ç¡®æåˆ°æ—¶æ‰æ·»åŠ çº¦æŸï¼‰
            is_science_host = any(kw in prompt for kw in ["ç§‘æ™®ä¸»æŒäºº", "ç§‘æ™®", "science host", "science presenter"])
            is_host = any(kw in prompt_lower for kw in ["ä¸»æŒäºº", "host", "presenter"])
            
            # åªåœ¨æ˜ç¡®æ˜¯"ç§‘æ™®ä¸»æŒäºº"ä¸”æœªæŒ‡å®šæ€§åˆ«æ—¶ï¼Œæ‰æ·»åŠ "ç”·æ€§"çº¦æŸ
            if is_science_host and not has_male and not has_female:
                # æ£€æŸ¥ prompt ä¸­æ˜¯å¦å·²ç»æœ‰ male/man/ç”· ç­‰å…³é”®è¯ï¼Œé¿å…é‡å¤
                prompt_lower_check = prompt.lower()
                if not any(kw in prompt_lower_check for kw in ["male", "man", "ç”·", "ç”·å£«", "å…ˆç”Ÿ"]):
                    # åªæ·»åŠ ä¸€ä¸ªç®€æ´çš„æ€§åˆ«çº¦æŸï¼Œé¿å…é‡å¤å…³é”®è¯
                    # åŒæ—¶æ·»åŠ ä¸­å›½/äºšæ´²äººç‰¹å¾ï¼Œç¡®ä¿ç”Ÿæˆä¸­å›½äººå½¢è±¡ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼Œæƒé‡æœ€é«˜ï¼‰
                    optimized_prompt = f"Chinese male, Asian male, {prompt}"
                    print(f"  â„¹ æ£€æµ‹åˆ°ç§‘æ™®ä¸»æŒäººåœºæ™¯ï¼Œè‡ªåŠ¨æ·»åŠ æ€§åˆ«å’Œç§æ—çº¦æŸï¼šChinese male, Asian maleï¼ˆç®€æ´ï¼‰")
                else:
                    # å³ä½¿å·²æœ‰æ€§åˆ«ä¿¡æ¯ï¼Œä¹Ÿæ·»åŠ ä¸­å›½/äºšæ´²äººç‰¹å¾ï¼ˆç¡®ä¿ä¸­å›½äººå½¢è±¡ï¼‰
                    if "chinese" not in prompt_lower_check and "asian" not in prompt_lower_check and "ä¸­å›½" not in prompt and "äºšæ´²" not in prompt:
                        optimized_prompt = f"Chinese, Asian, {prompt}"
                        print(f"  â„¹ æ·»åŠ ä¸­å›½/äºšæ´²äººç‰¹å¾ï¼šChinese, Asianï¼ˆç¡®ä¿ä¸­å›½äººå½¢è±¡ï¼‰")
                    else:
                        print(f"  â„¹ æç¤ºè¯ä¸­å·²åŒ…å«æ€§åˆ«å’Œç§æ—ä¿¡æ¯ï¼Œä¸é‡å¤æ·»åŠ ")
            # å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†æ€§åˆ«ï¼Œå®Œå…¨å°Šé‡ç”¨æˆ·æ„å›¾ï¼Œä¸åšä»»ä½•ä¿®æ”¹
            elif has_female or has_male:
                # ç”¨æˆ·å·²æ˜ç¡®æŒ‡å®šæ€§åˆ«ï¼Œä¸æ·»åŠ ä»»ä½•çº¦æŸ
                pass
            # å¦‚æœåªæ˜¯æ™®é€š"ä¸»æŒäºº"ä½†æœªæŒ‡å®šæ€§åˆ«ï¼Œä¹Ÿä¸å¼ºåˆ¶æ·»åŠ ï¼ˆè®©ç”¨æˆ·è‡ªç”±é€‰æ‹©ï¼‰
            elif is_host and not has_male and not has_female:
                # ä¸å¼ºåˆ¶æ·»åŠ æ€§åˆ«ï¼Œä¿æŒç”¨æˆ·åŸå§‹æ„å›¾
                pass
            
            # åªåœ¨æ˜ç¡®æ˜¯"ç§‘æ™®"åœºæ™¯æ—¶ï¼Œæ‰æ·»åŠ ä¸“ä¸šé£æ ¼çº¦æŸ
            if is_science_host:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¸“ä¸šç›¸å…³è¯æ±‡
                has_professional = any(kw in prompt for kw in ["ä¸“ä¸š", "professional", "æ­£å¼", "formal", "å•†åŠ¡"])
                if not has_professional:
                    optimized_prompt = f"{optimized_prompt}, ä¸“ä¸šå½¢è±¡"
                    print(f"  â„¹ æ£€æµ‹åˆ°ç§‘æ™®åœºæ™¯ï¼Œæ·»åŠ ä¸“ä¸šå½¢è±¡çº¦æŸ")
            
            # è´Ÿé¢æç¤ºè¯ï¼šæ·»åŠ é€šç”¨é£æ ¼çº¦æŸå’Œæ€§åˆ«çº¦æŸï¼ˆå¼ºåŒ–çœŸå®æ„Ÿï¼‰
            # å¼ºåŒ–è´Ÿé¢æç¤ºè¯ï¼Œç¡®ä¿æ’é™¤æ‰€æœ‰åŠ¨æ¼«ã€å¡é€šé£æ ¼
            style_negative = "cartoon, anime, animation, animated, fantasy, å¡é€š, åŠ¨æ¼«, åŠ¨ç”», å¹»æƒ³, ä¸ä¸“ä¸š, ä¸æ­£å¼, low quality, blurry, distorted, illustration, drawing, sketch, æ’ç”», ç»˜ç”», æ‰‹ç»˜, 2d, stylized, artistic style, comic style, manga style, 3d render, cgi, computer graphics, digital art, concept art, game character, video game, animated character, cartoon character, anime character, manga character, chibi, kawaii, moe, cel shading, toon shading"
            
            # å¦‚æœæ˜¯ç§‘æ™®ä¸»æŒäººä¸”æœªæŒ‡å®šæ€§åˆ«ï¼Œæ·»åŠ å¥³æ€§æ’é™¤ï¼ˆå¼ºåŒ–ï¼‰
            if is_science_host and not has_male and not has_female:
                gender_negative = "female, woman, girl, å¥³æ€§, å¥³äºº, å¥³å­©, å¥³å£«, å°å§, å¥³æ€§ç‰¹å¾, å¥³æ€§å½¢è±¡, feminine, female features, female appearance, feminine appearance, woman features"
                style_negative = f"{gender_negative}, {style_negative}"
            
            # åªåœ¨è´Ÿé¢æç¤ºè¯ä¸­ä¸åŒ…å«è¿™äº›è¯æ—¶æ‰æ·»åŠ 
            if optimized_negative:
                if "cartoon" not in optimized_negative.lower() and "å¡é€š" not in optimized_negative:
                    optimized_negative = f"{optimized_negative}, {style_negative}".strip(", ")
            else:
                optimized_negative = style_negative.strip(", ")
        
        return optimized_prompt, optimized_negative
    
    def _load_character_profiles(self) -> Dict[str, Any]:
        """åŠ è½½è§’è‰²æè¿°é…ç½®æ–‡ä»¶"""
        profile_path = Path(__file__).parent / "character_profiles.yaml"
        if not profile_path.exists():
            print(f"  âš ï¸  è§’è‰²æè¿°æ–‡ä»¶ä¸å­˜åœ¨: {profile_path}ï¼Œå°†ä¸ä½¿ç”¨è§’è‰²æè¿°")
            return {}
        
        try:
            with open(profile_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                profiles = data.get("characters", {})
                if profiles:
                    print(f"  âœ… å·²åŠ è½½è§’è‰²æè¿°æ–‡ä»¶: {len(profiles)} ä¸ªè§’è‰²")
                return profiles
        except Exception as e:
            print(f"  âš ï¸  åŠ è½½è§’è‰²æè¿°æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†ä¸ä½¿ç”¨è§’è‰²æè¿°")
            return {}
    
    def _build_character_prompt(self, character_id: str, prompt: str) -> str:
        """
        æ ¹æ®è§’è‰²æè¿°æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šé™åˆ¶é•¿åº¦ï¼Œé¿å…è¶…è¿‡ 77 tokensï¼‰
        
        Args:
            character_id: è§’è‰²IDï¼ˆå¦‚ "host_person"ï¼‰
            prompt: åŸå§‹æç¤ºè¯
            
        Returns:
            åŒ…å«è§’è‰²æè¿°çš„å®Œæ•´æç¤ºè¯ï¼ˆç²¾ç®€ç‰ˆï¼Œç¡®ä¿ä¸è¶…è¿‡ 77 tokensï¼‰
        """
        if character_id not in self.character_profiles:
            return prompt
        
        profile = self.character_profiles[character_id]
        parts = []
        
        # 1. èº«ä»½æè¿°ï¼ˆç²¾ç®€ï¼Œåªä¿ç•™æ ¸å¿ƒï¼‰
        identity = profile.get("identity", "")
        if identity:
            # åªå–ç¬¬ä¸€ä¸ªæ ¸å¿ƒæè¿°ï¼Œç§»é™¤æƒé‡æ ‡è®°
            identity_short = identity.split(",")[0].split(":")[0].strip()
            if identity_short:
                parts.append(identity_short)
        
        # 2. é¢éƒ¨ç‰¹å¾ï¼ˆç²¾ç®€ï¼Œåªä¿ç•™å‰ 2 ä¸ªå…³é”®è¯ï¼‰
        face_keywords = profile.get("face_keywords", "")
        if face_keywords:
            # åªä¿ç•™å‰ 2 ä¸ªå…³é”®è¯
            face_list = [f.strip() for f in face_keywords.split(",")[:2]]
            if face_list:
                parts.append(", ".join(face_list))
        
        # 3. å‘å‹ï¼ˆç²¾ç®€ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªæ ¸å¿ƒæè¿°ï¼‰
        hair = profile.get("hair", {})
        hair_keywords = hair.get("prompt_keywords", "")
        if hair_keywords:
            # åªä¿ç•™ç¬¬ä¸€ä¸ªæ ¸å¿ƒæè¿°ï¼ˆç§»é™¤æƒé‡ï¼‰
            hair_first = hair_keywords.split(",")[0].split(":")[0].strip()
            if hair_first:
                parts.append(hair_first)
        elif not hair_keywords:
            # ä½¿ç”¨å¤‡ç”¨å­—æ®µï¼ˆåªä¿ç•™æ ·å¼ï¼‰
            hair_style = hair.get("style", "")
            if hair_style:
                parts.append(hair_style.split()[0] if hair_style else "")
        
        # 4. æœé¥°ï¼ˆç²¾ç®€ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªæ ¸å¿ƒæè¿°ï¼‰
        clothes = profile.get("clothes", {})
        clothes_keywords = clothes.get("prompt_keywords", "")
        if clothes_keywords:
            # åªä¿ç•™ç¬¬ä¸€ä¸ªæ ¸å¿ƒæè¿°ï¼ˆç§»é™¤æƒé‡ï¼‰
            clothes_first = clothes_keywords.split(",")[0].split(":")[0].strip()
            if clothes_first:
                parts.append(clothes_first)
        elif not clothes_keywords:
            # ä½¿ç”¨å¤‡ç”¨å­—æ®µï¼ˆåªä¿ç•™æ ·å¼ï¼‰
            clothes_style = clothes.get("style", "")
            if clothes_style:
                parts.append(clothes_style.split()[0] if clothes_style else "")
        
        # 5. ç»„åˆï¼šè§’è‰²æè¿° + åŸå§‹ promptï¼ˆç¡®ä¿ä¸è¶…è¿‡ 77 tokensï¼‰
        # ä¼°ç®— token æ•°é‡ï¼ˆç²—ç•¥ï¼š1 token â‰ˆ 0.75 ä¸ªå•è¯ï¼Œä¸­æ–‡ 1 å­— â‰ˆ 1 tokenï¼‰
        character_desc = ", ".join(parts)
        
        # å¦‚æœè§’è‰²æè¿°å¤ªé•¿ï¼Œè¿›ä¸€æ­¥ç²¾ç®€
        # ç›®æ ‡ï¼šè§’è‰²æè¿° < 20 tokensï¼Œprompt < 57 tokensï¼Œæ€»è®¡ < 77 tokens
        if len(character_desc) > 30:  # ç²—ç•¥ä¼°ç®—ï¼š30 å­—ç¬¦ â‰ˆ 20 tokens
            # åªä¿ç•™èº«ä»½å’Œç¬¬ä¸€ä¸ªç‰¹å¾
            parts = parts[:2] if len(parts) >= 2 else parts
            character_desc = ", ".join(parts)
        
        # å¦‚æœ prompt å¤ªé•¿ï¼Œæˆªæ–­ï¼ˆä¿ç•™å‰ 40 å­—ç¬¦ï¼‰
        prompt_short = prompt
        if len(prompt) > 40:
            prompt_short = prompt[:40] + "..."
        
        enhanced_prompt = f"{character_desc}, {prompt_short}"
        
        # æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œåªä¿ç•™æ ¸å¿ƒéƒ¨åˆ†
        if len(enhanced_prompt) > 60:  # 60 å­—ç¬¦ â‰ˆ 40 tokensï¼ˆå®‰å…¨èŒƒå›´ï¼‰
            # åªä¿ç•™ï¼šèº«ä»½ + prompt å‰ 30 å­—ç¬¦
            if parts:
                enhanced_prompt = f"{parts[0]}, {prompt[:30]}"
            else:
                enhanced_prompt = prompt[:50]
        
        return enhanced_prompt
    
    def unload(self, model_name: Optional[str] = None):
        """
        å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ŒNone æ—¶å¸è½½æ‰€æœ‰æ¨¡å‹
        """
        if model_name is None:
            # å¸è½½æ‰€æœ‰æ¨¡å‹
            for pipeline in self.pipelines.values():
                pipeline.unload()
            self.pipelines.clear()
            torch.cuda.empty_cache()
            print("âœ… æ‰€æœ‰æ¨¡å‹å·²å¸è½½")
        else:
            # å¸è½½æŒ‡å®šæ¨¡å‹
            if model_name in self.pipelines:
                self.pipelines[model_name].unload()
                del self.pipelines[model_name]
                torch.cuda.empty_cache()
                print(f"âœ… {model_name} å·²å¸è½½")
    
    def list_models(self) -> Dict[str, bool]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹åŠå…¶çŠ¶æ€"""
        status = {}
        for model_name, model_path in self.model_paths.items():
            exists = Path(model_path).exists()
            loaded = model_name in self.pipelines and self.pipelines[model_name].loaded
            status[model_name] = {
                "exists": exists,
                "loaded": loaded,
                "path": model_path
            }
        return status


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»º ModelManager
    manager = ModelManager(lazy_load=True)
    
    # ç¤ºä¾‹ 1: ç”Ÿæˆç§‘æ™®ä¸»æŒäººè„¸
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 1: ç”Ÿæˆç§‘æ™®ä¸»æŒäººè„¸")
    print("="*80)
    # img1 = manager.generate(
    #     task="host_face",
    #     prompt="ä¸€ä½æ¸©æš–äº²å’Œçš„ä¸­å›½ç§‘æ™®ä¸»æŒäººï¼Œæ­£é¢å¯¹é•œå¤´ï¼Œä¸“ä¸šå½¢è±¡",
    #     width=1024,
    #     height=1024
    # )
    # img1.save("host_face.png")
    # print("âœ… å·²ä¿å­˜: host_face.png")
    
    # ç¤ºä¾‹ 2: ç”Ÿæˆç§‘å­¦èƒŒæ™¯
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 2: ç”Ÿæˆç§‘å­¦èƒŒæ™¯")
    print("="*80)
    # img2 = manager.generate(
    #     task="science_background",
    #     prompt="é‡å­è®¡ç®—æœºæ ¸å¿ƒå…‰å­¦å…ƒä»¶ï¼Œè“è‰²å…‰æ™•ï¼Œé«˜ç§‘æŠ€ï¼Œæœªæ¥æ„Ÿ",
    #     width=1024,
    #     height=1024
    # )
    # img2.save("science_background.png")
    # print("âœ… å·²ä¿å­˜: science_background.png")
    
    # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹çŠ¶æ€
    print("\n" + "="*80)
    print("æ¨¡å‹çŠ¶æ€")
    print("="*80)
    status = manager.list_models()
    for model_name, info in status.items():
        exists = "âœ…" if info["exists"] else "âŒ"
        loaded = "å·²åŠ è½½" if info["loaded"] else "æœªåŠ è½½"
        print(f"{exists} {model_name}: {loaded} ({info['path']})")


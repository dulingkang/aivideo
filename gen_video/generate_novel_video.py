#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°è¯´æ¨æ–‡è§†é¢‘ç”Ÿæˆè„šæœ¬
ä½¿ç”¨ Flux ç”Ÿæˆå›¾ç‰‡ï¼Œç„¶åç”¨ HunyuanVideo ç”Ÿæˆè§†é¢‘
"""

import sys
from pathlib import Path
from typing import Optional, Dict, Any
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from image_generator import ImageGenerator
from video_generator import VideoGenerator
from PIL import Image


class NovelVideoGenerator:
    """å°è¯´æ¨æ–‡è§†é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        self.config_path = Path(config_path)
        if not self.config_path.is_absolute():
            self.config_path = (project_root / self.config_path).resolve()
        
        # åŠ è½½é…ç½®
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–å›¾åƒç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ Fluxï¼‰
        print("=" * 60)
        print("åˆå§‹åŒ–å›¾åƒç”Ÿæˆå™¨ï¼ˆFluxï¼‰...")
        self.image_generator = ImageGenerator(str(self.config_path))
        
        # åˆå§‹åŒ–è§†é¢‘ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ HunyuanVideoï¼‰
        print("åˆå§‹åŒ–è§†é¢‘ç”Ÿæˆå™¨ï¼ˆHunyuanVideoï¼‰...")
        self.video_generator = VideoGenerator(str(self.config_path))
        
        # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹
        self._ensure_model_config()
        
        print("=" * 60)
        print("âœ… åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60)
    
    def _ensure_model_config(self):
        """ç¡®ä¿é…ç½®ä½¿ç”¨ Flux + HunyuanVideo"""
        # ä¿®æ”¹é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨ Flux ç”Ÿæˆå›¾åƒ
        image_config = self.config.get('image', {})
        if image_config.get('engine') != 'flux-instantid':
            print("  âš  è­¦å‘Š: image.engine ä¸æ˜¯ flux-instantidï¼Œå»ºè®®ä¿®æ”¹é…ç½®")
        
        # ä¿®æ”¹é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨ HunyuanVideo ç”Ÿæˆè§†é¢‘
        video_config = self.config.get('video', {})
        if video_config.get('model_type') != 'hunyuanvideo':
            print("  âš  è­¦å‘Š: video.model_type ä¸æ˜¯ hunyuanvideoï¼Œå»ºè®®ä¿®æ”¹é…ç½®")
            print("  â„¹ ä¸´æ—¶ä¿®æ”¹é…ç½®ä¸º hunyuanvideo")
            video_config['model_type'] = 'hunyuanvideo'
            self.video_generator.video_config['model_type'] = 'hunyuanvideo'
    
    def generate(
        self,
        prompt: str,
        output_dir: Optional[Path] = None,
        image_output_path: Optional[Path] = None,
        video_output_path: Optional[Path] = None,
        width: int = 1280,
        height: int = 768,
        num_frames: int = 120,
        fps: int = 24,
        scene: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Path]:
        """
        ç”Ÿæˆå°è¯´æ¨æ–‡è§†é¢‘
        
        Args:
            prompt: æ–‡æœ¬æç¤ºè¯ï¼ˆå°è¯´åœºæ™¯æè¿°ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            image_output_path: å›¾åƒè¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            video_output_path: è§†é¢‘è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            width: å›¾åƒå®½åº¦
            height: å›¾åƒé«˜åº¦
            num_frames: è§†é¢‘å¸§æ•°
            fps: è§†é¢‘å¸§ç‡
            scene: åœºæ™¯é…ç½®ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            dict: åŒ…å« 'image' å’Œ 'video' è·¯å¾„çš„å­—å…¸
        """
        print("=" * 60)
        print("å¼€å§‹ç”Ÿæˆå°è¯´æ¨æ–‡è§†é¢‘")
        print("=" * 60)
        print(f"æç¤ºè¯: {prompt}")
        print()
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = project_root / "outputs" / "novel_videos"
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ­¥éª¤1: ä½¿ç”¨ Flux ç”Ÿæˆå›¾åƒ
        print("=" * 60)
        print("æ­¥éª¤1: ä½¿ç”¨ Flux ç”Ÿæˆå›¾åƒ")
        print("=" * 60)
        
        if image_output_path is None:
            image_output_path = output_dir / "novel_image.png"
        
        try:
            # æ„å»ºsceneå­—å…¸ï¼ˆåŒ…å«widthå’Œheightï¼‰
            image_scene = scene.copy() if scene else {}
            image_scene['width'] = width
            image_scene['height'] = height
            
            # ç”Ÿæˆå›¾åƒï¼ˆä½¿ç”¨çº¯ Flux ç”Ÿæˆåœºæ™¯ï¼Œä¸ä½¿ç”¨ InstantIDï¼‰
            # å¯¹äºå°è¯´æ¨æ–‡ï¼Œåº”è¯¥ç”Ÿæˆåœºæ™¯å›¾åƒï¼Œè€Œä¸æ˜¯äººç‰©å›¾åƒ
            print(f"  [DEBUG] åŸå§‹prompt: {prompt}")
            print(f"  [DEBUG] scene: {image_scene}")
            print(f"  [DEBUG] model_engine: flux1")
            print(f"  [DEBUG] task_type: scene")
            
            # ä½¿ç”¨ Prompt Engine V2 ä¼˜åŒ–æç¤ºè¯ï¼ˆå®Œå…¨æœ¬åœ°æ¨¡å¼ï¼Œæ— éœ€LLM APIï¼‰
            print(f"  ğŸ”§ å¼€å§‹ä¼˜åŒ–æç¤ºè¯ï¼ˆä½¿ç”¨ Prompt Engine V2 æœ¬åœ°æ¨¡å¼ï¼‰...")
            original_prompt = prompt
            negative_prompt = None
            optimized_prompt = None
            
            try:
                from utils.prompt_engine_v2 import PromptEngine, UserRequest
                
                # åˆ›å»º Prompt Engine V2ï¼ˆé»˜è®¤æœ¬åœ°æ¨¡å¼ï¼Œæ— éœ€LLM APIï¼‰
                prompt_engine_v2 = PromptEngine()
                
                # åˆ›å»ºç”¨æˆ·è¯·æ±‚ï¼ˆå›¾åƒç”Ÿæˆé˜¶æ®µï¼‰
                req = UserRequest(
                    text=original_prompt,
                    scene_type="novel",  # å°è¯´æ¨æ–‡åœºæ™¯
                    style="novel",  # ä½¿ç”¨novelé£æ ¼æ¨¡æ¿
                    target_model="flux",  # å›¾åƒç”Ÿæˆä½¿ç”¨Flux
                    params={"width": width, "height": height}
                )
                
                # æ‰§è¡Œå¤„ç†
                pkg = prompt_engine_v2.run(req)
                
                # è·å–ä¼˜åŒ–åçš„promptå’Œnegative prompt
                optimized_prompt = pkg.final_prompt
                negative_prompt = pkg.negative
                
                # æ£€æŸ¥å¹¶é™åˆ¶æç¤ºè¯é•¿åº¦ï¼ˆCLIPé™åˆ¶77 tokensï¼‰
                def count_tokens(text: str) -> int:
                    """ä¼°ç®—tokenæ•°é‡ï¼ˆç®€å•æ–¹æ³•ï¼‰"""
                    try:
                        from transformers import CLIPTokenizer
                        tokenizer = CLIPTokenizer.from_pretrained(
                            "openai/clip-vit-large-patch14"
                        )
                        tokens = tokenizer(text, truncation=False, return_tensors="pt")
                        return tokens.input_ids.shape[1]
                    except Exception:
                        # å¦‚æœæ— æ³•åŠ è½½tokenizerï¼Œä½¿ç”¨ç®€å•ä¼°ç®—
                        # ä¸­æ–‡çº¦1.5 tokens/å­—ï¼Œè‹±æ–‡çº¦1.3 tokens/è¯
                        chinese_chars = sum(1 for c in text if ord(c) > 127)
                        english_words = len([w for w in text.split() if not any(ord(c) > 127 for c in w)])
                        return int(chinese_chars * 1.5 + english_words * 1.3)
                
                def truncate_prompt(prompt: str, max_tokens: int = 77) -> str:
                    """æˆªæ–­promptåˆ°æŒ‡å®štokenæ•°"""
                    current_tokens = count_tokens(prompt)
                    if current_tokens <= max_tokens:
                        return prompt
                    
                    # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œé€æ­¥ç§»é™¤åé¢çš„éƒ¨åˆ†
                    parts = [p.strip() for p in prompt.split(',')]
                    truncated_parts = []
                    truncated_prompt = ""
                    
                    for part in parts:
                        test_prompt = truncated_prompt + (", " if truncated_prompt else "") + part
                        if count_tokens(test_prompt) <= max_tokens:
                            truncated_parts.append(part)
                            truncated_prompt = test_prompt
                        else:
                            break
                    
                    if not truncated_parts:
                        # å¦‚æœç¬¬ä¸€éƒ¨åˆ†å°±è¶…è¿‡ï¼Œç›´æ¥æˆªæ–­å­—ç¬¦ä¸²
                        return prompt[:int(len(prompt) * max_tokens / current_tokens)]
                    
                    return ", ".join(truncated_parts)
                
                # å…ˆæ£€æŸ¥ä¼˜åŒ–åçš„prompté•¿åº¦
                optimized_tokens = count_tokens(optimized_prompt)
                print(f"  â„¹ ä¼˜åŒ–åprompt tokenæ•°: {optimized_tokens}")
                
                # æ·»åŠ åœºæ™¯å¼ºåŒ–å…³é”®è¯ï¼ˆç¡®ä¿æ˜¯åœºæ™¯è€Œéäººç‰©ï¼‰
                # ä½¿ç”¨æ›´ç®€æ´çš„scene_enhancersï¼Œé¿å…è¶…è¿‡tokené™åˆ¶
                scene_enhancers = "landscape, nature, no people"
                
                # æ£€æŸ¥æ·»åŠ scene_enhancersåæ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
                test_prompt = f"{optimized_prompt}, {scene_enhancers}"
                test_tokens = count_tokens(test_prompt)
                
                if test_tokens > 77:
                    print(f"  âš  æ·»åŠ scene_enhancersåä¼šè¶…è¿‡77 tokens ({test_tokens})ï¼Œå…ˆæˆªæ–­optimized_prompt")
                    # é¢„ç•™ç©ºé—´ç»™scene_enhancersï¼ˆçº¦5 tokensï¼‰
                    optimized_prompt = truncate_prompt(optimized_prompt, max_tokens=72)
                    optimized_tokens = count_tokens(optimized_prompt)
                    print(f"  â„¹ æˆªæ–­åprompt tokenæ•°: {optimized_tokens}")
                
                optimized_prompt = f"{optimized_prompt}, {scene_enhancers}"
                final_tokens = count_tokens(optimized_prompt)
                print(f"  â„¹ æœ€ç»ˆprompt tokenæ•°: {final_tokens}")
                
                if final_tokens > 77:
                    print(f"  âš  æœ€ç»ˆpromptä»ç„¶è¶…è¿‡77 tokens ({final_tokens})ï¼Œè¿›è¡Œæˆªæ–­")
                    optimized_prompt = truncate_prompt(optimized_prompt, max_tokens=77)
                    final_tokens = count_tokens(optimized_prompt)
                    print(f"  â„¹ æˆªæ–­åæœ€ç»ˆprompt tokenæ•°: {final_tokens}")
                
                # å¢å¼ºè´Ÿé¢æç¤ºè¯ï¼ˆç¡®ä¿æ’é™¤äººç‰©ï¼‰
                additional_negatives = [
                    "faces, portraits, black faces, dark faces, human faces, person faces, character faces",
                    "people in image, humans in scene, any people, any persons, any characters, any human figures"
                ]
                negative_prompt = f"{negative_prompt}, {', '.join(additional_negatives)}"
                
                print(f"  âœ“ Prompt Engine V2 å¤„ç†å®Œæˆ")
                print(f"  â„¹ åŸå§‹æç¤ºè¯: {original_prompt[:80]}...")
                print(f"  â„¹ ä¼˜åŒ–åæç¤ºè¯: {optimized_prompt[:100]}...")
                print(f"  â„¹ QAè¯„åˆ†: {pkg.metadata.get('qa_score', 0)}/{pkg.metadata.get('qa_max_score', 0)}")
                
            except Exception as e:
                print(f"  âš  Prompt Engine V2 å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                import traceback
                traceback.print_exc()
                
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹æç¤ºè¯+åœºæ™¯å¼ºåŒ–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…è¶…è¿‡tokené™åˆ¶ï¼‰
                scene_enhancers = "landscape, nature, no people"
                optimized_prompt = f"{original_prompt}, {scene_enhancers}"
                
                # æ£€æŸ¥tokenæ•°
                try:
                    from transformers import CLIPTokenizer
                    tokenizer = CLIPTokenizer.from_pretrained(
                        "openai/clip-vit-large-patch14"
                    )
                    tokens = tokenizer(optimized_prompt, truncation=False, return_tensors="pt")
                    token_count = tokens.input_ids.shape[1]
                    if token_count > 77:
                        print(f"  âš  å¤‡ç”¨æ–¹æ¡ˆpromptè¶…è¿‡77 tokens ({token_count})ï¼Œå°†è¢«CLIPæˆªæ–­")
                except Exception:
                    pass
                negative_prompt = "anime, cartoon, characters, people, persons, human figures, anime style, cartoon style, faces, portraits, black faces, dark faces, human faces, person faces, character faces, people in image, humans in scene, any people, any persons, any characters, any human figures, low quality, blurry, distorted, deformed, bad anatomy, bad hands, text, watermark, flickering, jittery, unstable, sudden movement, abrupt changes, low quality, worst quality, distorted proportions, unrealistic details"
            
            print(f"  âœ… æç¤ºè¯ä¼˜åŒ–å®Œæˆ:")
            print(f"     åŸå§‹: {original_prompt}")
            print(f"     ä¼˜åŒ–å: {optimized_prompt[:150]}...")
            print(f"     è´Ÿé¢æç¤ºè¯: {negative_prompt[:150]}...")
            
            prompt = optimized_prompt
            negative_prompt = negative_prompt
            
            # ç¡®ä¿sceneä¸­ä¸åŒ…å«è§’è‰²ä¿¡æ¯ï¼Œé¿å…è¢«è¯¯è¯†åˆ«ä¸ºäººç‰©ç”Ÿæˆ
            if image_scene:
                # ç§»é™¤å¯èƒ½è§¦å‘è§’è‰²æ£€æµ‹çš„å­—æ®µ
                image_scene.pop('character', None)
                image_scene.pop('characters', None)
                image_scene.pop('primary_character', None)
                image_scene.pop('face_reference_image_path', None)
                image_scene.pop('reference_image_path', None)
                print(f"  [DEBUG] å·²æ¸…ç†sceneä¸­çš„è§’è‰²ç›¸å…³å­—æ®µï¼Œç¡®ä¿ç”Ÿæˆåœºæ™¯å›¾åƒ")
            
            image_path = self.image_generator.generate_image(
                prompt=prompt,
                output_path=image_output_path,
                scene=image_scene,
                model_engine="flux1",  # ä½¿ç”¨çº¯ Flux 1ï¼Œä¸åŒ…å« InstantIDï¼ˆç”¨äºåœºæ™¯ç”Ÿæˆï¼‰
                task_type="scene",  # æ˜ç¡®æŒ‡å®šä¸ºåœºæ™¯ç”Ÿæˆä»»åŠ¡
                character_lora=None,  # æ˜ç¡®ä¸ä½¿ç”¨è§’è‰²LoRA
                use_lora=False,  # æ˜ç¡®ä¸ä½¿ç”¨LoRA
                face_reference_image_path=None,  # æ˜ç¡®ä¸ä½¿ç”¨é¢éƒ¨å‚è€ƒå›¾
                reference_image_path=None,  # æ˜ç¡®ä¸ä½¿ç”¨å‚è€ƒå›¾
                negative_prompt=negative_prompt,  # ä½¿ç”¨ä¼˜åŒ–åçš„è´Ÿé¢æç¤ºè¯
            )
            print(f"âœ… å›¾åƒç”ŸæˆæˆåŠŸ: {image_path}")
            
            # è¯»å–ç”Ÿæˆå›¾åƒçš„å®é™…åˆ†è¾¨ç‡ï¼Œç¡®ä¿è§†é¢‘ä½¿ç”¨ç›¸åŒçš„åˆ†è¾¨ç‡
            from PIL import Image as PILImage
            generated_image = PILImage.open(image_path)
            actual_image_width, actual_image_height = generated_image.size
            image_aspect_ratio = actual_image_width / actual_image_height
            print(f"  â„¹ ç”Ÿæˆå›¾åƒå®é™…åˆ†è¾¨ç‡: {actual_image_width}x{actual_image_height} (å®½é«˜æ¯”: {image_aspect_ratio:.2f})")
            
            # æ›´æ–°widthå’Œheightä¸ºå›¾åƒçš„å®é™…åˆ†è¾¨ç‡
            width = actual_image_width
            height = actual_image_height
        except Exception as e:
            print(f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        # æ¸…ç†å›¾åƒç”Ÿæˆå™¨çš„æ¨¡å‹å’Œæ˜¾å­˜ï¼Œä¸ºè§†é¢‘ç”Ÿæˆé‡Šæ”¾æ˜¾å­˜
        print()
        print("=" * 60)
        print("æ¸…ç†å›¾åƒç”Ÿæˆå™¨æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜")
        print("=" * 60)
        try:
            import torch
            import gc
            
            # è®°å½•æ¸…ç†å‰çš„æ˜¾å­˜çŠ¶æ€
            if torch.cuda.is_available():
                allocated_before = torch.cuda.memory_allocated() / 1024**3
                reserved_before = torch.cuda.memory_reserved() / 1024**3
                print(f"  â„¹ æ¸…ç†å‰æ˜¾å­˜: å·²åˆ†é…={allocated_before:.2f}GB, å·²ä¿ç•™={reserved_before:.2f}GB")
            
            # æ¸…ç†æ‰€æœ‰å¯èƒ½çš„pipelineå¼•ç”¨ï¼ˆå…ˆè°ƒç”¨unloadï¼Œå†åˆ é™¤å¼•ç”¨ï¼‰
            pipelines_to_clean = [
                'pipeline',
                'flux_pipeline',
                'flux1_pipeline',  # Flux.1 pipeline
                'flux2_pipeline',  # Flux.2 pipeline
                'sdxl_pipeline',
                'instantid_pipeline',
                'kolors_pipeline',
                'hunyuan_dit_pipeline',
            ]
            
            for pipeline_name in pipelines_to_clean:
                if hasattr(self.image_generator, pipeline_name):
                    pipeline = getattr(self.image_generator, pipeline_name)
                    if pipeline is not None:
                        try:
                            # å…ˆå°è¯•è°ƒç”¨unloadæ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                            if hasattr(pipeline, 'unload'):
                                pipeline.unload()
                                print(f"  âœ“ å·²å¸è½½ {pipeline_name} (é€šè¿‡unloadæ–¹æ³•)")
                            elif hasattr(pipeline, 'pipe'):
                                # å¦‚æœæ˜¯diffusers pipelineï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°CPUå¹¶åˆ é™¤
                                pipe = pipeline.pipe
                                try:
                                    # ç§»åŠ¨åˆ°CPU
                                    if hasattr(pipe, 'to'):
                                        pipe.to('cpu')
                                    # åˆ é™¤æ‰€æœ‰ç»„ä»¶
                                    components = ['transformer', 'vae', 'text_encoder', 'text_encoder_2', 'tokenizer', 'tokenizer_2']
                                    for comp_name in components:
                                        if hasattr(pipe, comp_name):
                                            comp = getattr(pipe, comp_name)
                                            if comp is not None:
                                                try:
                                                    if hasattr(comp, 'to'):
                                                        comp.to('cpu')
                                                    del comp
                                                except:
                                                    pass
                                    # åˆ é™¤pipe
                                    del pipe
                                    print(f"  âœ“ å·²å¸è½½ {pipeline_name} (æ‰‹åŠ¨æ¸…ç†diffusers pipeline)")
                                except Exception as e:
                                    print(f"  âš  æ‰‹åŠ¨æ¸…ç† {pipeline_name} æ—¶å‡ºé”™: {e}")
                        except Exception as e:
                            print(f"  âš  å¸è½½ {pipeline_name} æ—¶å‡ºé”™: {e}")
                        finally:
                            # åˆ é™¤å¼•ç”¨
                            try:
                                delattr(self.image_generator, pipeline_name)
                                setattr(self.image_generator, pipeline_name, None)
                            except:
                                pass
            
            # æ¸…ç†ModelManagerï¼ˆå¦‚æœä½¿ç”¨ï¼‰
            if hasattr(self.image_generator, 'model_manager') and self.image_generator.model_manager is not None:
                try:
                    if hasattr(self.image_generator.model_manager, 'unload'):
                        self.image_generator.model_manager.unload()
                        print("  âœ“ å·²å¸è½½ModelManageræ‰€æœ‰æ¨¡å‹")
                except Exception as e:
                    print(f"  âš  å¸è½½ModelManageræ—¶å‡ºé”™: {e}")
            
            # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰CUDAç¼“å­˜
            if torch.cuda.is_available():
                # å¤šæ¬¡æ¸…ç†ï¼Œç¡®ä¿å½»åº•é‡Šæ”¾
                for i in range(10):  # å¢åŠ åˆ°10æ¬¡
                    torch.cuda.empty_cache()
                    gc.collect()
                torch.cuda.synchronize()
                
                # å†æ¬¡æ¸…ç†
                torch.cuda.empty_cache()
                gc.collect()
                
                allocated_after = torch.cuda.memory_allocated() / 1024**3
                reserved_after = torch.cuda.memory_reserved() / 1024**3
                freed = allocated_before - allocated_after if torch.cuda.is_available() else 0
                print(f"  â„¹ æ¸…ç†åæ˜¾å­˜: å·²åˆ†é…={allocated_after:.2f}GB, å·²ä¿ç•™={reserved_after:.2f}GB")
                if freed > 0:
                    print(f"  âœ“ å·²é‡Šæ”¾æ˜¾å­˜: {freed:.2f}GB")
                else:
                    print(f"  âš  è­¦å‘Š: æ˜¾å­˜æœªé‡Šæ”¾ï¼ˆå¯èƒ½è¢«å…¶ä»–è¿›ç¨‹å ç”¨ï¼‰")
            
        except Exception as e:
            print(f"  âš  æ¸…ç†æ˜¾å­˜æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        
        # æ­¥éª¤2: ä½¿ç”¨ HunyuanVideo ç”Ÿæˆè§†é¢‘
        print()
        print("=" * 60)
        print("æ­¥éª¤2: ä½¿ç”¨ HunyuanVideo ç”Ÿæˆè§†é¢‘")
        print("=" * 60)
        
        if video_output_path is None:
            video_output_path = output_dir / "novel_video.mp4"
        
        try:
            # æ„å»ºè§†é¢‘ç”Ÿæˆæç¤ºè¯ï¼ˆå¯ä»¥æ›´è¯¦ç»†ï¼Œæè¿°è¿åŠ¨æ–¹å¼ï¼‰
            video_prompt = self._build_video_prompt(prompt, scene)
            
            # æ„å»ºsceneå­—å…¸ï¼ˆåŒ…å«promptä¿¡æ¯å’Œåˆ†è¾¨ç‡ï¼‰
            video_scene = scene.copy() if scene else {}
            video_scene['description'] = video_prompt
            video_scene['prompt'] = video_prompt  # ä¹Ÿæ·»åŠ åˆ°promptå­—æ®µ
            # é‡è¦ï¼šç¡®ä¿è§†é¢‘ä½¿ç”¨ä¸å›¾åƒç›¸åŒçš„åˆ†è¾¨ç‡ï¼Œä¿æŒé•¿å®½æ¯”ä¸€è‡´
            # widthå’Œheightå·²ç»åœ¨å›¾åƒç”Ÿæˆåæ›´æ–°ä¸ºå®é™…åˆ†è¾¨ç‡
            video_scene['width'] = width  # ä½¿ç”¨å›¾åƒçš„å®é™…å®½åº¦
            video_scene['height'] = height  # ä½¿ç”¨å›¾åƒçš„å®é™…é«˜åº¦
            print(f"  â„¹ è§†é¢‘å°†ä½¿ç”¨åˆ†è¾¨ç‡: {width}x{height} (ä¸å›¾åƒä¸€è‡´ï¼Œä¿æŒé•¿å®½æ¯” {width/height:.2f})")
            
            # ç”Ÿæˆè§†é¢‘
            video_path = self.video_generator.generate_video(
                image_path=str(image_path),
                output_path=str(video_output_path),
                num_frames=num_frames,
                fps=fps,
                scene=video_scene,
            )
            print(f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ: {video_path}")
        except Exception as e:
            print(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        print()
        print("=" * 60)
        print("âœ… å°è¯´æ¨æ–‡è§†é¢‘ç”Ÿæˆå®Œæˆ")
        print("=" * 60)
        print(f"å›¾åƒ: {image_path}")
        print(f"è§†é¢‘: {video_path}")
        
        return {
            'image': image_path,
            'video': video_path,
        }
    
    def _build_video_prompt(self, image_prompt: str, scene: Optional[Dict[str, Any]] = None) -> str:
        """
        æ„å»ºè§†é¢‘ç”Ÿæˆæç¤ºè¯ï¼ˆä½¿ç”¨ Prompt Engine V2ï¼‰
        
        Args:
            image_prompt: å›¾åƒç”Ÿæˆæ—¶çš„æç¤ºè¯
            scene: åœºæ™¯é…ç½®
            
        Returns:
            ä¼˜åŒ–åçš„è§†é¢‘ç”Ÿæˆæç¤ºè¯
        """
        def _extract_scene_motion(prompt_text: str) -> list:
            """
            ä»promptä¸­æå–åœºæ™¯å…ƒç´ ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„è¿åŠ¨æè¿°
            
            Args:
                prompt_text: æç¤ºè¯æ–‡æœ¬
                
            Returns:
                åœºæ™¯å…ƒç´ çš„è¿åŠ¨æè¿°åˆ—è¡¨
            """
            motion_keywords = {
                # æ°´ç›¸å…³ - ä½¿ç”¨æ›´å¼ºçƒˆçš„è¿åŠ¨æè¿°
                'ç€‘å¸ƒ': ['waterfall continuously flowing down', 'water cascading and rushing', 'waterfall in motion'],
                'ç€‘å¸ƒæµ': ['waterfall continuously flowing', 'water cascading'],
                'waterfall': ['waterfall continuously flowing down', 'water cascading'],
                'æ²³æµ': ['river flowing and streaming', 'water continuously moving'],
                'river': ['river flowing and streaming'],
                'æºªæµ': ['stream flowing and trickling', 'water moving'],
                'stream': ['stream flowing and trickling'],
                'æ°´': ['water rippling and flowing', 'water in motion'],
                'water': ['water rippling and flowing'],
                'æ¹–': ['lake rippling with waves', 'water gently moving'],
                'lake': ['lake rippling with waves'],
                'æµ·': ['waves rolling and crashing', 'ocean waves in motion'],
                'sea': ['waves rolling and crashing'],
                'ocean': ['waves rolling and crashing'],
                
                # å¤©ç©ºç›¸å…³ - ä½¿ç”¨æ›´æ˜ç¡®çš„è¿åŠ¨æè¿°
                'äº‘': ['clouds slowly drifting across the sky', 'clouds moving in the wind'],
                'äº‘å½©': ['clouds slowly drifting across the sky', 'clouds moving'],
                'cloud': ['clouds slowly drifting', 'clouds moving'],
                'clouds': ['clouds slowly drifting across the sky', 'clouds moving'],
                'å½©è™¹': ['rainbow shimmering and glowing', 'rainbow light effects in motion'],
                'rainbow': ['rainbow shimmering and glowing'],
                'é˜³å…‰': ['sunlight shifting and moving', 'light rays in motion'],
                'sunlight': ['sunlight shifting and moving'],
                'å…‰çº¿': ['light rays moving and shifting', 'light in motion'],
                'light': ['light rays moving and shifting'],
                
                # æ¤ç‰©ç›¸å…³ - å¼ºè°ƒè¿åŠ¨
                'æ ‘': ['leaves swaying in the wind', 'trees gently moving'],
                'æ ‘å¶': ['leaves swaying and rustling', 'leaves in motion'],
                'tree': ['leaves swaying in the wind'],
                'leaves': ['leaves swaying and rustling'],
                'è‰': ['grass swaying in the breeze', 'grass moving'],
                'grass': ['grass swaying in the breeze'],
                'èŠ±': ['flowers swaying gently', 'flowers moving'],
                'flower': ['flowers swaying gently'],
                'flowers': ['flowers swaying gently'],
                
                # é£ç›¸å…³
                'é£': ['wind blowing and moving', 'breeze in motion'],
                'wind': ['wind blowing and moving'],
                'breeze': ['wind blowing and moving'],
                
                # é›¾æ°”ç›¸å…³
                'é›¾': ['mist rising and drifting', 'fog moving'],
                'é›¾æ°”': ['mist rising and drifting'],
                'mist': ['mist rising and drifting'],
                'fog': ['mist rising and drifting'],
                
                # ç«ç›¸å…³
                'ç«': ['flames flickering and dancing', 'fire in motion'],
                'ç«ç„°': ['flames flickering and dancing'],
                'fire': ['flames flickering and dancing'],
                'flame': ['flames flickering and dancing'],
                
                # é›ªç›¸å…³
                'é›ª': ['snow falling and drifting', 'snowflakes in motion'],
                'snow': ['snow falling and drifting'],
                'snowflake': ['snow falling and drifting'],
                
                # é¸Ÿç›¸å…³
                'é¸Ÿ': ['birds flying and soaring', 'birds in motion'],
                'bird': ['birds flying and soaring'],
                'birds': ['birds flying and soaring'],
            }
            
            scene_motions = []
            prompt_lower = prompt_text.lower()
            
            # æ£€æŸ¥æ¯ä¸ªå…³é”®è¯
            for keyword, motions in motion_keywords.items():
                if keyword.lower() in prompt_lower:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªè¿åŠ¨æè¿°ï¼ˆæœ€å¸¸ç”¨ï¼‰
                    scene_motions.append(motions[0])
            
            return scene_motions
        
        try:
            from utils.prompt_engine_v2 import PromptEngine, UserRequest
            
            # åˆ›å»º Prompt Engine V2ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰
            prompt_engine_v2 = PromptEngine()
            
            # åˆ›å»ºç”¨æˆ·è¯·æ±‚ï¼ˆè§†é¢‘ç”Ÿæˆé˜¶æ®µï¼‰
            req = UserRequest(
                text=image_prompt,
                scene_type="novel",  # å°è¯´æ¨æ–‡åœºæ™¯
                style="novel",  # ä½¿ç”¨novelé£æ ¼æ¨¡æ¿
                target_model="hunyuanvideo",  # è§†é¢‘ç”Ÿæˆä½¿ç”¨HunyuanVideo
                params=scene.get('params', {}) if scene else {}
            )
            
            # æ‰§è¡Œå¤„ç†
            pkg = prompt_engine_v2.run(req)
            
            # è·å–ä¼˜åŒ–åçš„prompt
            video_prompt = pkg.final_prompt
            
            # æå–åœºæ™¯å…ƒç´ çš„è¿åŠ¨æè¿°ï¼ˆå…³é”®ï¼šæ·»åŠ ç‰©ä½“è¿åŠ¨ï¼Œè€Œä¸ä»…ä»…æ˜¯ç›¸æœºè¿åŠ¨ï¼‰
            scene_motions = _extract_scene_motion(image_prompt)
            
            # å…³é”®ä¿®å¤ï¼šå°†è¿åŠ¨æè¿°ç›´æ¥èå…¥åˆ°promptä¸­ï¼Œè€Œä¸æ˜¯ä½œä¸ºåç¼€
            # HunyuanVideoéœ€è¦è¿åŠ¨æè¿°ç›´æ¥èå…¥åˆ°åœºæ™¯æè¿°ä¸­
            if scene_motions:
                print(f"  â„¹ æ£€æµ‹åˆ°åœºæ™¯å…ƒç´ è¿åŠ¨: {', '.join(scene_motions)}")
                
                # å°†è¿åŠ¨æè¿°ç›´æ¥æ’å…¥åˆ°promptçš„å‰é¢éƒ¨åˆ†ï¼ˆåœ¨ä¸»ä½“æè¿°ä¹‹åï¼‰
                # æ ¼å¼ï¼šä¸»ä½“æè¿° + è¿åŠ¨æè¿° + å…¶ä»–æè¿°
                prompt_parts = video_prompt.split('.')
                if len(prompt_parts) > 1:
                    # åœ¨ç¬¬ä¸€ä¸ªå¥å·åæ’å…¥è¿åŠ¨æè¿°
                    enhanced_prompt = prompt_parts[0] + ". " + ", ".join(scene_motions) + ". " + ". ".join(prompt_parts[1:])
                    video_prompt = enhanced_prompt
                else:
                    # å¦‚æœæ²¡æœ‰å¥å·ï¼Œç›´æ¥æ·»åŠ åˆ°å‰é¢
                    video_prompt = ", ".join(scene_motions) + ". " + video_prompt
            
            # æ·»åŠ è¿åŠ¨æè¿°ï¼ˆå¢å¼ºç‰ˆï¼Œç¡®ä¿ç‰©ä½“è¿åŠ¨ï¼‰
            motion_descriptions = []
            
            # 1. å†æ¬¡å¼ºè°ƒåœºæ™¯å…ƒç´ çš„è¿åŠ¨ï¼ˆä½¿ç”¨æ›´å¼ºçƒˆçš„æè¿°ï¼‰
            if scene_motions:
                # ä½¿ç”¨æ›´å¼ºçƒˆçš„è¿åŠ¨æè¿°
                strong_motions = []
                for motion in scene_motions:
                    if 'flowing' in motion:
                        strong_motions.append("water continuously flowing, dynamic water movement")
                    elif 'drifting' in motion:
                        strong_motions.append("clouds slowly drifting, sky in motion")
                    elif 'shimmering' in motion:
                        strong_motions.append("rainbow shimmering and glowing, light effects in motion")
                    elif 'swaying' in motion:
                        strong_motions.append("leaves gently swaying, natural wind movement")
                    else:
                        strong_motions.append(motion + ", motion visible")
                motion_descriptions.extend(strong_motions)
            
            # 2. æ·»åŠ åœºæ™¯é…ç½®ä¸­çš„è¿åŠ¨å¼ºåº¦
            if scene and isinstance(scene, dict):
                motion_intensity = scene.get('motion_intensity', 'moderate')
                camera_motion = scene.get('camera_motion', {})
                
                if motion_intensity == 'dynamic':
                    motion_descriptions.append("dynamic movement, active motion, objects in motion")
                elif motion_intensity == 'moderate':
                    motion_descriptions.append("moderate movement, natural motion, elements moving")
                else:
                    motion_descriptions.append("gentle movement, subtle motion, natural flow")
                
                # 3. æ·»åŠ ç›¸æœºè¿åŠ¨ï¼ˆæ¬¡è¦ï¼Œé¿å…åªæœ‰ç›¸æœºè¿åŠ¨ï¼‰
                if isinstance(camera_motion, dict):
                    camera_type = camera_motion.get('type', 'static')
                    if camera_type == 'pan':
                        motion_descriptions.append("smooth camera pan")
                    elif camera_type == 'zoom':
                        motion_descriptions.append("smooth camera zoom")
                    elif camera_type == 'dolly':
                        motion_descriptions.append("smooth camera dolly")
            
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°åœºæ™¯è¿åŠ¨ï¼Œæ·»åŠ é»˜è®¤çš„è‡ªç„¶è¿åŠ¨æè¿°
            if not scene_motions:
                motion_descriptions.append("natural movement, subtle motion, elements in motion")
                print(f"  â„¹ æœªæ£€æµ‹åˆ°ç‰¹å®šåœºæ™¯å…ƒç´ ï¼Œæ·»åŠ é»˜è®¤è‡ªç„¶è¿åŠ¨æè¿°")
            
            # ç»„åˆè¿åŠ¨æè¿°ï¼ˆæ·»åŠ åˆ°promptæœ«å°¾ï¼Œä½œä¸ºè¡¥å……ï¼‰
            if motion_descriptions:
                video_prompt += ". " + ", ".join(motion_descriptions)
            
            # æ·»åŠ è§†é¢‘è´¨é‡æè¿°
            video_prompt += ". High quality, cinematic, smooth motion, natural movement, objects in motion"
            
            print(f"  âœ“ è§†é¢‘æç¤ºè¯å·²ä½¿ç”¨ Prompt Engine V2 ä¼˜åŒ–")
            print(f"  â„¹ QAè¯„åˆ†: {pkg.metadata.get('qa_score', 0)}/{pkg.metadata.get('qa_max_score', 0)}")
            
            return video_prompt
            
        except Exception as e:
            print(f"  âš  Prompt Engine V2 å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ¡ˆ")
            import traceback
            traceback.print_exc()
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºç¡€æç¤ºè¯æ„å»º
            video_prompt = image_prompt
            
            # æå–åœºæ™¯å…ƒç´ çš„è¿åŠ¨æè¿°
            scene_motions = _extract_scene_motion(image_prompt)
            
            # å…³é”®ä¿®å¤ï¼šå°†è¿åŠ¨æè¿°ç›´æ¥èå…¥åˆ°promptä¸­
            if scene_motions:
                print(f"  â„¹ æ£€æµ‹åˆ°åœºæ™¯å…ƒç´ è¿åŠ¨: {', '.join(scene_motions)}")
                
                # å°†è¿åŠ¨æè¿°ç›´æ¥æ’å…¥åˆ°promptçš„å‰é¢éƒ¨åˆ†
                prompt_parts = video_prompt.split('.')
                if len(prompt_parts) > 1:
                    enhanced_prompt = prompt_parts[0] + ". " + ", ".join(scene_motions) + ". " + ". ".join(prompt_parts[1:])
                    video_prompt = enhanced_prompt
                else:
                    video_prompt = ", ".join(scene_motions) + ". " + video_prompt
            
            # æ·»åŠ è¿åŠ¨æè¿°ï¼ˆå¢å¼ºç‰ˆï¼‰
            motion_descriptions = []
            
            # 1. å†æ¬¡å¼ºè°ƒåœºæ™¯å…ƒç´ çš„è¿åŠ¨ï¼ˆä½¿ç”¨æ›´å¼ºçƒˆçš„æè¿°ï¼‰
            if scene_motions:
                strong_motions = []
                for motion in scene_motions:
                    if 'flowing' in motion:
                        strong_motions.append("water continuously flowing, dynamic water movement")
                    elif 'drifting' in motion:
                        strong_motions.append("clouds slowly drifting, sky in motion")
                    elif 'shimmering' in motion:
                        strong_motions.append("rainbow shimmering and glowing, light effects in motion")
                    elif 'swaying' in motion:
                        strong_motions.append("leaves gently swaying, natural wind movement")
                    else:
                        strong_motions.append(motion + ", motion visible")
                motion_descriptions.extend(strong_motions)
            
            # 2. æ·»åŠ åœºæ™¯é…ç½®ä¸­çš„è¿åŠ¨å¼ºåº¦
            if scene and isinstance(scene, dict):
                motion_intensity = scene.get('motion_intensity', 'moderate')
                camera_motion = scene.get('camera_motion', {})
                
                if motion_intensity == 'dynamic':
                    motion_descriptions.append("dynamic movement, active motion, objects in motion")
                elif motion_intensity == 'moderate':
                    motion_descriptions.append("moderate movement, natural motion, elements moving")
                else:
                    motion_descriptions.append("gentle movement, subtle motion, natural flow")
                
                # 3. æ·»åŠ ç›¸æœºè¿åŠ¨
                if isinstance(camera_motion, dict):
                    camera_type = camera_motion.get('type', 'static')
                    if camera_type == 'pan':
                        motion_descriptions.append("smooth camera pan")
                    elif camera_type == 'zoom':
                        motion_descriptions.append("smooth camera zoom")
                    elif camera_type == 'dolly':
                        motion_descriptions.append("smooth camera dolly")
            
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°åœºæ™¯è¿åŠ¨ï¼Œæ·»åŠ é»˜è®¤çš„è‡ªç„¶è¿åŠ¨æè¿°
            if not scene_motions:
                motion_descriptions.append("natural movement, subtle motion, elements in motion")
                print(f"  â„¹ æœªæ£€æµ‹åˆ°ç‰¹å®šåœºæ™¯å…ƒç´ ï¼Œæ·»åŠ é»˜è®¤è‡ªç„¶è¿åŠ¨æè¿°")
            
            # ç»„åˆè¿åŠ¨æè¿°
            if motion_descriptions:
                video_prompt += ". " + ", ".join(motion_descriptions)
            
            # æ·»åŠ è´¨é‡æè¿°
            video_prompt += ". High quality, cinematic, smooth motion, natural movement, objects in motion"
            
            return video_prompt


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå°è¯´æ¨æ–‡è§†é¢‘")
    parser.add_argument("--prompt", type=str, required=True, help="æ–‡æœ¬æç¤ºè¯ï¼ˆå°è¯´åœºæ™¯æè¿°ï¼‰")
    parser.add_argument("--output-dir", type=str, default=None, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--width", type=int, default=1280, help="å›¾åƒå®½åº¦")
    parser.add_argument("--height", type=int, default=768, help="å›¾åƒé«˜åº¦")
    parser.add_argument("--num-frames", type=int, default=120, help="è§†é¢‘å¸§æ•°")
    parser.add_argument("--fps", type=int, default=24, help="è§†é¢‘å¸§ç‡")
    parser.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = NovelVideoGenerator(config_path=args.config)
    
    # ç”Ÿæˆè§†é¢‘
    result = generator.generate(
        prompt=args.prompt,
        output_dir=Path(args.output_dir) if args.output_dir else None,
        width=args.width,
        height=args.height,
        num_frames=args.num_frames,
        fps=args.fps,
    )
    
    print("\nç”Ÿæˆå®Œæˆï¼")
    print(f"å›¾åƒ: {result['image']}")
    print(f"è§†é¢‘: {result['video']}")


if __name__ == "__main__":
    main()


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºŽ InsightFace çš„äººè„¸ç›¸ä¼¼åº¦ç­›é€‰è„šæœ¬ã€‚

ç”¨é€”ï¼š
- è¯»å–ç´ æå…ƒæ•°æ® CSV
- ä½¿ç”¨ reference ç›®å½•ä¸­çš„ä¸»è§’ç…§ç‰‡æž„å»ºäººè„¸ç‰¹å¾
- å¯¹å€™é€‰ç´ æé€å¸§æ£€æµ‹äººè„¸å¹¶è®¡ç®—ä¸Žä¸»è§’çš„ä½™å¼¦ç›¸ä¼¼åº¦
- ç”Ÿæˆç­›é€‰æ ‡è®°ï¼Œå¯é€‰æ‹©å¤åˆ¶é€šè¿‡çš„å›¾ç‰‡

ä¾èµ–ï¼š
    pip install insightface onnxruntime-gpu pillow numpy opencv-python

ç¤ºä¾‹ï¼š
    python filter_stills_face.py \
        --metadata ../assets/library/metadata.csv \
        --reference-dir ../reference_image/éŸ©ç«‹ \
        --threshold 0.32 \
        --append-column face_flag \
        --selected-dir ../assets/library/face_selected
"""

from __future__ import annotations

import argparse
import csv
import math
from pathlib import Path
from typing import Iterable, List, Optional, Sequence

import cv2
import numpy as np
from tqdm import tqdm

try:
    from insightface.app import FaceAnalysis  # type: ignore
except ImportError as exc:  # pragma: no cover - runtime dependency
    raise SystemExit(
        "ç¼ºå°‘ insightface ä¾èµ–ï¼Œè¯·å…ˆæ‰§è¡Œ `pip install insightface onnxruntime-gpu`"
    ) from exc


REPO_ROOT = Path(__file__).resolve().parents[2]
DEFAULT_METADATA = REPO_ROOT / "gen_video" / "assets" / "library" / "metadata.csv"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="ä½¿ç”¨äººè„¸è¯†åˆ«ç­›é€‰ç´ æå¸§")
    parser.add_argument(
        "--metadata",
        type=Path,
        default=DEFAULT_METADATA,
        help="ç´ æå…ƒæ•°æ® CSVï¼ˆé»˜è®¤: assets/library/metadata.csvï¼‰",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="ç­›é€‰åŽ CSV è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤è¦†ç›– metadata æ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--image-root",
        type=Path,
        default=None,
        help="å›¾ç‰‡ç›¸å¯¹è·¯å¾„çš„æ ¹ç›®å½•ï¼ˆé»˜è®¤æŒ‰å…ƒæ•°æ®è·¯å¾„è§£æžï¼‰",
    )
    parser.add_argument(
        "--reference-dir",
        type=Path,
        required=True,
        help="ä¸»è§’å‚è€ƒå›¾ç‰‡ç›®å½•ï¼ˆå°†ä½¿ç”¨å…¶ä¸­çš„äººè„¸æž„å»ºç‰¹å¾ï¼‰",
    )
    parser.add_argument(
        "--globs",
        nargs="*",
        default=("*.*",),
        help="å‚è€ƒç›®å½•ä¸‹çš„åŒ¹é…æ¨¡å¼ï¼ˆé»˜è®¤æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.32,
        help="åˆ¤å®šä¸ºä¸»è§’çš„æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.32ï¼‰",
    )
    parser.add_argument(
        "--append-column",
        type=str,
        default="face_flag",
        help="åœ¨ CSV ä¸­å†™å…¥çš„æ ‡è®°åˆ—ï¼ˆé»˜è®¤ face_flagï¼‰",
    )
    parser.add_argument(
        "--score-column",
        type=str,
        default="face_similarity",
        help="åœ¨ CSV ä¸­å†™å…¥ç›¸ä¼¼åº¦åˆ†æ•°çš„åˆ—åï¼ˆé»˜è®¤ face_similarityï¼‰",
    )
    parser.add_argument(
        "--det-size",
        type=int,
        nargs=2,
        metavar=("W", "H"),
        default=(640, 640),
        help="äººè„¸æ£€æµ‹è¾“å…¥å°ºå¯¸ï¼ˆé»˜è®¤ 640x640ï¼‰",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="buffalo_l",
        help="InsightFace æ¨¡åž‹åç§°ï¼ˆé»˜è®¤ buffalo_lï¼‰",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=("cuda", "cpu"),
        help="æŽ¨ç†è®¾å¤‡ï¼ˆé»˜è®¤ cudaï¼Œå¯é€‰ cpuï¼‰",
    )
    parser.add_argument(
        "--selected-dir",
        type=Path,
        help="å¯é€‰ï¼šå¤åˆ¶é€šè¿‡ç­›é€‰çš„å›¾ç‰‡åˆ°æ­¤ç›®å½•",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ä»…è®¡ç®—åˆ†æ•°ï¼Œä¸å†™å…¥ CSV æˆ–å¤åˆ¶æ–‡ä»¶",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="å‡å°‘æ—¥å¿—è¾“å‡º",
    )
    return parser.parse_args()


def load_metadata(path: Path) -> List[dict]:
    if not path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° metadata æ–‡ä»¶: {path}")
    with path.open("r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return list(reader)


def write_metadata(path: Path, rows: Sequence[dict]) -> None:
    if not rows:
        return
    fieldnames = list(rows[0].keys())
    with path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


def resolve_image_path(row: dict, image_root: Optional[Path]) -> Optional[Path]:
    path_str = row.get("frame_path") or row.get("image_path")
    if not path_str:
        return None
    p = Path(path_str)
    if image_root:
        p = (image_root / p).resolve() if not p.is_absolute() else p
    return p if p.exists() else None


def load_face_app(model: str, device: str, det_size: tuple[int, int]) -> FaceAnalysis:
    ctx_id = 0 if device == "cuda" else -1
    app = FaceAnalysis(name=model, providers=["CUDAExecutionProvider", "CPUExecutionProvider"])
    app.prepare(ctx_id=ctx_id, det_size=det_size)
    return app


def extract_face_embeddings(app: FaceAnalysis, image: np.ndarray) -> List[np.ndarray]:
    faces = app.get(image)
    embeddings: List[np.ndarray] = []
    for face in faces:
        if face.normed_embedding is None:
            continue
        embeddings.append(face.normed_embedding.astype(np.float32))
    return embeddings


def load_reference_embeddings(
    app: FaceAnalysis,
    ref_dir: Path,
    patterns: Iterable[str],
) -> np.ndarray:
    embeddings: List[np.ndarray] = []
    files: List[Path] = []
    for pattern in patterns:
        files.extend(ref_dir.glob(pattern))
    if not files:
        raise FileNotFoundError(f"å‚è€ƒç›®å½• {ref_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")

    for path in files:
        img = cv2.imread(str(path))
        if img is None:
            continue
        faces = app.get(img)
        if not faces:
            continue
        largest = max(faces, key=lambda f: (f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))
        if largest.normed_embedding is None:
            continue
        embeddings.append(largest.normed_embedding.astype(np.float32))

    if not embeddings:
        raise RuntimeError("å‚è€ƒç›®å½•å†…æœªæ£€æµ‹åˆ°æœ‰æ•ˆäººè„¸ï¼Œè¯·ç¡®è®¤ç´ ææ˜¯å¦æ¸…æ™°")

    return np.stack(embeddings, axis=0)


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom <= 1e-8:
        return -1.0
    return float(np.dot(a, b) / denom)


def main() -> None:
    args = parse_args()
    metadata_path = args.metadata
    output_path = args.output or metadata_path

    rows = load_metadata(metadata_path)
    if not rows:
        print("âš  metadata ä¸ºç©ºï¼Œæ— éœ€å¤„ç†")
        return

    image_root = args.image_root.resolve() if args.image_root else None

    app = load_face_app(args.model, args.device, tuple(args.det_size))
    ref_embeddings = load_reference_embeddings(app, args.reference_dir, args.globs)
    ref_center = ref_embeddings.mean(axis=0)
    ref_center /= np.linalg.norm(ref_center)

    valid_indices: List[int] = []
    image_paths: List[Path] = []
    for idx, row in enumerate(rows):
        path = resolve_image_path(row, image_root)
        if path:
            valid_indices.append(idx)
            image_paths.append(path)
        else:
            rows[idx][args.append_column] = "missing"
            rows[idx][args.score_column] = ""

    selected: List[int] = []
    for img_idx, row_idx in enumerate(tqdm(valid_indices, desc="äººè„¸ç­›é€‰", disable=args.quiet)):
        path = image_paths[img_idx]
        img = cv2.imread(str(path))
        if img is None:
            rows[row_idx][args.append_column] = "load_failed"
            rows[row_idx][args.score_column] = ""
            continue
        faces = app.get(img)
        if not faces:
            rows[row_idx][args.append_column] = "no_face"
            rows[row_idx][args.score_column] = ""
            continue
        sims = [
            cosine_similarity(face.normed_embedding.astype(np.float32), ref_center)
            for face in faces
            if face.normed_embedding is not None
        ]
        score = max(sims) if sims else -1.0
        rows[row_idx][args.score_column] = f"{score:.4f}" if score >= 0 else ""
        keep = score >= args.threshold
        rows[row_idx][args.append_column] = "keep" if keep else "reject"
        if keep:
            selected.append(row_idx)

    kept = len(selected)
    print(f"âœ“ äººè„¸ç­›é€‰å®Œæˆ: ä¿ç•™ {kept}/{len(valid_indices)} å¼  (é˜ˆå€¼ {args.threshold})")

    if args.dry_run:
        print("ðŸ€ dry-run æ¨¡å¼ï¼Œä¸å†™å…¥æ–‡ä»¶")
        return

    if args.selected_dir and kept:
        from shutil import copy2

        dst_root = args.selected_dir
        dst_root.mkdir(parents=True, exist_ok=True)
        for row_idx in selected:
            src = resolve_image_path(rows[row_idx], image_root)
            if not src:
                continue
            if image_root and src.is_relative_to(image_root):
                rel = src.relative_to(image_root)
            elif not src.is_absolute():
                rel = src
            else:
                rel = Path(src.name)
            dst = dst_root / rel
            dst.parent.mkdir(parents=True, exist_ok=True)
            try:
                copy2(src, dst)
            except Exception as exc:
                print(f"âš  å¤åˆ¶å¤±è´¥ {src} -> {dst}: {exc}")

    write_metadata(output_path, rows)
    print(f"âœ“ å·²å†™å…¥ç­›é€‰ç»“æžœ: {output_path}")


if __name__ == "__main__":
    main()


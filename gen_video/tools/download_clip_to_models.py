#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½ CLIP æ¨¡å‹åˆ° models ç›®å½•

å°† openai/clip-vit-large-patch14 ä¸‹è½½åˆ° models/clip/openai-clip-vit-large-patch14

æ”¯æŒé•œåƒç«™åŠ é€Ÿï¼š
export HF_ENDPOINT=https://hf-mirror.com
python3 tools/download_clip_to_models.py
"""

import os
import sys
from pathlib import Path
import shutil
import json

# é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
models_dir = project_root / "models" / "clip" / "openai-clip-vit-large-patch14"

# å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œè¯¢é—®æ˜¯å¦è¦†ç›–
if models_dir.exists():
    print(f"âš ï¸  ç›®æ ‡ç›®å½•å·²å­˜åœ¨: {models_dir}")
    print(f"   æ˜¯å¦è¦†ç›–ï¼Ÿ(y/n): ", end="")
    response = input().strip().lower()
    if response != 'y':
        print("   å–æ¶ˆä¸‹è½½")
        sys.exit(0)
    print(f"   ğŸ—‘ï¸  åˆ é™¤æ—§ç›®å½•...")
    shutil.rmtree(models_dir)

models_dir.mkdir(parents=True, exist_ok=True)

# æ£€æŸ¥æ˜¯å¦è®¾ç½®äº†é•œåƒç«™
hf_endpoint = os.environ.get("HF_ENDPOINT", "")
if hf_endpoint:
    print(f"ğŸŒ ä½¿ç”¨é•œåƒç«™: {hf_endpoint}")

print(f"ğŸ“¦ ç›®æ ‡ç›®å½•: {models_dir}")
print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ CLIP æ¨¡å‹: openai/clip-vit-large-patch14")
print(f"ğŸ’¡ æç¤ºï¼š")
print(f"   - å¦‚æœä¸‹è½½æ…¢ï¼Œå¯ä»¥è®¾ç½®é•œåƒç«™: export HF_ENDPOINT=https://hf-mirror.com")
print(f"   - æˆ–ä½¿ç”¨ proxychains4: proxychains4 python3 tools/download_clip_to_models.py")
print("")

model_id = "openai/clip-vit-large-patch14"

try:
    from transformers import CLIPTokenizer, CLIPTextModel
    
    print("1ï¸âƒ£ ä¸‹è½½ CLIP Tokenizer...")
    max_retries = 3
    tokenizer = None
    for attempt in range(max_retries):
        try:
            tokenizer = CLIPTokenizer.from_pretrained(model_id)
            print(f"   âœ“ Tokenizer ä¸‹è½½æˆåŠŸ")
            break
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"   âš ï¸  ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if not hf_endpoint and attempt == 0:
                    print(f"   ğŸ’¡ æç¤ºï¼šå¯ä»¥è®¾ç½®é•œåƒç«™åŠ é€Ÿ: export HF_ENDPOINT=https://hf-mirror.com")
                print(f"   ğŸ”„ é‡è¯•ä¸­...")
            else:
                raise
    
    print("2ï¸âƒ£ ä¸‹è½½ CLIP Text Model...")
    print("   ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼‰")
    if hf_endpoint:
        print(f"   ğŸŒ ä½¿ç”¨é•œåƒç«™: {hf_endpoint}ï¼ˆåº”è¯¥ä¼šæ›´å¿«ï¼‰")
        print(f"   âš ï¸  æ³¨æ„ï¼šå¦‚æœé•œåƒç«™æ–‡ä»¶æœ‰é—®é¢˜ï¼Œä¸‹è½½åå¯èƒ½æŸå")
    
    model = None
    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨ resume_download=True æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            model = CLIPTextModel.from_pretrained(
                model_id,
                resume_download=True  # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä½†æŸåï¼Œä¼šé‡æ–°ä¸‹è½½
            )
            print(f"   âœ“ Model ä¸‹è½½æˆåŠŸ")
            break
        except Exception as e:
            error_str = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯ safetensors æ–‡ä»¶æŸå
            is_corrupted = any(keyword in error_str for keyword in [
                "SafetensorError", "invalid JSON", "EOF", "deserializing header"
            ])
            
            if attempt < max_retries - 1:
                print(f"   âš ï¸  ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_str[:100]}")
                
                if is_corrupted:
                    print(f"   ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶æŸåï¼Œæ¸…ç†ç¼“å­˜åé‡è¯•...")
                    # æ¸…ç† HuggingFace ç¼“å­˜ä¸­çš„æŸåæ–‡ä»¶
                    hf_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
                    if not os.path.exists(hf_home):
                        hf_home = "/vepfs-dev/shawn/.cache/huggingface"
                    
                    # æ‰€æœ‰å¯èƒ½çš„ç¼“å­˜è·¯å¾„
                    cache_bases = [hf_home]
                    if os.path.exists(os.path.expanduser("~/.cache/huggingface")):
                        cache_bases.append(os.path.expanduser("~/.cache/huggingface"))
                    if os.path.exists("/root/.cache/huggingface"):
                        cache_bases.append("/root/.cache/huggingface")
                    
                    cache_name = f"models--{model_id.replace('/', '--')}"
                    
                    for cache_base in cache_bases:
                        if not os.path.exists(cache_base):
                            continue
                        
                        # æ£€æŸ¥ä¸¤ç§å¯èƒ½çš„è·¯å¾„ç»“æ„
                        for subpath in ["hub", ""]:
                            if subpath:
                                cache_path = os.path.join(cache_base, subpath, cache_name)
                            else:
                                cache_path = os.path.join(cache_base, cache_name)
                            
                            if os.path.exists(cache_path):
                                print(f"      ğŸ—‘ï¸  æ¸…ç†ç¼“å­˜ç›®å½•: {cache_path}")
                                try:
                                    # å…ˆå°è¯•åˆ é™¤æ•´ä¸ªç›®å½•
                                    shutil.rmtree(cache_path)
                                    print(f"      âœ… å·²åˆ é™¤: {cache_path}")
                                except Exception as cleanup_error:
                                    # å¦‚æœåˆ é™¤æ•´ä¸ªç›®å½•å¤±è´¥ï¼Œå°è¯•åªåˆ é™¤ snapshots å’Œ blobs
                                    print(f"      âš ï¸  åˆ é™¤æ•´ä¸ªç›®å½•å¤±è´¥ï¼Œå°è¯•åˆ é™¤å­ç›®å½•...")
                                    for subdir in ["snapshots", "blobs"]:
                                        subdir_path = os.path.join(cache_path, subdir)
                                        if os.path.exists(subdir_path):
                                            try:
                                                shutil.rmtree(subdir_path)
                                                print(f"      âœ… {subdir} ç›®å½•å·²æ¸…ç†")
                                            except Exception:
                                                pass
                                    
                                    # ä¹Ÿå°è¯•åˆ é™¤æŸåçš„ safetensors æ–‡ä»¶
                                    for root, dirs, files in os.walk(cache_path):
                                        for file in files:
                                            if file.endswith(".safetensors"):
                                                file_path = os.path.join(root, file)
                                                try:
                                                    # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                                                    with open(file_path, "rb") as f:
                                                        header_len_bytes = f.read(8)
                                                        if len(header_len_bytes) < 8:
                                                            raise ValueError("æ–‡ä»¶å¤´ä¸å®Œæ•´")
                                                        header_len = int.from_bytes(header_len_bytes, "little")
                                                        if header_len <= 0 or header_len > 1024 * 1024:  # é™åˆ¶æœ€å¤§ 1MB
                                                            raise ValueError("æ–‡ä»¶å¤´é•¿åº¦å¼‚å¸¸")
                                                        header_json = f.read(header_len).decode("utf-8")
                                                        json.loads(header_json)  # éªŒè¯ JSON
                                                except Exception:
                                                    # æ–‡ä»¶æŸåï¼Œåˆ é™¤å®ƒ
                                                    print(f"      ğŸ—‘ï¸  åˆ é™¤æŸåæ–‡ä»¶: {file_path}")
                                                    try:
                                                        os.remove(file_path)
                                                    except:
                                                        pass
                        
                        # ä¹Ÿæ¸…ç† locks
                        for lock_subpath in ["hub/.locks", ".locks"]:
                            lock_path = os.path.join(cache_base, lock_subpath, cache_name)
                            if os.path.exists(lock_path):
                                try:
                                    shutil.rmtree(lock_path)
                                    print(f"      âœ… é”æ–‡ä»¶å·²æ¸…ç†: {lock_path}")
                                except Exception:
                                    pass
                    
                    # æ¸…ç† transformers ç¼“å­˜
                    try:
                        import torch
                        if hasattr(torch, 'cuda'):
                            torch.cuda.empty_cache()
                    except:
                        pass
                    
                    print(f"   ğŸ”„ æ¸…ç†å®Œæˆï¼Œé‡æ–°ä¸‹è½½...")
                else:
                    print(f"   ğŸ”„ é‡è¯•ä¸­...")
            else:
                raise
    
    print("3ï¸âƒ£ éªŒè¯ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§...")
    # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
    try:
        # å°è¯•é‡æ–°åŠ è½½éªŒè¯
        from transformers import CLIPTextModel
        test_model = CLIPTextModel.from_pretrained(model_id, local_files_only=True)
        print(f"   âœ“ æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡")
        del test_model
    except Exception as verify_error:
        error_str = str(verify_error)
        if any(keyword in error_str for keyword in ["SafetensorError", "invalid JSON", "EOF", "deserializing header"]):
            print(f"   âš ï¸  æ¨¡å‹æ–‡ä»¶æŸåï¼é”™è¯¯: {verify_error}")
            print(f"   ğŸ’¡ å¯èƒ½åŸå› ï¼š")
            print(f"      1. é•œåƒç«™æ–‡ä»¶æœ¬èº«æœ‰é—®é¢˜")
            print(f"      2. ä¸‹è½½è¿‡ç¨‹ä¸­ç½‘ç»œä¸­æ–­")
            print(f"      3. ç£ç›˜ç©ºé—´ä¸è¶³å¯¼è‡´å†™å…¥ä¸å®Œæ•´")
            print(f"   ğŸ”„ æ¸…ç†ç¼“å­˜å¹¶é‡æ–°ä¸‹è½½ï¼ˆä¸ä½¿ç”¨é•œåƒç«™ï¼‰...")
            
            # æ¸…ç†ç¼“å­˜
            hf_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
            if not os.path.exists(hf_home):
                hf_home = "/vepfs-dev/shawn/.cache/huggingface"
            
            cache_name = f"models--{model_id.replace('/', '--')}"
            cache_path = os.path.join(hf_home, "hub", cache_name)
            if os.path.exists(cache_path):
                print(f"      ğŸ—‘ï¸  æ¸…ç†ç¼“å­˜: {cache_path}")
                try:
                    shutil.rmtree(cache_path)
                except:
                    pass
            
            # å°è¯•ä¸ä½¿ç”¨é•œåƒç«™é‡æ–°ä¸‹è½½ï¼ˆä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼‰
            if hf_endpoint:
                print(f"   ğŸ”„ ç¦ç”¨é•œåƒç«™ï¼Œä½¿ç”¨å®˜æ–¹æºé‡æ–°ä¸‹è½½ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰...")
                os.environ.pop("HF_ENDPOINT", None)
                model = CLIPTextModel.from_pretrained(
                    model_id,
                    resume_download=True  # æ–­ç‚¹ç»­ä¼ ï¼Œåªä¸‹è½½ç¼ºå¤±æˆ–æŸåçš„éƒ¨åˆ†
                )
                print(f"   âœ“ ä½¿ç”¨å®˜æ–¹æºä¸‹è½½æˆåŠŸ")
            else:
                raise
        else:
            raise
    
    print("4ï¸âƒ£ ä¿å­˜åˆ° models ç›®å½•...")
    # ä¿å­˜ tokenizer
    print("   ä¿å­˜ Tokenizer...")
    tokenizer.save_pretrained(str(models_dir))
    print(f"   âœ“ Tokenizer å·²ä¿å­˜")
    
    # ä¿å­˜ model
    print("   ä¿å­˜ Model...")
    model.save_pretrained(str(models_dir))
    print(f"   âœ“ Model å·²ä¿å­˜")
    
    # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
    print("5ï¸âƒ£ éªŒè¯ä¿å­˜çš„æ–‡ä»¶å®Œæ•´æ€§...")
    try:
        # éªŒè¯ safetensors æ–‡ä»¶
        safetensors_files = list(models_dir.glob("*.safetensors"))
        for safetensors_file in safetensors_files:
            print(f"   éªŒè¯: {safetensors_file.name}...")
            with open(safetensors_file, "rb") as f:
                # è¯»å–æ–‡ä»¶å¤´é•¿åº¦ï¼ˆ8å­—èŠ‚ï¼‰
                header_len_bytes = f.read(8)
                if len(header_len_bytes) < 8:
                    raise ValueError(f"æ–‡ä»¶å¤´ä¸å®Œæ•´: {safetensors_file}")
                
                header_len = int.from_bytes(header_len_bytes, "little")
                if header_len <= 0 or header_len > 10 * 1024 * 1024:  # é™åˆ¶æœ€å¤§ 10MB
                    raise ValueError(f"æ–‡ä»¶å¤´é•¿åº¦å¼‚å¸¸: {header_len} bytes")
                
                # è¯»å–å¹¶éªŒè¯ JSON
                header_json = f.read(header_len).decode("utf-8")
                header_data = json.loads(header_json)
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = safetensors_file.stat().st_size
                expected_size = header_len + 8  # è‡³å°‘æ˜¯å¤´éƒ¨å¤§å°
                for tensor_info in header_data.values():
                    if isinstance(tensor_info, dict) and "data_offsets" in tensor_info:
                        offsets = tensor_info["data_offsets"]
                        expected_size = max(expected_size, offsets[1] + 8)
                
                if file_size < expected_size:
                    raise ValueError(f"æ–‡ä»¶å¤§å°ä¸å®Œæ•´: {file_size} < {expected_size}")
                
                print(f"      âœ“ {safetensors_file.name} å®Œæ•´æ€§éªŒè¯é€šè¿‡ ({file_size / 1024 / 1024:.2f} MB)")
    except Exception as verify_error:
        print(f"   âš ï¸  æ–‡ä»¶éªŒè¯å¤±è´¥: {verify_error}")
        print(f"   ğŸ’¡ å»ºè®®é‡æ–°ä¸‹è½½")
        raise
    
    # éªŒè¯æ–‡ä»¶
    required_files = [
        "tokenizer_config.json",
        "vocab.json",
        "merges.txt",
        "special_tokens_map.json",
        "model.safetensors"  # æˆ– model.safetensors.index.json
    ]
    
    print("\n4ï¸âƒ£ éªŒè¯æ–‡ä»¶...")
    all_exist = True
    missing_files = []
    for file in required_files:
        file_path = models_dir / file
        if file_path.exists():
            file_size = file_path.stat().st_size / 1024 / 1024
            print(f"   âœ“ {file} ({file_size:.2f} MB)")
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰ .index.json æ–‡ä»¶
            if file == "model.safetensors":
                index_file = models_dir / "model.safetensors.index.json"
                if index_file.exists():
                    print(f"   âœ“ model.safetensors.index.json (åˆ†ç‰‡æ¨¡å‹)")
                    continue
            print(f"   âš ï¸  {file} ä¸å­˜åœ¨")
            missing_files.append(file)
            all_exist = False
    
    total_size = sum(
        os.path.getsize(os.path.join(dirpath, filename))
        for dirpath, dirnames, filenames in os.walk(models_dir)
        for filename in filenames
    )
    
    if all_exist:
        print(f"\nâœ… CLIP æ¨¡å‹å·²ä¸‹è½½åˆ°: {models_dir}")
        print(f"   æ€»æ–‡ä»¶å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        print("\nâœ… ä¸‹è½½å®Œæˆï¼ç°åœ¨å¯ä»¥åœ¨ç¦»çº¿ç¯å¢ƒä¸­ä½¿ç”¨æœ¬åœ° CLIP æ¨¡å‹äº†ã€‚")
        print(f"\nğŸ’¡ ä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œè·¯å¾„: {models_dir}")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±: {', '.join(missing_files)}")
        print(f"   ä½†ä¸»è¦æ–‡ä»¶å·²ä¸‹è½½ï¼Œæ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        print(f"   å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¼ºå¤±çš„æ–‡ä»¶")
    
except Exception as e:
    print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ CLIP è¯­ä¹‰ç›¸ä¼¼åº¦å¯¹ç´ æå¸§è¿›è¡Œåˆç­›ã€‚

- è¯»å–ç´ æå…ƒæ•°æ® CSVï¼ˆé»˜è®¤ `assets/library/metadata.csv`ï¼‰
- ä¸ºæ¯å¼ å›¾ç‰‡è®¡ç®—ä¸Žâ€œä¸»è§’æç¤ºè¯â€å’Œâ€œæŽ’é™¤æç¤ºè¯â€çš„ç›¸ä¼¼åº¦
- è¾“å‡ºç­›é€‰æ ‡è®°ï¼Œå¯é€‰æ‹©å¤åˆ¶é€šè¿‡çš„å›¾ç‰‡åˆ°æŒ‡å®šç›®å½•

ç¤ºä¾‹ï¼š
    python filter_stills_clip.py \
        --metadata ../assets/library/metadata.csv \
        --positive "close-up portrait of Han Li, male cultivator in green robe" \
        --positive-file hanli_positive.txt \
        --negative "crowd of people" \
        --positive-threshold 0.26 \
        --negative-threshold 0.22 \
        --append-column clip_flag \
        --selected-dir ../assets/library/clip_selected
"""

from __future__ import annotations

import argparse
import csv
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Sequence

import numpy as np
import open_clip
from PIL import Image
import torch
from tqdm import tqdm


REPO_ROOT = Path(__file__).resolve().parents[2]
DEFAULT_METADATA = REPO_ROOT / "gen_video" / "assets" / "library" / "metadata.csv"


@dataclass
class ClipConfig:
    checkpoint: str
    pretrained: str
    device: str
    batch_size: int
    quiet: bool


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="åˆ©ç”¨ CLIP å¯¹ç´ æå¸§è¿›è¡Œè¯­ä¹‰åˆç­›")
    parser.add_argument(
        "--metadata",
        type=Path,
        default=DEFAULT_METADATA,
        help="ç´ æå…ƒæ•°æ® CSVï¼ˆé»˜è®¤: assets/library/metadata.csvï¼‰",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="ç­›é€‰åŽ CSV è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤è¦†ç›– metadata æ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--image-root",
        type=Path,
        default=None,
        help="å¯é€‰ï¼šå›¾ç‰‡ç›¸å¯¹è·¯å¾„çš„æ ¹ç›®å½•ï¼Œé»˜è®¤å–å…ƒæ•°æ®é‡Œçš„è·¯å¾„",
    )
    parser.add_argument(
        "--positive",
        type=str,
        nargs="*",
        default=[],
        help="ä¸»è§’æ­£å‘æç¤ºè¯ï¼ˆå‘½ä»¤è¡ŒæŒ‡å®šï¼Œå¯å¤šæ¡ï¼‰",
    )
    parser.add_argument(
        "--positive-file",
        type=Path,
        help="åŒ…å«æ­£å‘æç¤ºè¯çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œæˆ– YAML çš„ prompts åˆ—ï¼‰",
    )
    parser.add_argument(
        "--negative",
        type=str,
        nargs="*",
        default=[],
        help="æŽ’é™¤æç¤ºè¯åˆ—è¡¨ï¼Œå¯å¤šæ¡",
    )
    parser.add_argument(
        "--negative-file",
        type=Path,
        help="åŒ…å«æŽ’é™¤æç¤ºè¯çš„æ–‡æœ¬æ–‡ä»¶",
    )
    parser.add_argument(
        "--positive-threshold",
        type=float,
        default=0.25,
        help="åˆ¤å®šä¸ºä¸»è§’ç”»é¢çš„æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.25ï¼‰",
    )
    parser.add_argument(
        "--negative-threshold",
        type=float,
        default=0.23,
        help="åˆ¤å®šä¸ºæŽ’é™¤ç”»é¢çš„æœ€å¤§ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.23ï¼‰",
    )
    parser.add_argument(
        "--append-column",
        type=str,
        default="clip_flag",
        help="åœ¨ CSV ä¸­å†™å…¥çš„æ ‡è®°åˆ—åï¼ˆé»˜è®¤ clip_flagï¼‰",
    )
    parser.add_argument(
        "--score-columns",
        nargs=2,
        metavar=("POS_COL", "NEG_COL"),
        default=("clip_pos_score", "clip_neg_score"),
        help="åœ¨ CSV ä¸­å†™å…¥æ­£å‘ / è´Ÿå‘ç›¸ä¼¼åº¦çš„åˆ—åï¼ˆé»˜è®¤ clip_pos_score clip_neg_scoreï¼‰",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="ViT-L-14",
        help="CLIP æ¨¡åž‹åç§°ï¼ˆé»˜è®¤ ViT-L-14ï¼‰",
    )
    parser.add_argument(
        "--pretrained",
        type=str,
        default="openai",
        help="é¢„è®­ç»ƒæƒé‡æ¥æºï¼ˆé»˜è®¤ openaiï¼‰",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="æŽ¨ç†è®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨é€‰æ‹© cudaï¼‰",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤ 32ï¼‰",
    )
    parser.add_argument(
        "--selected-dir",
        type=Path,
        help="å¯é€‰ï¼šå¤åˆ¶é€šè¿‡ç­›é€‰çš„å›¾ç‰‡åˆ°æ­¤ç›®å½•ï¼ˆä¼šæŒ‰åŽŸç›¸å¯¹è·¯å¾„å±‚çº§ä¿å­˜ï¼‰",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="å‡å°‘æ—¥å¿—è¾“å‡º",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ä»…è®¡ç®—åˆ†æ•°ï¼Œä¸å†™å…¥ CSV æˆ–å¤åˆ¶æ–‡ä»¶",
    )
    return parser.parse_args()


def load_prompts_from_file(path: Path) -> List[str]:
    if path.suffix.lower() in {".yaml", ".yml"}:
        import yaml  # type: ignore

        data = yaml.safe_load(path.read_text(encoding="utf-8"))
        if isinstance(data, dict) and "prompts" in data:
            prompts = data["prompts"]
        else:
            prompts = data
        if not isinstance(prompts, list):
            raise ValueError(f"{path} ä¸­ prompts ä¸æ˜¯åˆ—è¡¨")
        return [str(p).strip() for p in prompts if str(p).strip()]
    else:
        return [
            line.strip()
            for line in path.read_text(encoding="utf-8").splitlines()
            if line.strip()
        ]


def load_metadata(path: Path) -> List[dict]:
    if not path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° metadata æ–‡ä»¶: {path}")
    with path.open("r", newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return list(reader)


def write_metadata(path: Path, rows: Sequence[dict]) -> None:
    if not rows:
        return
    fieldnames = list(rows[0].keys())
    with path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


def batch_iter(seq: Sequence[Path], batch_size: int) -> Iterable[Sequence[Path]]:
    for i in range(0, len(seq), batch_size):
        yield seq[i : i + batch_size]


def encode_texts(
    prompts: Sequence[str],
    model,
    tokenizer,
    device: str,
) -> np.ndarray:
    if not prompts:
        return np.zeros((0, model.visual.output_dim), dtype=np.float32)
    with torch.no_grad():
        tokens = tokenizer(prompts).to(device)
        features = model.encode_text(tokens)
        features /= features.norm(dim=-1, keepdim=True)
        return features.cpu().numpy()


def encode_images(
    paths: Sequence[Path],
    preprocess,
    model,
    device: str,
    batch_size: int,
    quiet: bool,
) -> np.ndarray:
    features: List[np.ndarray] = []
    batches = list(batch_iter(paths, batch_size))
    for batch_paths in tqdm(batches, desc="ç¼–ç å›¾åƒ", disable=quiet):
        imgs = []
        for path in batch_paths:
            try:
                img = Image.open(path).convert("RGB")
                imgs.append(preprocess(img))
            except Exception:
                imgs.append(torch.zeros((3, 224, 224)))
        batch_tensor = torch.stack(imgs).to(device)
        with torch.no_grad():
            feats = model.encode_image(batch_tensor)
            feats /= feats.norm(dim=-1, keepdim=True)
        features.append(feats.cpu().numpy())
    if not features:
        return np.zeros((0, model.visual.output_dim), dtype=np.float32)
    return np.concatenate(features, axis=0)


def resolve_image_path(row: dict, image_root: Optional[Path]) -> Optional[Path]:
    path_str = row.get("frame_path") or row.get("image_path")
    if not path_str:
        return None
    p = Path(path_str)
    if image_root:
        p = (image_root / p).resolve() if not p.is_absolute() else p
    return p if p.exists() else None


def main() -> None:
    args = parse_args()
    metadata_path = args.metadata
    output_path = args.output or metadata_path

    rows = load_metadata(metadata_path)
    if not rows:
        print("âš  metadata ä¸ºç©ºï¼Œæ— éœ€å¤„ç†")
        return

    pos_prompts = list(args.positive)
    if args.positive_file:
        pos_prompts.extend(load_prompts_from_file(args.positive_file))
    neg_prompts = list(args.negative)
    if args.negative_file:
        neg_prompts.extend(load_prompts_from_file(args.negative_file))

    pos_prompts = [p.strip() for p in pos_prompts if p.strip()]
    neg_prompts = [p.strip() for p in neg_prompts if p.strip()]
    if not pos_prompts:
        print("âœ— æœªæä¾›æ­£å‘æç¤ºè¯ --positive/--positive-file", file=sys.stderr)
        sys.exit(1)

    image_root = args.image_root.resolve() if args.image_root else None
    image_paths: List[Path] = []
    valid_indices: List[int] = []
    for idx, row in enumerate(rows):
        path = resolve_image_path(row, image_root)
        if path:
            image_paths.append(path)
            valid_indices.append(idx)
        else:
            rows[idx][args.append_column] = "missing"

    if not image_paths:
        print("âœ— æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡è·¯å¾„ï¼Œè¯·æ£€æŸ¥ metadata çš„ frame_path/image_path å­—æ®µ", file=sys.stderr)
        sys.exit(1)

    model, _, preprocess = open_clip.create_model_and_transforms(
        args.checkpoint,
        pretrained=args.pretrained,
        device=args.device,
    )
    tokenizer = open_clip.get_tokenizer(args.checkpoint)
    pos_features = encode_texts(pos_prompts, model, tokenizer, args.device)
    neg_features = encode_texts(neg_prompts, model, tokenizer, args.device)

    img_features = encode_images(
        image_paths,
        preprocess,
        model,
        device=args.device,
        batch_size=args.batch_size,
        quiet=args.quiet,
    )

    pos_scores = img_features @ pos_features.T if pos_features.size else np.zeros((len(img_features), 0))
    neg_scores = img_features @ neg_features.T if neg_features.size else np.zeros((len(img_features), 0))

    pos_best = pos_scores.max(axis=1) if pos_scores.size else np.zeros(len(img_features))
    neg_best = neg_scores.max(axis=1) if neg_scores.size else np.zeros(len(img_features))

    pos_col, neg_col = args.score_columns
    selected_indices: List[int] = []
    for feat_idx, row_idx in enumerate(valid_indices):
        row = rows[row_idx]
        row[pos_col] = f"{pos_best[feat_idx]:.4f}"
        row[neg_col] = f"{neg_best[feat_idx]:.4f}" if neg_scores.size else ""
        keep = pos_best[feat_idx] >= args.positive_threshold
        if neg_scores.size:
            keep = keep and neg_best[feat_idx] < args.negative_threshold
        row[args.append_column] = "keep" if keep else "reject"
        if keep:
            selected_indices.append(row_idx)

    kept = len(selected_indices)
    print(
        f"âœ“ CLIP ç­›é€‰å®Œæˆ: ä¿ç•™ {kept}/{len(valid_indices)} å¼  "
        f"(pos>= {args.positive_threshold}, neg<{args.negative_threshold})"
    )

    if args.dry_run:
        print("ðŸ€ dry-run æ¨¡å¼ï¼Œä¸å†™å…¥æ–‡ä»¶")
        return

    if args.selected_dir and kept:
        dst_root = args.selected_dir
        dst_root.mkdir(parents=True, exist_ok=True)
        from shutil import copy2

        for row_idx in selected_indices:
            src = resolve_image_path(rows[row_idx], image_root)
            if not src:
                continue
            if image_root and src.is_relative_to(image_root):
                rel = src.relative_to(image_root)
            elif not src.is_absolute():
                rel = src
            else:
                rel = Path(src.name)
            dst = dst_root / rel
            dst.parent.mkdir(parents=True, exist_ok=True)
            try:
                copy2(src, dst)
            except Exception as exc:
                print(f"âš  å¤åˆ¶å¤±è´¥ {src} -> {dst}: {exc}")

    write_metadata(output_path, rows)
    print(f"âœ“ å·²å†™å…¥ç­›é€‰ç»“æžœ: {output_path}")


if __name__ == "__main__":
    main()


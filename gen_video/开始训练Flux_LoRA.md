# å¼€å§‹è®­ç»ƒ Flux LoRA

## âœ… å‡†å¤‡å®Œæˆ

- âœ… è™šæ‹Ÿç¯å¢ƒï¼š`/vepfs-dev/shawn/venv/py312/bin/activate`
- âœ… GPUï¼šH20 (97GB æ˜¾å­˜)
- âœ… è®­ç»ƒæ•°æ®ï¼š`train_data/host_person/`
- âœ… åŸºç¡€æ¨¡å‹ï¼š`models/flux1-dev`
- âœ… å®˜æ–¹è®­ç»ƒè„šæœ¬ï¼š`diffusers/examples/dreambooth/train_dreambooth_lora_flux.py`

---

## ğŸš€ æ–¹æ¡ˆ 1ï¼šä½¿ç”¨å®˜æ–¹è„šæœ¬ï¼ˆæ¨èï¼‰

### æ­¥éª¤ 1ï¼šå‡çº§ diffusers

```bash
source /vepfs-dev/shawn/venv/py312/bin/activate
cd /vepfs-dev/shawn/vid/fanren/gen_video

# ä»æºç å®‰è£…æœ€æ–°ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
proxychains4 git clone https://github.com/huggingface/diffusers.git --depth=1
cd diffusers
pip install -e .
cd examples/dreambooth
pip install -r requirements_flux.txt
```

### æ­¥éª¤ 2ï¼šé…ç½® accelerate

```bash
accelerate config default
```

### æ­¥éª¤ 3ï¼šå¼€å§‹è®­ç»ƒ

```bash
cd /vepfs-dev/shawn/vid/fanren/gen_video/diffusers/examples/dreambooth

accelerate launch train_dreambooth_lora_flux.py \
    --pretrained_model_name_or_path=/vepfs-dev/shawn/vid/fanren/gen_video/models/flux1-dev \
    --instance_data_dir=/vepfs-dev/shawn/vid/fanren/gen_video/train_data/host_person \
    --output_dir=/vepfs-dev/shawn/vid/fanren/gen_video/models/lora/host_person \
    --mixed_precision="bf16" \
    --instance_prompt="ç§‘æ™®ä¸»æŒäººï¼Œä¸“ä¸šå½¢è±¡" \
    --resolution=1024 \
    --train_batch_size=2 \
    --gradient_accumulation_steps=2 \
    --optimizer="prodigy" \
    --learning_rate=1.0 \
    --lr_scheduler="constant" \
    --lr_warmup_steps=0 \
    --max_train_steps=1000 \
    --validation_prompt="ç§‘æ™®ä¸»æŒäººï¼Œä¸“ä¸šå½¢è±¡ï¼Œå¾®ç¬‘ï¼Œæ­£å¼ç€è£…" \
    --validation_epochs=25 \
    --seed=0 \
    --rank=32 \
    --lora_alpha=16
```

**æ³¨æ„ï¼š** å®˜æ–¹è„šæœ¬ä½¿ç”¨ç»Ÿä¸€çš„ `instance_prompt`ï¼Œä¸æ”¯æŒä»æ–‡ä»¶åæå–æç¤ºè¯ã€‚å¦‚æœéœ€è¦ä½¿ç”¨æ–‡ä»¶åä¸­çš„æç¤ºè¯ï¼Œè¯·ä½¿ç”¨æ–¹æ¡ˆ 2ã€‚

---

## ğŸš€ æ–¹æ¡ˆ 2ï¼šä½¿ç”¨é€‚é…è„šæœ¬ï¼ˆå…¼å®¹å½“å‰ç‰ˆæœ¬ï¼‰

### æ­¥éª¤ 1ï¼šç›´æ¥å¼€å§‹è®­ç»ƒ

```bash
source /vepfs-dev/shawn/venv/py312/bin/activate
cd /vepfs-dev/shawn/vid/fanren/gen_video

python train_flux_lora_final.py \
    --data-dir train_data/host_person \
    --output-dir models/lora/host_person \
    --base-model models/flux1-dev \
    --epochs 10 \
    --batch-size 2 \
    --gradient-accumulation 2 \
    --learning-rate 1e-4 \
    --lora-rank 32 \
    --lora-alpha 16 \
    --use-bf16
```

**ä¼˜ç‚¹ï¼š**
- âœ… å…¼å®¹å½“å‰ diffusers ç‰ˆæœ¬ï¼ˆ0.35.2ï¼‰
- âœ… æ”¯æŒä»æ–‡ä»¶åæå–æç¤ºè¯ï¼ˆä½ çš„æ•°æ®æ ¼å¼ï¼‰
- âœ… å·²ä¼˜åŒ– H20 GPU é…ç½®

---

## âš™ï¸ H20 GPU æ¨èé…ç½®

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `train_batch_size` | 2 | H20 æ˜¾å­˜å……è¶³ï¼Œå¯ä»¥æ›´å¤§ |
| `gradient_accumulation_steps` | 2 | æ¢¯åº¦ç´¯ç§¯ |
| `learning_rate` | 1e-4 (AdamW) æˆ– 1.0 (Prodigy) | æ ¹æ®ä¼˜åŒ–å™¨é€‰æ‹© |
| `lora_rank` | 32 | LoRA ç»´åº¦ |
| `lora_alpha` | 16 | LoRA alpha |
| `resolution` | 1024 | è®­ç»ƒåˆ†è¾¨ç‡ |
| `use_bf16` | true | H20 æ”¯æŒ bf16 |
| `max_train_steps` | 1000 | è®­ç»ƒæ­¥æ•° |

---

## ğŸ“ è®­ç»ƒæ•°æ®æ ¼å¼

ä½ çš„æ•°æ®æ ¼å¼ï¼ˆå·²æ”¯æŒï¼‰ï¼š
```
train_data/host_person/
  _repeat_10_ç§‘æ™®ä¸»æŒäººï¼Œç”·æ€§ï¼Œä¸“ä¸šå½¢è±¡ï¼Œå¾®ç¬‘ï¼Œæ­£å¼ç€è£…ï¼Œæ­£é¢ï¼Œæ¼”æ’­å®¤èƒŒæ™¯.png
  _repeat_10_ç§‘æ™®ä¸»æŒäººï¼Œç”·æ€§ï¼Œä¸“ä¸šå½¢è±¡ï¼Œæ¸©å’Œï¼Œå•†åŠ¡æ­£è£…ï¼Œæ­£é¢ï¼Œçº¯è‰²èƒŒæ™¯.png
  ...
```

è„šæœ¬ä¼šè‡ªåŠ¨ä»æ–‡ä»¶åæå–æç¤ºè¯ã€‚

---

## âœ… è®­ç»ƒå®Œæˆå

LoRA æ¨¡å‹ä¿å­˜åœ¨ï¼š
```
models/lora/host_person/pytorch_lora_weights.safetensors
```

åœ¨ `model_manager.py` ä¸­é…ç½®ï¼š
```python
self.lora_configs = {
    "host_face": {
        "lora_path": "models/lora/host_person/pytorch_lora_weights.safetensors",
        "lora_alpha": 0.7
    }
}
```

---

## ğŸ”— å‚è€ƒæ–‡æ¡£

- `ä½¿ç”¨å®˜æ–¹Fluxè®­ç»ƒè„šæœ¬.md` - å®˜æ–¹è„šæœ¬è¯¦ç»†è¯´æ˜
- `ä½¿ç”¨diffuserså®˜æ–¹è®­ç»ƒFlux.md` - é€šç”¨è®­ç»ƒæŒ‡å—


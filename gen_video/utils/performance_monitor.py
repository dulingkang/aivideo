#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç›‘æ§å·¥å…·
ç”¨äºç›‘æ§è§†é¢‘ç”Ÿæˆçš„æ€§èƒ½æŒ‡æ ‡
"""

import time
import torch
from typing import Dict, Optional, Any
from dataclasses import dataclass, field
from pathlib import Path


@dataclass
class GenerationMetrics:
    """ç”ŸæˆæŒ‡æ ‡"""
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    memory_before: float = 0.0
    memory_after: float = 0.0
    memory_peak: float = 0.0
    num_frames: int = 0
    resolution: tuple = (0, 0)
    num_inference_steps: int = 0
    retry_count: int = 0
    success: bool = False
    error: Optional[str] = None
    
    @property
    def duration(self) -> float:
        """ç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰"""
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time
    
    @property
    def memory_used(self) -> float:
        """ä½¿ç”¨çš„æ˜¾å­˜ï¼ˆGBï¼‰"""
        return self.memory_after - self.memory_before
    
    @property
    def fps_generation(self) -> float:
        """ç”Ÿæˆé€Ÿåº¦ï¼ˆå¸§/ç§’ï¼‰"""
        if self.duration > 0 and self.num_frames > 0:
            return self.num_frames / self.duration
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "duration": self.duration,
            "memory_used_gb": self.memory_used,
            "memory_peak_gb": self.memory_peak,
            "num_frames": self.num_frames,
            "resolution": f"{self.resolution[0]}x{self.resolution[1]}",
            "num_inference_steps": self.num_inference_steps,
            "retry_count": self.retry_count,
            "fps_generation": self.fps_generation,
            "success": self.success,
            "error": self.error,
        }


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, log_file: Optional[str] = None):
        """åˆå§‹åŒ–ç›‘æ§å™¨"""
        self.log_file = log_file
        self.metrics_history: list[GenerationMetrics] = []
    
    def start_generation(self, num_frames: int, resolution: tuple, num_inference_steps: int) -> GenerationMetrics:
        """å¼€å§‹ç›‘æ§ä¸€æ¬¡ç”Ÿæˆ"""
        metrics = GenerationMetrics(
            num_frames=num_frames,
            resolution=resolution,
            num_inference_steps=num_inference_steps,
        )
        
        # è®°å½•ç”Ÿæˆå‰æ˜¾å­˜
        if torch.cuda.is_available():
            metrics.memory_before = torch.cuda.memory_allocated() / 1024**3
        
        return metrics
    
    def update_memory_peak(self, metrics: GenerationMetrics):
        """æ›´æ–°å³°å€¼æ˜¾å­˜"""
        if torch.cuda.is_available():
            current = torch.cuda.memory_allocated() / 1024**3
            if current > metrics.memory_peak:
                metrics.memory_peak = current
    
    def end_generation(self, metrics: GenerationMetrics, success: bool = True, error: Optional[str] = None):
        """ç»“æŸç›‘æ§ä¸€æ¬¡ç”Ÿæˆ"""
        metrics.end_time = time.time()
        metrics.success = success
        metrics.error = error
        
        # è®°å½•ç”Ÿæˆåæ˜¾å­˜
        if torch.cuda.is_available():
            metrics.memory_after = torch.cuda.memory_allocated() / 1024**3
        
        # ä¿å­˜åˆ°å†å²
        self.metrics_history.append(metrics)
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(metrics)
        
        # ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶
        if self.log_file:
            self._save_to_log(metrics)
    
    def _print_summary(self, metrics: GenerationMetrics):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        print(f"\n  ğŸ“Š æ€§èƒ½æ‘˜è¦:")
        print(f"     - è€—æ—¶: {metrics.duration:.1f}ç§’")
        print(f"     - ç”Ÿæˆé€Ÿåº¦: {metrics.fps_generation:.2f} å¸§/ç§’")
        print(f"     - æ˜¾å­˜ä½¿ç”¨: {metrics.memory_used:.2f}GB")
        if metrics.memory_peak > 0:
            print(f"     - å³°å€¼æ˜¾å­˜: {metrics.memory_peak:.2f}GB")
        if metrics.retry_count > 0:
            print(f"     - é‡è¯•æ¬¡æ•°: {metrics.retry_count}")
        if not metrics.success:
            print(f"     - çŠ¶æ€: å¤±è´¥ ({metrics.error})")
    
    def _save_to_log(self, metrics: GenerationMetrics):
        """ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶"""
        log_path = Path(self.log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        import json
        log_entry = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            **metrics.to_dict()
        }
        
        # è¿½åŠ åˆ°æ—¥å¿—æ–‡ä»¶
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.metrics_history:
            return {}
        
        successful = [m for m in self.metrics_history if m.success]
        if not successful:
            return {"error": "æ²¡æœ‰æˆåŠŸçš„ç”Ÿæˆè®°å½•"}
        
        durations = [m.duration for m in successful]
        memory_usages = [m.memory_used for m in successful]
        fps_rates = [m.fps_generation for m in successful if m.fps_generation > 0]
        
        return {
            "total_generations": len(self.metrics_history),
            "successful_generations": len(successful),
            "failed_generations": len(self.metrics_history) - len(successful),
            "avg_duration": sum(durations) / len(durations),
            "min_duration": min(durations),
            "max_duration": max(durations),
            "avg_memory_usage": sum(memory_usages) / len(memory_usages),
            "max_memory_usage": max(memory_usages),
            "avg_fps": sum(fps_rates) / len(fps_rates) if fps_rates else 0,
        }
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_statistics()
        if "error" in stats:
            print(f"  âš  {stats['error']}")
            return
        
        print(f"\n  ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"     - æ€»ç”Ÿæˆæ¬¡æ•°: {stats['total_generations']}")
        print(f"     - æˆåŠŸ: {stats['successful_generations']}, å¤±è´¥: {stats['failed_generations']}")
        print(f"     - å¹³å‡è€—æ—¶: {stats['avg_duration']:.1f}ç§’")
        print(f"     - è€—æ—¶èŒƒå›´: {stats['min_duration']:.1f}ç§’ - {stats['max_duration']:.1f}ç§’")
        print(f"     - å¹³å‡æ˜¾å­˜ä½¿ç”¨: {stats['avg_memory_usage']:.2f}GB")
        print(f"     - æœ€å¤§æ˜¾å­˜ä½¿ç”¨: {stats['max_memory_usage']:.2f}GB")
        if stats['avg_fps'] > 0:
            print(f"     - å¹³å‡ç”Ÿæˆé€Ÿåº¦: {stats['avg_fps']:.2f} å¸§/ç§’")


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_global_monitor: Optional[PerformanceMonitor] = None


def get_monitor(log_file: Optional[str] = None) -> PerformanceMonitor:
    """è·å–å…¨å±€ç›‘æ§å™¨"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = PerformanceMonitor(log_file)
    return _global_monitor


if __name__ == "__main__":
    """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""
    monitor = PerformanceMonitor("test_performance.log")
    
    # æ¨¡æ‹Ÿä¸€æ¬¡ç”Ÿæˆ
    metrics = monitor.start_generation(
        num_frames=24,
        resolution=(640, 480),
        num_inference_steps=30
    )
    
    # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
    import time
    time.sleep(1)
    
    # æ›´æ–°å³°å€¼æ˜¾å­˜
    if torch.cuda.is_available():
        monitor.update_memory_peak(metrics)
    
    # ç»“æŸç”Ÿæˆ
    monitor.end_generation(metrics, success=True)
    
    # æ‰“å°ç»Ÿè®¡
    monitor.print_statistics()


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ˜¾å­˜ç®¡ç†å™¨
ç”¨äºä¼˜åŒ–æ‰¹é‡ç”Ÿæˆæ—¶çš„æ˜¾å­˜ç®¡ç†å’Œæ¨¡å‹åŠ è½½ç­–ç•¥

åŠŸèƒ½ç‰¹æ€§:
1. æ˜¾å­˜ç›‘æ§ - å®æ—¶ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
2. æ™ºèƒ½å¸è½½ - æ ¹æ®æ˜¾å­˜å‹åŠ›è‡ªåŠ¨å¸è½½ä¸æ´»è·ƒçš„æ¨¡å‹
3. æ¨¡å‹ç¼“å­˜ - ç¼“å­˜å¸¸ç”¨æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½
4. å»¶è¿ŸåŠ è½½ - æŒ‰éœ€åŠ è½½æ¨¡å‹
5. æ‰¹é‡ä¼˜åŒ– - ä¼˜åŒ–æ‰¹é‡ç”Ÿæˆæ—¶çš„æ˜¾å­˜åˆ†é…

Author: AI Video Team
Date: 2025-12-17
"""

import gc
import time
import torch
import weakref
import threading
from typing import Dict, Any, Optional, List, Callable, Set
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import logging
from contextlib import contextmanager

logger = logging.getLogger(__name__)


class MemoryPriority(Enum):
    """æ¨¡å‹æ˜¾å­˜ä¼˜å…ˆçº§"""
    CRITICAL = 1    # æ ¸å¿ƒæ¨¡å‹ï¼Œå°½é‡ä¸å¸è½½
    HIGH = 2        # é«˜ä¼˜å…ˆçº§ï¼Œè¾ƒå°‘ä½¿ç”¨æ—¶å¯å¸è½½
    MEDIUM = 3      # ä¸­ç­‰ä¼˜å…ˆçº§
    LOW = 4         # ä½ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆå¸è½½


@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    loader: Callable[[], Any]  # æ¨¡å‹åŠ è½½å‡½æ•°
    unloader: Optional[Callable[[Any], None]] = None  # æ¨¡å‹å¸è½½å‡½æ•°
    priority: MemoryPriority = MemoryPriority.MEDIUM
    estimated_size_gb: float = 0.0  # ä¼°è®¡çš„æ˜¾å­˜å ç”¨
    last_used: float = field(default_factory=time.time)
    use_count: int = 0
    loaded: bool = False
    instance: Any = None


@dataclass
class MemoryStats:
    """æ˜¾å­˜ç»Ÿè®¡"""
    total_gb: float = 0.0
    allocated_gb: float = 0.0
    reserved_gb: float = 0.0
    free_gb: float = 0.0
    cached_gb: float = 0.0
    
    @classmethod
    def current(cls) -> 'MemoryStats':
        """è·å–å½“å‰æ˜¾å­˜çŠ¶æ€"""
        if not torch.cuda.is_available():
            return cls()
        
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        cached = reserved - allocated
        free = total - reserved
        
        return cls(
            total_gb=total,
            allocated_gb=allocated,
            reserved_gb=reserved,
            free_gb=free,
            cached_gb=cached
        )
    
    def to_dict(self) -> Dict[str, float]:
        return {
            "total_gb": round(self.total_gb, 2),
            "allocated_gb": round(self.allocated_gb, 2),
            "reserved_gb": round(self.reserved_gb, 2),
            "free_gb": round(self.free_gb, 2),
            "cached_gb": round(self.cached_gb, 2),
            "usage_percent": round((self.reserved_gb / self.total_gb) * 100, 1) if self.total_gb > 0 else 0
        }


class MemoryManager:
    """
    æ˜¾å­˜ç®¡ç†å™¨
    
    æä¾›ç»Ÿä¸€çš„æ˜¾å­˜ç®¡ç†æ¥å£ï¼Œæ”¯æŒï¼š
    - æ¨¡å‹æ³¨å†Œå’Œå»¶è¿ŸåŠ è½½
    - æ˜¾å­˜ç›‘æ§å’Œé¢„è­¦
    - æ™ºèƒ½å¸è½½ç­–ç•¥
    - æ‰¹é‡ç”Ÿæˆä¼˜åŒ–
    """
    
    def __init__(
        self,
        max_memory_gb: Optional[float] = None,
        warning_threshold: float = 0.85,
        critical_threshold: float = 0.95,
        auto_cleanup: bool = True
    ):
        """
        åˆå§‹åŒ–æ˜¾å­˜ç®¡ç†å™¨
        
        Args:
            max_memory_gb: æœ€å¤§å¯ç”¨æ˜¾å­˜ï¼ˆGBï¼‰ï¼ŒNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
            warning_threshold: æ˜¾å­˜è­¦å‘Šé˜ˆå€¼ï¼ˆ0-1ï¼‰
            critical_threshold: æ˜¾å­˜å±é™©é˜ˆå€¼ï¼ˆ0-1ï¼‰
            auto_cleanup: æ˜¯å¦è‡ªåŠ¨æ¸…ç†ç¼“å­˜
        """
        self.models: Dict[str, ModelInfo] = {}
        self.max_memory_gb = max_memory_gb
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.auto_cleanup = auto_cleanup
        
        # æ£€æµ‹å¯ç”¨æ˜¾å­˜
        if torch.cuda.is_available():
            if self.max_memory_gb is None:
                self.max_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"æ˜¾å­˜ç®¡ç†å™¨åˆå§‹åŒ–: æœ€å¤§æ˜¾å­˜ {self.max_memory_gb:.1f}GB")
        else:
            self.max_memory_gb = 0
            logger.warning("CUDA ä¸å¯ç”¨ï¼Œæ˜¾å­˜ç®¡ç†å™¨å°†ä»¥ CPU æ¨¡å¼è¿è¡Œ")
        
        # çº¿ç¨‹é”
        self._lock = threading.RLock()
        
        # æ˜¾å­˜å†å²è®°å½•
        self._memory_history: List[Dict[str, Any]] = []
    
    def register_model(
        self,
        name: str,
        loader: Callable[[], Any],
        unloader: Optional[Callable[[Any], None]] = None,
        priority: MemoryPriority = MemoryPriority.MEDIUM,
        estimated_size_gb: float = 0.0
    ):
        """
        æ³¨å†Œæ¨¡å‹
        
        Args:
            name: æ¨¡å‹åç§°
            loader: æ¨¡å‹åŠ è½½å‡½æ•°
            unloader: æ¨¡å‹å¸è½½å‡½æ•°
            priority: ä¼˜å…ˆçº§
            estimated_size_gb: ä¼°è®¡çš„æ˜¾å­˜å ç”¨
        """
        with self._lock:
            self.models[name] = ModelInfo(
                name=name,
                loader=loader,
                unloader=unloader,
                priority=priority,
                estimated_size_gb=estimated_size_gb
            )
            logger.debug(f"æ³¨å†Œæ¨¡å‹: {name} (ä¼˜å…ˆçº§: {priority.name}, é¢„ä¼°: {estimated_size_gb:.1f}GB)")
    
    def get_model(self, name: str, ensure_memory: bool = True) -> Any:
        """
        è·å–æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        
        Args:
            name: æ¨¡å‹åç§°
            ensure_memory: æ˜¯å¦ç¡®ä¿æœ‰è¶³å¤Ÿæ˜¾å­˜
            
        Returns:
            æ¨¡å‹å®ä¾‹
        """
        with self._lock:
            if name not in self.models:
                raise KeyError(f"æ¨¡å‹æœªæ³¨å†Œ: {name}")
            
            info = self.models[name]
            
            # å¦‚æœæ¨¡å‹æœªåŠ è½½
            if not info.loaded or info.instance is None:
                # æ£€æŸ¥æ˜¾å­˜
                if ensure_memory:
                    self._ensure_memory(info.estimated_size_gb)
                
                # åŠ è½½æ¨¡å‹
                logger.info(f"åŠ è½½æ¨¡å‹: {name}")
                start_time = time.time()
                
                try:
                    info.instance = info.loader()
                    info.loaded = True
                    
                    load_time = time.time() - start_time
                    logger.info(f"æ¨¡å‹ {name} åŠ è½½å®Œæˆ ({load_time:.1f}ç§’)")
                    
                except Exception as e:
                    logger.error(f"æ¨¡å‹ {name} åŠ è½½å¤±è´¥: {e}")
                    raise
            
            # æ›´æ–°ä½¿ç”¨ä¿¡æ¯
            info.last_used = time.time()
            info.use_count += 1
            
            return info.instance
    
    def unload_model(self, name: str, force: bool = False):
        """
        å¸è½½æ¨¡å‹
        
        Args:
            name: æ¨¡å‹åç§°
            force: æ˜¯å¦å¼ºåˆ¶å¸è½½
        """
        with self._lock:
            if name not in self.models:
                return
            
            info = self.models[name]
            
            if not info.loaded:
                return
            
            # æ£€æŸ¥ä¼˜å…ˆçº§
            if not force and info.priority == MemoryPriority.CRITICAL:
                logger.warning(f"æ¨¡å‹ {name} æ˜¯å…³é”®æ¨¡å‹ï¼Œè·³è¿‡å¸è½½")
                return
            
            logger.info(f"å¸è½½æ¨¡å‹: {name}")
            
            # è°ƒç”¨è‡ªå®šä¹‰å¸è½½å‡½æ•°
            if info.unloader and info.instance:
                try:
                    info.unloader(info.instance)
                except Exception as e:
                    logger.warning(f"æ¨¡å‹ {name} è‡ªå®šä¹‰å¸è½½å¤±è´¥: {e}")
            
            # åˆ é™¤å®ä¾‹å¼•ç”¨
            info.instance = None
            info.loaded = False
            
            # æ¸…ç†æ˜¾å­˜
            self._cleanup_memory()
    
    def unload_all(self, include_critical: bool = False):
        """
        å¸è½½æ‰€æœ‰æ¨¡å‹
        
        Args:
            include_critical: æ˜¯å¦åŒ…æ‹¬å…³é”®æ¨¡å‹
        """
        with self._lock:
            for name, info in list(self.models.items()):
                if info.loaded:
                    if include_critical or info.priority != MemoryPriority.CRITICAL:
                        self.unload_model(name, force=include_critical)
    
    def _ensure_memory(self, required_gb: float):
        """
        ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ˜¾å­˜
        
        Args:
            required_gb: éœ€è¦çš„æ˜¾å­˜ï¼ˆGBï¼‰
        """
        if not torch.cuda.is_available():
            return
        
        stats = MemoryStats.current()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡Šæ”¾æ˜¾å­˜
        if stats.free_gb < required_gb:
            logger.warning(f"æ˜¾å­˜ä¸è¶³: éœ€è¦ {required_gb:.1f}GB, å¯ç”¨ {stats.free_gb:.1f}GB")
            
            # å°è¯•æ™ºèƒ½å¸è½½
            self._smart_unload(required_gb - stats.free_gb)
            
            # å†æ¬¡æ£€æŸ¥
            stats = MemoryStats.current()
            if stats.free_gb < required_gb:
                logger.error(f"æ— æ³•é‡Šæ”¾è¶³å¤Ÿæ˜¾å­˜: éœ€è¦ {required_gb:.1f}GB, å¯ç”¨ {stats.free_gb:.1f}GB")
    
    def _smart_unload(self, required_gb: float):
        """
        æ™ºèƒ½å¸è½½æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜
        
        ä¼˜å…ˆå¸è½½ï¼š
        1. ä¼˜å…ˆçº§ä½çš„æ¨¡å‹
        2. æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
        3. ä½¿ç”¨æ¬¡æ•°å°‘çš„æ¨¡å‹
        
        Args:
            required_gb: éœ€è¦é‡Šæ”¾çš„æ˜¾å­˜ï¼ˆGBï¼‰
        """
        with self._lock:
            # è·å–å·²åŠ è½½çš„æ¨¡å‹ï¼ŒæŒ‰ä¼˜å…ˆçº§å’Œä½¿ç”¨æ—¶é—´æ’åº
            loaded_models = [
                (name, info) for name, info in self.models.items()
                if info.loaded and info.priority != MemoryPriority.CRITICAL
            ]
            
            # æ’åºï¼šä¼˜å…ˆçº§ä½çš„åœ¨å‰ï¼ŒåŒä¼˜å…ˆçº§æŒ‰æœ€åä½¿ç”¨æ—¶é—´æ’åº
            loaded_models.sort(key=lambda x: (
                -x[1].priority.value,  # ä¼˜å…ˆçº§å€¼è¶Šå¤§è¶Šé å‰ï¼ˆè¶Šå…ˆå¸è½½ï¼‰
                x[1].last_used         # æœ€åä½¿ç”¨æ—¶é—´è¶Šæ—©è¶Šé å‰
            ))
            
            freed_gb = 0.0
            for name, info in loaded_models:
                if freed_gb >= required_gb:
                    break
                
                logger.info(f"æ™ºèƒ½å¸è½½æ¨¡å‹: {name} (é¢„è®¡é‡Šæ”¾ {info.estimated_size_gb:.1f}GB)")
                self.unload_model(name)
                freed_gb += info.estimated_size_gb
    
    def _cleanup_memory(self):
        """æ¸…ç†æ˜¾å­˜ç¼“å­˜"""
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            gc.collect()
    
    def get_stats(self) -> MemoryStats:
        """è·å–å½“å‰æ˜¾å­˜çŠ¶æ€"""
        return MemoryStats.current()
    
    def log_stats(self, prefix: str = ""):
        """è®°å½•æ˜¾å­˜çŠ¶æ€åˆ°æ—¥å¿—"""
        stats = self.get_stats()
        usage_pct = (stats.reserved_gb / stats.total_gb) * 100 if stats.total_gb > 0 else 0
        
        status_emoji = "ğŸŸ¢" if usage_pct < self.warning_threshold * 100 else (
            "ğŸŸ¡" if usage_pct < self.critical_threshold * 100 else "ğŸ”´"
        )
        
        logger.info(
            f"{prefix}æ˜¾å­˜ {status_emoji}: "
            f"å·²åˆ†é…={stats.allocated_gb:.2f}GB, "
            f"å·²ä¿ç•™={stats.reserved_gb:.2f}GB, "
            f"å¯ç”¨={stats.free_gb:.2f}GB, "
            f"ä½¿ç”¨ç‡={usage_pct:.1f}%"
        )
    
    def get_loaded_models(self) -> List[str]:
        """è·å–å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨"""
        with self._lock:
            return [name for name, info in self.models.items() if info.loaded]
    
    def get_model_info(self, name: str) -> Optional[Dict[str, Any]]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        with self._lock:
            if name not in self.models:
                return None
            
            info = self.models[name]
            return {
                "name": info.name,
                "priority": info.priority.name,
                "estimated_size_gb": info.estimated_size_gb,
                "loaded": info.loaded,
                "last_used": info.last_used,
                "use_count": info.use_count
            }
    
    @contextmanager
    def memory_context(self, operation_name: str = "operation"):
        """
        æ˜¾å­˜ç®¡ç†ä¸Šä¸‹æ–‡
        
        è®°å½•æ“ä½œå‰åçš„æ˜¾å­˜å˜åŒ–
        
        Args:
            operation_name: æ“ä½œåç§°
            
        Example:
            with memory_manager.memory_context("generate_image"):
                image = generator.generate(...)
        """
        before = MemoryStats.current()
        start_time = time.time()
        
        try:
            yield
        finally:
            after = MemoryStats.current()
            duration = time.time() - start_time
            
            delta = after.allocated_gb - before.allocated_gb
            delta_sign = "+" if delta >= 0 else ""
            
            logger.debug(
                f"{operation_name} å®Œæˆ: "
                f"è€—æ—¶={duration:.1f}s, "
                f"æ˜¾å­˜å˜åŒ–={delta_sign}{delta:.2f}GB "
                f"({before.allocated_gb:.2f}GB â†’ {after.allocated_gb:.2f}GB)"
            )
            
            # è®°å½•å†å²
            self._memory_history.append({
                "operation": operation_name,
                "timestamp": time.time(),
                "duration": duration,
                "before": before.to_dict(),
                "after": after.to_dict(),
                "delta_gb": delta
            })
            
            # è‡ªåŠ¨æ¸…ç†
            if self.auto_cleanup:
                usage_pct = after.reserved_gb / after.total_gb if after.total_gb > 0 else 0
                if usage_pct > self.critical_threshold:
                    logger.warning(f"æ˜¾å­˜ä½¿ç”¨ç‡ {usage_pct*100:.1f}% è¶…è¿‡å±é™©é˜ˆå€¼ï¼Œæ‰§è¡Œæ¸…ç†")
                    self._cleanup_memory()
    
    @contextmanager
    def batch_context(self, batch_name: str = "batch"):
        """
        æ‰¹é‡ç”Ÿæˆä¸Šä¸‹æ–‡
        
        åœ¨æ‰¹é‡å¼€å§‹æ—¶è®°å½•çŠ¶æ€ï¼Œç»“æŸæ—¶æ¸…ç†
        
        Args:
            batch_name: æ‰¹é‡åç§°
        """
        logger.info(f"å¼€å§‹æ‰¹é‡ '{batch_name}'")
        self.log_stats(f"[{batch_name}] å¼€å§‹å‰ - ")
        start_time = time.time()
        
        try:
            yield
        finally:
            duration = time.time() - start_time
            logger.info(f"å®Œæˆæ‰¹é‡ '{batch_name}' (è€—æ—¶: {duration:.1f}s)")
            self.log_stats(f"[{batch_name}] å®Œæˆå - ")
            
            # æ‰¹é‡ç»“æŸåæ¸…ç†ç¼“å­˜
            if self.auto_cleanup:
                self._cleanup_memory()


# å…¨å±€å®ä¾‹
_global_memory_manager: Optional[MemoryManager] = None


def get_memory_manager(**kwargs) -> MemoryManager:
    """è·å–å…¨å±€æ˜¾å­˜ç®¡ç†å™¨"""
    global _global_memory_manager
    if _global_memory_manager is None:
        _global_memory_manager = MemoryManager(**kwargs)
    return _global_memory_manager


def log_memory_status(prefix: str = ""):
    """å¿«æ·å‡½æ•°ï¼šè®°å½•å½“å‰æ˜¾å­˜çŠ¶æ€"""
    stats = MemoryStats.current()
    if stats.total_gb > 0:
        usage_pct = (stats.reserved_gb / stats.total_gb) * 100
        logger.info(
            f"{prefix}æ˜¾å­˜: å·²åˆ†é…={stats.allocated_gb:.2f}GB, "
            f"å·²ä¿ç•™={stats.reserved_gb:.2f}GB, "
            f"å¯ç”¨={stats.free_gb:.2f}GB ({usage_pct:.1f}%)"
        )


def cleanup_memory():
    """å¿«æ·å‡½æ•°ï¼šæ¸…ç†æ˜¾å­˜ç¼“å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        torch.cuda.empty_cache()
        gc.collect()


if __name__ == "__main__":
    """æµ‹è¯•æ˜¾å­˜ç®¡ç†å™¨"""
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("æ˜¾å­˜ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = MemoryManager()
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    stats = manager.get_stats()
    print(f"\nå½“å‰æ˜¾å­˜çŠ¶æ€:")
    for key, value in stats.to_dict().items():
        print(f"  {key}: {value}")
    
    # æ³¨å†Œæ¨¡å‹ç¤ºä¾‹
    def dummy_loader():
        return "dummy_model"
    
    def dummy_unloader(model):
        pass
    
    manager.register_model(
        name="test_model",
        loader=dummy_loader,
        unloader=dummy_unloader,
        priority=MemoryPriority.MEDIUM,
        estimated_size_gb=2.0
    )
    
    # è·å–æ¨¡å‹
    with manager.memory_context("load_test_model"):
        model = manager.get_model("test_model")
        print(f"\nåŠ è½½çš„æ¨¡å‹: {model}")
    
    # æ˜¾ç¤ºåŠ è½½çš„æ¨¡å‹
    print(f"\nå·²åŠ è½½æ¨¡å‹: {manager.get_loaded_models()}")
    
    # å¸è½½æ¨¡å‹
    manager.unload_all()
    print(f"\nå¸è½½åå·²åŠ è½½æ¨¡å‹: {manager.get_loaded_models()}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘èº«ä»½åˆ†æå™¨
ç”¨äºåˆ†æè§†é¢‘ç”Ÿæˆæ—¶çš„è§’è‰²èº«ä»½ä¸€è‡´æ€§

åŠŸèƒ½ç‰¹æ€§:
1. è§†é¢‘å¸§èº«ä»½æ£€æµ‹ - æ£€æµ‹æ¯å¸§ä¸­çš„äººè„¸èº«ä»½
2. ä¸å‚è€ƒå›¾å¯¹æ¯” - è®¡ç®—ä¸åŸå§‹å‚è€ƒå›¾çš„ç›¸ä¼¼åº¦
3. èº«ä»½æ¼‚ç§»åˆ†æ - æ£€æµ‹èº«ä»½æ¼‚ç§»ä¸¥é‡çš„å¸§
4. å¸§é—´ä¸€è‡´æ€§ - åˆ†æç›¸é‚»å¸§ä¹‹é—´çš„èº«ä»½ä¸€è‡´æ€§
5. æŠ¥å‘Šç”Ÿæˆ - ç”Ÿæˆè¯¦ç»†çš„èº«ä»½åˆ†ææŠ¥å‘Š

Author: AI Video Team
Date: 2025-12-17
Project: M6 - è§†é¢‘èº«ä»½ä¿æŒç ”ç©¶
"""

import numpy as np
import cv2
from typing import Dict, Any, List, Tuple, Optional, Union
from dataclasses import dataclass, field
from pathlib import Path
from PIL import Image
import logging
import json
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class FrameIdentityResult:
    """å•å¸§èº«ä»½ç»“æœ"""
    frame_idx: int
    timestamp_sec: float
    similarity: float  # ä¸å‚è€ƒå›¾çš„ç›¸ä¼¼åº¦
    face_detected: bool
    face_bbox: Optional[Tuple[int, int, int, int]] = None  # x1, y1, x2, y2
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "frame_idx": self.frame_idx,
            "timestamp_sec": round(self.timestamp_sec, 2),
            "similarity": round(float(self.similarity), 4),
            "face_detected": self.face_detected,
            "face_bbox": self.face_bbox
        }


@dataclass
class VideoIdentityReport:
    """è§†é¢‘èº«ä»½åˆ†ææŠ¥å‘Š"""
    video_path: str = ""
    reference_path: str = ""
    timestamp: str = ""
    
    # å¸§åˆ†æç»“æœ
    total_frames: int = 0
    analyzed_frames: int = 0
    sample_interval: int = 1
    fps: float = 0.0
    duration_sec: float = 0.0
    
    # èº«ä»½ç›¸ä¼¼åº¦æŒ‡æ ‡
    frame_similarities: List[float] = field(default_factory=list)
    avg_similarity: float = 0.0
    min_similarity: float = 0.0
    max_similarity: float = 0.0
    std_similarity: float = 0.0
    
    # ç›¸é‚»å¸§ä¸€è‡´æ€§
    adjacent_similarities: List[float] = field(default_factory=list)
    avg_adjacent_similarity: float = 0.0
    
    # èº«ä»½æ¼‚ç§»
    drift_threshold: float = 0.5
    drift_frames: List[int] = field(default_factory=list)
    drift_ratio: float = 0.0
    
    # äººè„¸æ£€æµ‹
    face_detected_ratio: float = 0.0
    
    # æ€»ä½“ç»“è®º
    overall_passed: bool = False
    issues: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "video_path": self.video_path,
            "reference_path": self.reference_path,
            "timestamp": self.timestamp,
            "total_frames": self.total_frames,
            "analyzed_frames": self.analyzed_frames,
            "sample_interval": self.sample_interval,
            "fps": self.fps,
            "duration_sec": round(self.duration_sec, 2),
            "avg_similarity": round(self.avg_similarity, 4),
            "min_similarity": round(self.min_similarity, 4),
            "max_similarity": round(self.max_similarity, 4),
            "std_similarity": round(self.std_similarity, 4),
            "avg_adjacent_similarity": round(self.avg_adjacent_similarity, 4),
            "drift_threshold": self.drift_threshold,
            "drift_frame_count": len(self.drift_frames),
            "drift_ratio": round(self.drift_ratio, 4),
            "face_detected_ratio": round(self.face_detected_ratio, 4),
            "overall_passed": self.overall_passed,
            "issues": self.issues
        }
    
    def to_json(self, indent: int = 2) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=indent)


class VideoIdentityAnalyzer:
    """
    è§†é¢‘èº«ä»½åˆ†æå™¨
    
    åˆ†æè§†é¢‘ç”Ÿæˆæ—¶çš„è§’è‰²èº«ä»½ä¸€è‡´æ€§ï¼Œæ£€æµ‹èº«ä»½æ¼‚ç§»é—®é¢˜
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.face_analyzer = None
        self.device = self.config.get("device", "cuda")
        
        # é˜ˆå€¼é…ç½®
        self.similarity_threshold = self.config.get("similarity_threshold", 0.65)
        self.drift_threshold = self.config.get("drift_threshold", 0.50)
        self.adjacent_threshold = self.config.get("adjacent_threshold", 0.85)
    
    def _load_face_analyzer(self):
        """å»¶è¿ŸåŠ è½½äººè„¸åˆ†æå™¨"""
        if self.face_analyzer is not None:
            return
        
        try:
            from insightface.app import FaceAnalysis
            import os
            
            # è·å–æ¨¡å‹è·¯å¾„
            model_root = self.config.get("insightface_root", None)
            
            if model_root is None:
                # é»˜è®¤è·¯å¾„ï¼šgen_video ç›®å½•
                gen_video_dir = Path(__file__).parent.parent
                model_root = str(gen_video_dir)
            elif not os.path.isabs(model_root):
                gen_video_dir = Path(__file__).parent.parent
                model_root = str(gen_video_dir / model_root)
            
            logger.debug(f"InsightFace æ¨¡å‹æ ¹ç›®å½•: {model_root}")
            
            self.face_analyzer = FaceAnalysis(
                name='antelopev2',
                root=model_root,
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
            )
            self.face_analyzer.prepare(ctx_id=0)
            logger.info("âœ… è§†é¢‘èº«ä»½åˆ†æå™¨: äººè„¸åˆ†æå™¨åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ äººè„¸åˆ†æå™¨åŠ è½½å¤±è´¥: {e}")
            self.face_analyzer = None
    
    def _extract_reference_embedding(
        self,
        reference_image: Union[str, Path, Image.Image]
    ) -> Optional[np.ndarray]:
        """æå–å‚è€ƒå›¾çš„äººè„¸åµŒå…¥"""
        self._load_face_analyzer()
        
        if self.face_analyzer is None:
            return None
        
        try:
            # åŠ è½½å›¾åƒ
            if isinstance(reference_image, (str, Path)):
                img = Image.open(reference_image).convert('RGB')
            else:
                img = reference_image.convert('RGB')
            
            img_np = np.array(img)
            
            # æ£€æµ‹äººè„¸
            faces = self.face_analyzer.get(img_np)
            
            if not faces:
                logger.warning("å‚è€ƒå›¾ä¸­æœªæ£€æµ‹åˆ°äººè„¸")
                return None
            
            # ä½¿ç”¨æœ€å¤§çš„äººè„¸
            largest_face = max(faces, key=lambda f: (f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))
            
            return largest_face.embedding
            
        except Exception as e:
            logger.error(f"æå–å‚è€ƒå›¾åµŒå…¥å¤±è´¥: {e}")
            return None
    
    def _calculate_similarity(
        self,
        embedding1: np.ndarray,
        embedding2: np.ndarray
    ) -> float:
        """è®¡ç®—ä¸¤ä¸ªåµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        if embedding1 is None or embedding2 is None:
            return 0.0
        
        similarity = np.dot(embedding1, embedding2) / (
            np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
        )
        
        return float(similarity)
    
    def analyze_video(
        self,
        video_path: str,
        reference_image: Union[str, Path, Image.Image],
        sample_interval: int = 5,
        max_frames: Optional[int] = None
    ) -> VideoIdentityReport:
        """
        åˆ†æè§†é¢‘çš„èº«ä»½ä¸€è‡´æ€§
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            reference_image: å‚è€ƒå›¾ï¼ˆè·¯å¾„æˆ– PIL Imageï¼‰
            sample_interval: é‡‡æ ·é—´éš”ï¼ˆæ¯éš”å‡ å¸§åˆ†æä¸€å¸§ï¼‰
            max_frames: æœ€å¤§åˆ†æå¸§æ•°
            
        Returns:
            VideoIdentityReport åˆ†ææŠ¥å‘Š
        """
        report = VideoIdentityReport(
            video_path=str(video_path),
            reference_path=str(reference_image) if isinstance(reference_image, (str, Path)) else "PIL.Image",
            timestamp=datetime.now().isoformat(),
            sample_interval=sample_interval,
            drift_threshold=self.drift_threshold
        )
        
        # æå–å‚è€ƒå›¾åµŒå…¥
        ref_embedding = self._extract_reference_embedding(reference_image)
        if ref_embedding is None:
            report.issues.append("å‚è€ƒå›¾ä¸­æœªæ£€æµ‹åˆ°äººè„¸")
            return report
        
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            report.issues.append(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return report
        
        # è·å–è§†é¢‘ä¿¡æ¯
        report.fps = cap.get(cv2.CAP_PROP_FPS)
        report.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        report.duration_sec = report.total_frames / report.fps if report.fps > 0 else 0
        
        logger.info(f"åˆ†æè§†é¢‘: {video_path}")
        logger.info(f"  å¸§æ•°: {report.total_frames}, FPS: {report.fps:.1f}, æ—¶é•¿: {report.duration_sec:.1f}s")
        
        # åˆ†æå¸§
        frame_results: List[FrameIdentityResult] = []
        prev_embedding = None
        
        frame_idx = 0
        analyzed_count = 0
        face_detected_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æŒ‰é—´éš”é‡‡æ ·
            if frame_idx % sample_interval != 0:
                frame_idx += 1
                continue
            
            # æ£€æŸ¥æœ€å¤§å¸§æ•°é™åˆ¶
            if max_frames and analyzed_count >= max_frames:
                break
            
            # è½¬æ¢é¢œè‰²ç©ºé—´
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # æ£€æµ‹äººè„¸
            result = self._analyze_frame(
                frame_rgb,
                frame_idx,
                ref_embedding,
                prev_embedding,
                report.fps
            )
            
            frame_results.append(result)
            
            if result.face_detected:
                face_detected_count += 1
                # ä¿å­˜å½“å‰å¸§åµŒå…¥ç”¨äºç›¸é‚»å¸§å¯¹æ¯”
                faces = self.face_analyzer.get(frame_rgb)
                if faces:
                    prev_embedding = faces[0].embedding
            
            analyzed_count += 1
            frame_idx += 1
        
        cap.release()
        
        # æ±‡æ€»ç»“æœ
        report.analyzed_frames = analyzed_count
        report.face_detected_ratio = face_detected_count / analyzed_count if analyzed_count > 0 else 0
        
        # è®¡ç®—ç›¸ä¼¼åº¦ç»Ÿè®¡
        similarities = [r.similarity for r in frame_results if r.face_detected]
        if similarities:
            report.frame_similarities = similarities
            report.avg_similarity = float(np.mean(similarities))
            report.min_similarity = float(np.min(similarities))
            report.max_similarity = float(np.max(similarities))
            report.std_similarity = float(np.std(similarities))
        
        # æ£€æµ‹èº«ä»½æ¼‚ç§»å¸§
        report.drift_frames = [
            r.frame_idx for r in frame_results
            if r.face_detected and r.similarity < self.drift_threshold
        ]
        report.drift_ratio = len(report.drift_frames) / face_detected_count if face_detected_count > 0 else 0
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        report.overall_passed = (
            report.avg_similarity >= self.similarity_threshold and
            report.drift_ratio <= 0.10 and  # æ¼‚ç§»å¸§ä¸è¶…è¿‡ 10%
            report.face_detected_ratio >= 0.8  # äººè„¸æ£€æµ‹ç‡
        )
        
        # ç”Ÿæˆé—®é¢˜åˆ—è¡¨
        if report.avg_similarity < self.similarity_threshold:
            report.issues.append(
                f"å¹³å‡ç›¸ä¼¼åº¦ä¸è¶³: {report.avg_similarity:.3f} < {self.similarity_threshold}"
            )
        
        if report.drift_ratio > 0.10:
            report.issues.append(
                f"èº«ä»½æ¼‚ç§»å¸§è¿‡å¤š: {len(report.drift_frames)}å¸§ ({report.drift_ratio*100:.1f}%)"
            )
        
        if report.face_detected_ratio < 0.8:
            report.issues.append(
                f"äººè„¸æ£€æµ‹ç‡ä½: {report.face_detected_ratio*100:.1f}%"
            )
        
        if report.min_similarity < 0.3:
            report.issues.append(
                f"å­˜åœ¨æä½ç›¸ä¼¼åº¦å¸§: {report.min_similarity:.3f}"
            )
        
        return report
    
    def _analyze_frame(
        self,
        frame_rgb: np.ndarray,
        frame_idx: int,
        ref_embedding: np.ndarray,
        prev_embedding: Optional[np.ndarray],
        fps: float
    ) -> FrameIdentityResult:
        """åˆ†æå•å¸§"""
        result = FrameIdentityResult(
            frame_idx=frame_idx,
            timestamp_sec=frame_idx / fps if fps > 0 else 0,
            similarity=0.0,
            face_detected=False
        )
        
        if self.face_analyzer is None:
            return result
        
        try:
            faces = self.face_analyzer.get(frame_rgb)
            
            if not faces:
                return result
            
            # ä½¿ç”¨æœ€å¤§çš„äººè„¸
            largest_face = max(faces, key=lambda f: (f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))
            
            result.face_detected = True
            result.face_bbox = tuple(map(int, largest_face.bbox))
            
            # è®¡ç®—ä¸å‚è€ƒå›¾çš„ç›¸ä¼¼åº¦
            result.similarity = self._calculate_similarity(
                largest_face.embedding,
                ref_embedding
            )
            
        except Exception as e:
            logger.debug(f"å¸§ {frame_idx} åˆ†æå¤±è´¥: {e}")
        
        return result
    
    def analyze_frames(
        self,
        frames: List[Image.Image],
        reference_image: Image.Image
    ) -> List[float]:
        """
        åˆ†æå¸§åˆ—è¡¨çš„èº«ä»½ç›¸ä¼¼åº¦
        
        Args:
            frames: å¸§åˆ—è¡¨
            reference_image: å‚è€ƒå›¾
            
        Returns:
            ç›¸ä¼¼åº¦åˆ—è¡¨
        """
        ref_embedding = self._extract_reference_embedding(reference_image)
        if ref_embedding is None:
            return [0.0] * len(frames)
        
        similarities = []
        for frame in frames:
            frame_np = np.array(frame)
            result = self._analyze_frame(frame_np, 0, ref_embedding, None, 1.0)
            similarities.append(result.similarity)
        
        return similarities
    
    def format_report(self, report: VideoIdentityReport) -> str:
        """æ ¼å¼åŒ–æŠ¥å‘Šä¸ºå¯è¯»å­—ç¬¦ä¸²"""
        lines = []
        lines.append("=" * 60)
        lines.append("ğŸ“¹ è§†é¢‘èº«ä»½åˆ†ææŠ¥å‘Š")
        lines.append("=" * 60)
        lines.append("")
        
        # è§†é¢‘ä¿¡æ¯
        lines.append(f"ğŸ“ è§†é¢‘: {Path(report.video_path).name}")
        lines.append(f"ğŸ“ å¸§æ•°: {report.total_frames} ({report.analyzed_frames} å·²åˆ†æ)")
        lines.append(f"â±ï¸ æ—¶é•¿: {report.duration_sec:.1f}s @ {report.fps:.1f}fps")
        lines.append("")
        
        # èº«ä»½ç›¸ä¼¼åº¦
        status = "âœ… é€šè¿‡" if report.overall_passed else "âŒ æœªé€šè¿‡"
        lines.append(f"ğŸ¯ æ€»ä½“çŠ¶æ€: {status}")
        lines.append("")
        
        lines.append("ğŸ‘¤ èº«ä»½ç›¸ä¼¼åº¦:")
        lines.append(f"   å¹³å‡: {report.avg_similarity:.3f}")
        lines.append(f"   æœ€ä½: {report.min_similarity:.3f}")
        lines.append(f"   æœ€é«˜: {report.max_similarity:.3f}")
        lines.append(f"   æ ‡å‡†å·®: {report.std_similarity:.3f}")
        lines.append("")
        
        # æ¼‚ç§»åˆ†æ
        lines.append("ğŸ“Š æ¼‚ç§»åˆ†æ:")
        lines.append(f"   æ¼‚ç§»é˜ˆå€¼: {report.drift_threshold}")
        lines.append(f"   æ¼‚ç§»å¸§æ•°: {len(report.drift_frames)} ({report.drift_ratio*100:.1f}%)")
        lines.append(f"   äººè„¸æ£€æµ‹ç‡: {report.face_detected_ratio*100:.1f}%")
        lines.append("")
        
        # é—®é¢˜
        if report.issues:
            lines.append("âš ï¸ é—®é¢˜:")
            for issue in report.issues:
                lines.append(f"   â€¢ {issue}")
            lines.append("")
        
        lines.append("=" * 60)
        
        return "\n".join(lines)
    
    def log_report(self, report: VideoIdentityReport):
        """è®°å½•æŠ¥å‘Šåˆ°æ—¥å¿—"""
        status = "âœ… é€šè¿‡" if report.overall_passed else "âŒ æœªé€šè¿‡"
        logger.info(f"è§†é¢‘èº«ä»½åˆ†æ: {status}")
        logger.info(f"  å¹³å‡ç›¸ä¼¼åº¦: {report.avg_similarity:.3f}")
        logger.info(f"  æ¼‚ç§»å¸§æ¯”ä¾‹: {report.drift_ratio*100:.1f}%")
        
        if report.issues:
            for issue in report.issues:
                logger.warning(f"  âš ï¸ {issue}")
    
    def unload(self):
        """å¸è½½æ¨¡å‹"""
        if self.face_analyzer is not None:
            del self.face_analyzer
            self.face_analyzer = None
            
            import gc
            gc.collect()
            
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except ImportError:
                pass
            
            logger.info("è§†é¢‘èº«ä»½åˆ†æå™¨å·²å¸è½½")


def analyze_video(
    video_path: str,
    reference_image: Union[str, Image.Image],
    sample_interval: int = 5,
    config: Optional[Dict[str, Any]] = None
) -> VideoIdentityReport:
    """
    å¿«æ·å‡½æ•°ï¼šåˆ†æè§†é¢‘èº«ä»½ä¸€è‡´æ€§
    
    Args:
        video_path: è§†é¢‘è·¯å¾„
        reference_image: å‚è€ƒå›¾
        sample_interval: é‡‡æ ·é—´éš”
        config: é…ç½®
        
    Returns:
        VideoIdentityReport
    """
    analyzer = VideoIdentityAnalyzer(config)
    try:
        report = analyzer.analyze_video(video_path, reference_image, sample_interval)
        return report
    finally:
        analyzer.unload()


if __name__ == "__main__":
    """æµ‹è¯•è§†é¢‘èº«ä»½åˆ†æå™¨"""
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("è§†é¢‘èº«ä»½åˆ†æå™¨æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = VideoIdentityAnalyzer()
    
    # æµ‹è¯•ï¼šå¦‚æœæœ‰è§†é¢‘æ–‡ä»¶
    test_video = Path("outputs/test_video.mp4")
    test_ref = Path("reference_image/hanli_mid.jpg")
    
    if test_video.exists() and test_ref.exists():
        print(f"\nåˆ†æè§†é¢‘: {test_video}")
        report = analyzer.analyze_video(
            str(test_video),
            str(test_ref),
            sample_interval=5
        )
        print(analyzer.format_report(report))
    else:
        print("\nâš ï¸ æµ‹è¯•è§†é¢‘æˆ–å‚è€ƒå›¾ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")
        print(f"  è§†é¢‘: {test_video} ({'å­˜åœ¨' if test_video.exists() else 'ä¸å­˜åœ¨'})")
        print(f"  å‚è€ƒå›¾: {test_ref} ({'å­˜åœ¨' if test_ref.exists() else 'ä¸å­˜åœ¨'})")
    
    # æ¸…ç†
    analyzer.unload()
    print("\nâœ… æµ‹è¯•å®Œæˆ!")

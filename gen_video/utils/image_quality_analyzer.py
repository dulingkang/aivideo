#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾åƒè´¨é‡åˆ†æå™¨
ç”¨äºè‡ªåŠ¨è¯„ä¼°ç”Ÿæˆçš„å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬ï¼š
- äººè„¸ç›¸ä¼¼åº¦ï¼ˆèº«ä»½ä¿æŒåº¦ï¼‰
- æ„å›¾è´¨é‡ï¼ˆè¿œæ™¯/ä¸­æ™¯/è¿‘æ™¯åˆ¤æ–­ï¼‰
- æ¸…æ™°åº¦è¯„ä¼°ï¼ˆæ‹‰æ™®æ‹‰æ–¯æ–¹å·®ï¼‰
- é¥±å’Œåº¦è¯„ä¼°ï¼ˆè‰²å½©ä¸°å¯Œåº¦ï¼‰
- å¯¹æ¯”åº¦è¯„ä¼°
- æ•´ä½“è´¨é‡è¯„åˆ†

Author: AI Video Team
Date: 2025-12-17
"""

from typing import Dict, Any, List, Tuple, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import cv2
from pathlib import Path
from PIL import Image
import logging
import json
from datetime import datetime

logger = logging.getLogger(__name__)


class ShotType(Enum):
    """é•œå¤´ç±»å‹"""
    WIDE = "wide"           # è¿œæ™¯
    MEDIUM = "medium"       # ä¸­æ™¯
    CLOSE = "close"         # è¿‘æ™¯
    EXTREME_CLOSE = "extreme_close"  # ç‰¹å†™
    UNKNOWN = "unknown"     # æœªçŸ¥


class QualityLevel(Enum):
    """è´¨é‡ç­‰çº§"""
    EXCELLENT = "excellent"  # ä¼˜ç§€ (90-100)
    GOOD = "good"           # è‰¯å¥½ (70-89)
    FAIR = "fair"           # ä¸€èˆ¬ (50-69)
    POOR = "poor"           # è¾ƒå·® (30-49)
    BAD = "bad"             # å¾ˆå·® (0-29)


@dataclass
class FaceSimilarityResult:
    """äººè„¸ç›¸ä¼¼åº¦ç»“æœ"""
    similarity: float = 0.0
    passed: bool = False
    threshold: float = 0.7
    face_detected_in_generated: bool = False
    face_detected_in_reference: bool = False
    error: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ Python åŸç”Ÿç±»å‹ï¼Œä»¥ä¾¿ JSON åºåˆ—åŒ–
        return {
            "similarity": float(self.similarity) if self.similarity else 0.0,
            "passed": bool(self.passed),
            "threshold": float(self.threshold),
            "face_detected_in_generated": bool(self.face_detected_in_generated),
            "face_detected_in_reference": bool(self.face_detected_in_reference),
            "error": self.error
        }


@dataclass
class CompositionResult:
    """æ„å›¾åˆ†æç»“æœ"""
    shot_type: ShotType = ShotType.UNKNOWN
    person_ratio: float = 0.0  # äººç‰©å ç”»é¢æ¯”ä¾‹
    center_weight: float = 0.0  # ä¸­å¿ƒåŒºåŸŸæƒé‡
    rule_of_thirds_score: float = 0.0  # ä¸‰åˆ†æ³•è¯„åˆ†
    face_position: Optional[Tuple[float, float]] = None  # äººè„¸åœ¨ç”»é¢ä¸­çš„ç›¸å¯¹ä½ç½®
    
    def to_dict(self) -> Dict[str, Any]:
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ Python åŸç”Ÿç±»å‹
        face_pos = None
        if self.face_position:
            face_pos = (float(self.face_position[0]), float(self.face_position[1]))
        return {
            "shot_type": self.shot_type.value,
            "person_ratio": float(self.person_ratio),
            "center_weight": float(self.center_weight),
            "rule_of_thirds_score": float(self.rule_of_thirds_score),
            "face_position": face_pos
        }


@dataclass
class TechnicalQualityResult:
    """æŠ€æœ¯è´¨é‡ç»“æœ"""
    sharpness: float = 0.0          # æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)
    sharpness_level: QualityLevel = QualityLevel.FAIR
    saturation: float = 0.0         # é¥±å’Œåº¦
    saturation_level: QualityLevel = QualityLevel.FAIR
    brightness: float = 0.0         # äº®åº¦
    brightness_level: QualityLevel = QualityLevel.FAIR
    contrast: float = 0.0           # å¯¹æ¯”åº¦
    contrast_level: QualityLevel = QualityLevel.FAIR
    noise_level: float = 0.0        # å™ªç‚¹æ°´å¹³ (è¶Šä½è¶Šå¥½)
    
    def to_dict(self) -> Dict[str, Any]:
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ Python åŸç”Ÿç±»å‹
        return {
            "sharpness": float(self.sharpness),
            "sharpness_level": self.sharpness_level.value,
            "saturation": float(self.saturation),
            "saturation_level": self.saturation_level.value,
            "brightness": float(self.brightness),
            "brightness_level": self.brightness_level.value,
            "contrast": float(self.contrast),
            "contrast_level": self.contrast_level.value,
            "noise_level": float(self.noise_level)
        }


@dataclass
class ImageQualityReport:
    """å›¾åƒè´¨é‡æŠ¥å‘Š"""
    image_path: Optional[str] = None
    image_size: Tuple[int, int] = (0, 0)
    timestamp: str = ""
    
    # å„é¡¹è¯„ä¼°ç»“æœ
    face_similarity: Optional[FaceSimilarityResult] = None
    composition: Optional[CompositionResult] = None
    technical: Optional[TechnicalQualityResult] = None
    
    # ç»¼åˆè¯„åˆ†
    overall_score: float = 0.0
    overall_level: QualityLevel = QualityLevel.FAIR
    
    # é—®é¢˜å’Œå»ºè®®
    issues: List[str] = field(default_factory=list)
    suggestions: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ Python åŸç”Ÿç±»å‹
        return {
            "image_path": self.image_path,
            "image_size": (int(self.image_size[0]), int(self.image_size[1])),
            "timestamp": self.timestamp,
            "face_similarity": self.face_similarity.to_dict() if self.face_similarity else None,
            "composition": self.composition.to_dict() if self.composition else None,
            "technical": self.technical.to_dict() if self.technical else None,
            "overall_score": float(self.overall_score),
            "overall_level": self.overall_level.value,
            "issues": list(self.issues),
            "suggestions": list(self.suggestions)
        }
    
    def to_json(self, indent: int = 2) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=indent)


class ImageQualityAnalyzer:
    """
    å›¾åƒè´¨é‡åˆ†æå™¨
    
    æä¾›å…¨é¢çš„å›¾åƒè´¨é‡è¯„ä¼°åŠŸèƒ½ï¼Œç”¨äºéªŒè¯ç”Ÿæˆå›¾åƒçš„è´¨é‡
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯é€‰
        """
        self.config = config or {}
        self.face_analyzer = None
        self.device = self.config.get("device", "cuda")
        
        # è´¨é‡é˜ˆå€¼
        self.thresholds = {
            "sharpness": {
                "excellent": 200,
                "good": 100,
                "fair": 50,
                "poor": 25
            },
            "saturation": {
                "excellent": 60,
                "good": 40,
                "fair": 25,
                "poor": 15
            },
            "brightness": {
                "min": 40,
                "max": 220,
                "optimal_min": 80,
                "optimal_max": 180
            },
            "contrast": {
                "excellent": 70,
                "good": 50,
                "fair": 35,
                "poor": 20
            }
        }
        
        # é•œå¤´ç±»å‹åˆ¤æ–­é˜ˆå€¼ (äººç‰©å ç”»é¢æ¯”ä¾‹)
        self.shot_thresholds = {
            "extreme_close": 0.5,  # >50% ä¸ºç‰¹å†™
            "close": 0.25,         # 25-50% ä¸ºè¿‘æ™¯
            "medium": 0.1,         # 10-25% ä¸ºä¸­æ™¯
            "wide": 0.0            # <10% ä¸ºè¿œæ™¯
        }
    
    def _load_face_analyzer(self):
        """å»¶è¿ŸåŠ è½½äººè„¸åˆ†æå™¨"""
        if self.face_analyzer is not None:
            return
        
        try:
            from insightface.app import FaceAnalysis
            import os
            
            # è·å–æ¨¡å‹è·¯å¾„
            # InsightFace ä¼šåœ¨ root/models/{name} ä¸‹æŸ¥æ‰¾æ¨¡å‹
            # æ‰€ä»¥ root åº”è¯¥æ˜¯åŒ…å« models ç›®å½•çš„çˆ¶ç›®å½•
            model_root = self.config.get("insightface_root", None)
            
            if model_root is None:
                # é»˜è®¤è·¯å¾„ï¼šgen_video ç›®å½•ï¼ˆå…¶ä¸‹æœ‰ models/antelopev2ï¼‰
                gen_video_dir = Path(__file__).parent.parent
                model_root = str(gen_video_dir)
            elif not os.path.isabs(model_root):
                # ç›¸å¯¹è·¯å¾„æ—¶ï¼ŒåŸºäº gen_video ç›®å½•
                gen_video_dir = Path(__file__).parent.parent
                model_root = str(gen_video_dir / model_root)
            
            logger.debug(f"InsightFace æ¨¡å‹æ ¹ç›®å½•: {model_root}")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            model_path = os.path.join(model_root, "models", "antelopev2")
            if not os.path.exists(model_path):
                logger.warning(f"InsightFace æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
                # å°è¯•å¤‡ç”¨è·¯å¾„
                alt_paths = [
                    os.path.join(gen_video_dir, "models", "antelopev2"),
                    os.path.expanduser("~/.insightface/models/antelopev2"),
                ]
                for alt_path in alt_paths:
                    if os.path.exists(alt_path):
                        model_root = os.path.dirname(os.path.dirname(alt_path))
                        logger.info(f"ä½¿ç”¨å¤‡ç”¨æ¨¡å‹è·¯å¾„: {alt_path}")
                        break
            
            self.face_analyzer = FaceAnalysis(
                name='antelopev2',
                root=model_root,
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
            )
            self.face_analyzer.prepare(ctx_id=0)
            logger.info("âœ… äººè„¸åˆ†æå™¨åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ äººè„¸åˆ†æå™¨åŠ è½½å¤±è´¥: {e}")
            self.face_analyzer = None
    
    def analyze(
        self,
        image: Union[str, Path, Image.Image, np.ndarray],
        reference_image: Optional[Union[str, Path, Image.Image, np.ndarray]] = None,
        similarity_threshold: float = 0.7,
        expected_shot_type: Optional[str] = None
    ) -> ImageQualityReport:
        """
        åˆ†æå›¾åƒè´¨é‡
        
        Args:
            image: è¦åˆ†æçš„å›¾åƒ (è·¯å¾„ã€PIL Image æˆ– numpy array)
            reference_image: å‚è€ƒå›¾åƒ (ç”¨äºäººè„¸ç›¸ä¼¼åº¦æ¯”è¾ƒ)
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            expected_shot_type: æœŸæœ›çš„é•œå¤´ç±»å‹
            
        Returns:
            ImageQualityReport åˆ†ææŠ¥å‘Š
        """
        report = ImageQualityReport(timestamp=datetime.now().isoformat())
        
        # åŠ è½½å›¾åƒ
        pil_image, image_np, image_path = self._load_image(image)
        if pil_image is None:
            report.issues.append("æ— æ³•åŠ è½½å›¾åƒ")
            return report
        
        report.image_path = image_path
        report.image_size = pil_image.size
        
        # 1. åˆ†ææŠ€æœ¯è´¨é‡
        report.technical = self._analyze_technical_quality(image_np)
        
        # 2. åˆ†ææ„å›¾
        report.composition = self._analyze_composition(image_np, pil_image)
        
        # 3. åˆ†æäººè„¸ç›¸ä¼¼åº¦ (å¦‚æœæä¾›äº†å‚è€ƒå›¾)
        if reference_image is not None:
            ref_pil, ref_np, _ = self._load_image(reference_image)
            if ref_pil is not None:
                report.face_similarity = self._analyze_face_similarity(
                    pil_image, ref_pil, similarity_threshold
                )
        
        # 4. è®¡ç®—ç»¼åˆè¯„åˆ†
        report.overall_score = self._calculate_overall_score(report)
        report.overall_level = self._score_to_level(report.overall_score)
        
        # 5. ç”Ÿæˆé—®é¢˜å’Œå»ºè®®
        self._generate_issues_and_suggestions(report, expected_shot_type)
        
        return report
    
    def _load_image(
        self,
        image: Union[str, Path, Image.Image, np.ndarray]
    ) -> Tuple[Optional[Image.Image], Optional[np.ndarray], Optional[str]]:
        """åŠ è½½å›¾åƒå¹¶è½¬æ¢ä¸ºéœ€è¦çš„æ ¼å¼"""
        image_path = None
        
        try:
            if isinstance(image, (str, Path)):
                image_path = str(image)
                pil_image = Image.open(image).convert('RGB')
                image_np = np.array(pil_image)
            elif isinstance(image, Image.Image):
                pil_image = image.convert('RGB')
                image_np = np.array(pil_image)
            elif isinstance(image, np.ndarray):
                image_np = image
                if image_np.shape[-1] == 4:  # RGBA
                    image_np = image_np[:, :, :3]
                pil_image = Image.fromarray(image_np)
            else:
                return None, None, None
            
            return pil_image, image_np, image_path
            
        except Exception as e:
            logger.error(f"åŠ è½½å›¾åƒå¤±è´¥: {e}")
            return None, None, None
    
    def _analyze_technical_quality(self, image_np: np.ndarray) -> TechnicalQualityResult:
        """åˆ†ææŠ€æœ¯è´¨é‡æŒ‡æ ‡"""
        result = TechnicalQualityResult()
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
        
        # 1. æ¸…æ™°åº¦ (æ‹‰æ™®æ‹‰æ–¯æ–¹å·®)
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        result.sharpness = float(laplacian.var())
        result.sharpness_level = self._get_quality_level("sharpness", result.sharpness)
        
        # 2. é¥±å’Œåº¦ (HSV ç©ºé—´çš„ S é€šé“)
        hsv = cv2.cvtColor(image_np, cv2.COLOR_RGB2HSV)
        result.saturation = float(hsv[:, :, 1].mean())
        result.saturation_level = self._get_quality_level("saturation", result.saturation)
        
        # 3. äº®åº¦ (HSV ç©ºé—´çš„ V é€šé“)
        result.brightness = float(hsv[:, :, 2].mean())
        result.brightness_level = self._get_brightness_level(result.brightness)
        
        # 4. å¯¹æ¯”åº¦ (ç°åº¦å›¾çš„æ ‡å‡†å·®)
        result.contrast = float(gray.std())
        result.contrast_level = self._get_quality_level("contrast", result.contrast)
        
        # 5. å™ªç‚¹ä¼°è®¡ (ä½¿ç”¨é«˜é¢‘æˆåˆ†)
        # ä½¿ç”¨é«˜æ–¯æ¨¡ç³Šåçš„å·®å¼‚æ¥ä¼°è®¡å™ªç‚¹
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        noise = np.abs(gray.astype(float) - blurred.astype(float))
        result.noise_level = float(noise.mean())
        
        return result
    
    def _analyze_composition(
        self,
        image_np: np.ndarray,
        pil_image: Image.Image
    ) -> CompositionResult:
        """åˆ†ææ„å›¾"""
        result = CompositionResult()
        h, w = image_np.shape[:2]
        
        # å°è¯•ä½¿ç”¨äººè„¸æ£€æµ‹æ¥åˆ¤æ–­æ„å›¾
        self._load_face_analyzer()
        
        face_bbox = None
        if self.face_analyzer is not None:
            try:
                faces = self.face_analyzer.get(image_np)
                if faces:
                    # ä½¿ç”¨æœ€å¤§çš„äººè„¸
                    largest_face = max(faces, key=lambda f: (f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))
                    face_bbox = largest_face.bbox  # [x1, y1, x2, y2]
                    
                    # è®¡ç®—äººè„¸å ç”»é¢æ¯”ä¾‹
                    face_w = face_bbox[2] - face_bbox[0]
                    face_h = face_bbox[3] - face_bbox[1]
                    face_area = face_w * face_h
                    image_area = w * h
                    result.person_ratio = face_area / image_area
                    
                    # è®¡ç®—äººè„¸ä¸­å¿ƒä½ç½® (å½’ä¸€åŒ–åˆ° 0-1)
                    face_center_x = (face_bbox[0] + face_bbox[2]) / 2 / w
                    face_center_y = (face_bbox[1] + face_bbox[3]) / 2 / h
                    result.face_position = (face_center_x, face_center_y)
                    
            except Exception as e:
                logger.debug(f"äººè„¸æ£€æµ‹å¤±è´¥: {e}")
        
        # åˆ¤æ–­é•œå¤´ç±»å‹
        if result.person_ratio > self.shot_thresholds["extreme_close"]:
            result.shot_type = ShotType.EXTREME_CLOSE
        elif result.person_ratio > self.shot_thresholds["close"]:
            result.shot_type = ShotType.CLOSE
        elif result.person_ratio > self.shot_thresholds["medium"]:
            result.shot_type = ShotType.MEDIUM
        elif result.person_ratio > 0:
            result.shot_type = ShotType.WIDE
        else:
            # å¦‚æœæ²¡æ£€æµ‹åˆ°äººè„¸ï¼Œä½¿ç”¨ç®€å•çš„äº®åº¦å¯¹æ¯”åº¦åˆ¤æ–­
            result.shot_type = self._estimate_shot_type_by_contrast(image_np)
        
        # è®¡ç®—ä¸­å¿ƒæƒé‡ (ç”¨äºåˆ¤æ–­äººç‰©æ˜¯å¦åœ¨ç”»é¢ä¸­å¿ƒ)
        center_region = image_np[h//4:3*h//4, w//4:3*w//4]
        result.center_weight = float(center_region.mean()) / 255.0
        
        # è®¡ç®—ä¸‰åˆ†æ³•è¯„åˆ† (äººè„¸ä½ç½®æ˜¯å¦åœ¨ä¸‰åˆ†çº¿ä¸Š)
        if result.face_position:
            thirds_score = self._calculate_rule_of_thirds_score(result.face_position)
            result.rule_of_thirds_score = thirds_score
        
        return result
    
    def _estimate_shot_type_by_contrast(self, image_np: np.ndarray) -> ShotType:
        """é€šè¿‡å¯¹æ¯”åº¦ä¼°è®¡é•œå¤´ç±»å‹"""
        h, w = image_np.shape[:2]
        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
        
        # ä¸­å¿ƒåŒºåŸŸ
        center = gray[h//4:3*h//4, w//4:3*w//4]
        center_brightness = center.mean()
        
        # è¾¹ç¼˜åŒºåŸŸ
        edge_regions = [
            gray[:h//8, :],      # ä¸Š
            gray[-h//8:, :],     # ä¸‹
            gray[:, :w//8],      # å·¦
            gray[:, -w//8:]      # å³
        ]
        edge_brightness = np.mean([r.mean() for r in edge_regions])
        
        contrast = abs(center_brightness - edge_brightness)
        
        if contrast > 60:
            return ShotType.CLOSE
        elif contrast > 30:
            return ShotType.MEDIUM
        else:
            return ShotType.WIDE
    
    def _calculate_rule_of_thirds_score(
        self,
        position: Tuple[float, float]
    ) -> float:
        """è®¡ç®—ä¸‰åˆ†æ³•è¯„åˆ†"""
        x, y = position
        
        # ä¸‰åˆ†çº¿ä½ç½®: 1/3, 2/3
        thirds = [1/3, 2/3]
        
        # è®¡ç®—åˆ°æœ€è¿‘ä¸‰åˆ†çº¿çš„è·ç¦»
        x_distance = min(abs(x - t) for t in thirds)
        y_distance = min(abs(y - t) for t in thirds)
        
        # è·ç¦»è¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜
        x_score = max(0, 1 - x_distance * 3)
        y_score = max(0, 1 - y_distance * 3)
        
        return (x_score + y_score) / 2
    
    def _analyze_face_similarity(
        self,
        generated: Image.Image,
        reference: Image.Image,
        threshold: float
    ) -> FaceSimilarityResult:
        """åˆ†æäººè„¸ç›¸ä¼¼åº¦"""
        result = FaceSimilarityResult(threshold=threshold)
        
        self._load_face_analyzer()
        
        if self.face_analyzer is None:
            result.error = "äººè„¸åˆ†æå™¨æœªåŠ è½½"
            return result
        
        try:
            gen_np = np.array(generated)
            ref_np = np.array(reference)
            
            # æ£€æµ‹äººè„¸
            gen_faces = self.face_analyzer.get(gen_np)
            ref_faces = self.face_analyzer.get(ref_np)
            
            result.face_detected_in_generated = len(gen_faces) > 0
            result.face_detected_in_reference = len(ref_faces) > 0
            
            if not gen_faces:
                result.error = "ç”Ÿæˆå›¾åƒä¸­æœªæ£€æµ‹åˆ°äººè„¸"
                return result
            
            if not ref_faces:
                result.error = "å‚è€ƒå›¾åƒä¸­æœªæ£€æµ‹åˆ°äººè„¸"
                return result
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            gen_emb = gen_faces[0].embedding
            ref_emb = ref_faces[0].embedding
            
            similarity = np.dot(gen_emb, ref_emb) / (
                np.linalg.norm(gen_emb) * np.linalg.norm(ref_emb)
            )
            
            result.similarity = float(similarity)
            result.passed = similarity >= threshold
            
        except Exception as e:
            result.error = str(e)
            logger.error(f"äººè„¸ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")
        
        return result
    
    def _get_quality_level(self, metric: str, value: float) -> QualityLevel:
        """æ ¹æ®æŒ‡æ ‡å€¼è·å–è´¨é‡ç­‰çº§"""
        thresholds = self.thresholds.get(metric, {})
        
        if value >= thresholds.get("excellent", float('inf')):
            return QualityLevel.EXCELLENT
        elif value >= thresholds.get("good", float('inf')):
            return QualityLevel.GOOD
        elif value >= thresholds.get("fair", float('inf')):
            return QualityLevel.FAIR
        elif value >= thresholds.get("poor", float('inf')):
            return QualityLevel.POOR
        else:
            return QualityLevel.BAD
    
    def _get_brightness_level(self, brightness: float) -> QualityLevel:
        """è·å–äº®åº¦ç­‰çº§"""
        thresholds = self.thresholds["brightness"]
        
        if brightness < thresholds["min"] or brightness > thresholds["max"]:
            return QualityLevel.POOR
        elif thresholds["optimal_min"] <= brightness <= thresholds["optimal_max"]:
            return QualityLevel.EXCELLENT
        else:
            return QualityLevel.GOOD
    
    def _score_to_level(self, score: float) -> QualityLevel:
        """åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 90:
            return QualityLevel.EXCELLENT
        elif score >= 70:
            return QualityLevel.GOOD
        elif score >= 50:
            return QualityLevel.FAIR
        elif score >= 30:
            return QualityLevel.POOR
        else:
            return QualityLevel.BAD
    
    def _calculate_overall_score(self, report: ImageQualityReport) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        scores = []
        weights = []
        
        # æŠ€æœ¯è´¨é‡æƒé‡: 40%
        if report.technical:
            tech = report.technical
            tech_score = 0
            
            # æ¸…æ™°åº¦ (å æŠ€æœ¯è´¨é‡çš„ 40%)
            sharpness_score = min(100, tech.sharpness / 2)
            tech_score += sharpness_score * 0.4
            
            # é¥±å’Œåº¦ (å æŠ€æœ¯è´¨é‡çš„ 25%)
            saturation_score = min(100, tech.saturation * 1.5)
            tech_score += saturation_score * 0.25
            
            # äº®åº¦ (å æŠ€æœ¯è´¨é‡çš„ 20%)
            # æœ€ä½³äº®åº¦åœ¨ 80-180 èŒƒå›´å†…
            if 80 <= tech.brightness <= 180:
                brightness_score = 100
            else:
                distance = min(abs(tech.brightness - 80), abs(tech.brightness - 180))
                brightness_score = max(0, 100 - distance)
            tech_score += brightness_score * 0.2
            
            # å¯¹æ¯”åº¦ (å æŠ€æœ¯è´¨é‡çš„ 15%)
            contrast_score = min(100, tech.contrast * 1.5)
            tech_score += contrast_score * 0.15
            
            scores.append(tech_score)
            weights.append(0.4)
        
        # æ„å›¾æƒé‡: 20%
        if report.composition:
            comp = report.composition
            comp_score = 50  # åŸºç¡€åˆ†
            
            # ä¸‰åˆ†æ³•è¯„åˆ†
            if comp.rule_of_thirds_score > 0:
                comp_score += comp.rule_of_thirds_score * 30
            
            # äººç‰©æ¯”ä¾‹åˆç†æ€§
            if comp.shot_type != ShotType.UNKNOWN:
                comp_score += 20
            
            scores.append(min(100, comp_score))
            weights.append(0.2)
        
        # äººè„¸ç›¸ä¼¼åº¦æƒé‡: 40%
        if report.face_similarity:
            face = report.face_similarity
            if face.similarity > 0:
                # ç›¸ä¼¼åº¦ 0.5-1.0 æ˜ å°„åˆ° 0-100
                face_score = max(0, (face.similarity - 0.5) * 200)
                scores.append(min(100, face_score))
                weights.append(0.4)
            elif not face.face_detected_in_generated:
                # æœªæ£€æµ‹åˆ°äººè„¸ï¼Œç»™äºˆè¾ƒä½åˆ†æ•°
                scores.append(30)
                weights.append(0.4)
        
        # è®¡ç®—åŠ æƒå¹³å‡
        if not scores:
            return 50.0
        
        total_weight = sum(weights)
        weighted_score = sum(s * w for s, w in zip(scores, weights)) / total_weight
        
        return round(weighted_score, 1)
    
    def _generate_issues_and_suggestions(
        self,
        report: ImageQualityReport,
        expected_shot_type: Optional[str] = None
    ):
        """ç”Ÿæˆé—®é¢˜å’Œå»ºè®®"""
        issues = []
        suggestions = []
        
        # æ£€æŸ¥æŠ€æœ¯è´¨é‡
        if report.technical:
            tech = report.technical
            
            if tech.sharpness_level in [QualityLevel.POOR, QualityLevel.BAD]:
                issues.append(f"å›¾åƒæ¨¡ç³Š (æ¸…æ™°åº¦: {tech.sharpness:.1f})")
                suggestions.append("å»ºè®®å¢åŠ ç”Ÿæˆæ­¥æ•°æˆ–é™ä½å¼•å¯¼å¼ºåº¦")
            
            if tech.saturation_level in [QualityLevel.POOR, QualityLevel.BAD]:
                issues.append(f"è‰²å½©é¥±å’Œåº¦è¿‡ä½ (é¥±å’Œåº¦: {tech.saturation:.1f})")
                suggestions.append("å»ºè®®åœ¨ prompt ä¸­æ·»åŠ è‰²å½©æè¿°è¯")
            
            if tech.brightness_level == QualityLevel.POOR:
                if tech.brightness < 50:
                    issues.append(f"å›¾åƒè¿‡æš— (äº®åº¦: {tech.brightness:.1f})")
                    suggestions.append("å»ºè®®è°ƒæ•´åœºæ™¯å…‰ç…§æè¿°")
                elif tech.brightness > 200:
                    issues.append(f"å›¾åƒè¿‡äº® (äº®åº¦: {tech.brightness:.1f})")
                    suggestions.append("å»ºè®®è°ƒæ•´åœºæ™¯å…‰ç…§æè¿°")
        
        # æ£€æŸ¥äººè„¸ç›¸ä¼¼åº¦
        if report.face_similarity:
            face = report.face_similarity
            
            if not face.face_detected_in_generated:
                issues.append("ç”Ÿæˆå›¾åƒä¸­æœªæ£€æµ‹åˆ°äººè„¸")
                suggestions.append("æ£€æŸ¥äººç‰©æ˜¯å¦åœ¨ç”»é¢ä¸­ï¼Œæˆ–å°è¯•ä½¿ç”¨è¿‘æ™¯é•œå¤´")
            elif not face.passed:
                issues.append(f"äººè„¸ç›¸ä¼¼åº¦ä¸è¶³ ({face.similarity:.3f} < {face.threshold})")
                suggestions.append("å»ºè®®å¢åŠ å‚è€ƒå¼ºåº¦æˆ–ä½¿ç”¨æ›´æ¸…æ™°çš„å‚è€ƒå›¾")
        
        # æ£€æŸ¥æ„å›¾
        if report.composition and expected_shot_type:
            actual_shot = report.composition.shot_type.value
            if actual_shot != expected_shot_type and report.composition.shot_type != ShotType.UNKNOWN:
                issues.append(f"é•œå¤´ç±»å‹ä¸åŒ¹é… (æœŸæœ›: {expected_shot_type}, å®é™…: {actual_shot})")
                suggestions.append("å»ºè®®è°ƒæ•´ prompt ä¸­çš„é•œå¤´æè¿°")
        
        report.issues = issues
        report.suggestions = suggestions
    
    def format_report(
        self,
        report: ImageQualityReport,
        verbose: bool = True
    ) -> str:
        """
        æ ¼å¼åŒ–æŠ¥å‘Šä¸ºå¯è¯»å­—ç¬¦ä¸²
        
        Args:
            report: è´¨é‡æŠ¥å‘Š
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        lines = []
        lines.append("=" * 60)
        lines.append("ğŸ“Š å›¾åƒè´¨é‡åˆ†ææŠ¥å‘Š")
        lines.append("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        if report.image_path:
            lines.append(f"ğŸ“ æ–‡ä»¶: {Path(report.image_path).name}")
        lines.append(f"ğŸ“ å°ºå¯¸: {report.image_size[0]}x{report.image_size[1]}")
        lines.append(f"ğŸ• æ—¶é—´: {report.timestamp}")
        lines.append("")
        
        # ç»¼åˆè¯„åˆ†
        level_emoji = {
            QualityLevel.EXCELLENT: "ğŸŒŸ",
            QualityLevel.GOOD: "âœ…",
            QualityLevel.FAIR: "ğŸŸ¡",
            QualityLevel.POOR: "ğŸŸ ",
            QualityLevel.BAD: "ğŸ”´"
        }
        emoji = level_emoji.get(report.overall_level, "â“")
        lines.append(f"ğŸ¯ ç»¼åˆè¯„åˆ†: {report.overall_score:.1f}/100 {emoji} {report.overall_level.value.upper()}")
        lines.append("")
        
        # äººè„¸ç›¸ä¼¼åº¦
        if report.face_similarity:
            face = report.face_similarity
            lines.append("ğŸ‘¤ äººè„¸ç›¸ä¼¼åº¦:")
            if face.error:
                lines.append(f"   âš ï¸ {face.error}")
            else:
                status = "âœ… é€šè¿‡" if face.passed else "âŒ æœªé€šè¿‡"
                lines.append(f"   ç›¸ä¼¼åº¦: {face.similarity:.3f} (é˜ˆå€¼: {face.threshold}) {status}")
                
                # ç›¸ä¼¼åº¦ç­‰çº§
                if face.similarity >= 0.8:
                    sim_level = "ğŸŸ¢ ä¼˜ç§€"
                elif face.similarity >= 0.7:
                    sim_level = "ğŸŸ¡ è‰¯å¥½"
                elif face.similarity >= 0.5:
                    sim_level = "ğŸŸ  ä¸€èˆ¬"
                else:
                    sim_level = "ğŸ”´ è¾ƒå·®"
                lines.append(f"   ç­‰çº§: {sim_level}")
            lines.append("")
        
        # æ„å›¾åˆ†æ
        if report.composition and verbose:
            comp = report.composition
            lines.append("ğŸ¬ æ„å›¾åˆ†æ:")
            shot_emoji = {
                ShotType.EXTREME_CLOSE: "ğŸ”",
                ShotType.CLOSE: "ğŸ‘ï¸",
                ShotType.MEDIUM: "ğŸ“·",
                ShotType.WIDE: "ğŸï¸",
                ShotType.UNKNOWN: "â“"
            }
            lines.append(f"   é•œå¤´ç±»å‹: {shot_emoji.get(comp.shot_type, '')} {comp.shot_type.value}")
            if comp.person_ratio > 0:
                lines.append(f"   äººç‰©å æ¯”: {comp.person_ratio*100:.1f}%")
            if comp.face_position:
                lines.append(f"   äººè„¸ä½ç½®: ({comp.face_position[0]:.2f}, {comp.face_position[1]:.2f})")
            if comp.rule_of_thirds_score > 0:
                lines.append(f"   ä¸‰åˆ†æ³•è¯„åˆ†: {comp.rule_of_thirds_score*100:.1f}%")
            lines.append("")
        
        # æŠ€æœ¯è´¨é‡
        if report.technical and verbose:
            tech = report.technical
            lines.append("ğŸ“Š æŠ€æœ¯æŒ‡æ ‡:")
            
            level_symbols = {
                QualityLevel.EXCELLENT: "ğŸŸ¢",
                QualityLevel.GOOD: "ğŸŸ¢",
                QualityLevel.FAIR: "ğŸŸ¡",
                QualityLevel.POOR: "ğŸŸ ",
                QualityLevel.BAD: "ğŸ”´"
            }
            
            lines.append(f"   æ¸…æ™°åº¦: {tech.sharpness:.1f} {level_symbols[tech.sharpness_level]}")
            lines.append(f"   é¥±å’Œåº¦: {tech.saturation:.1f} {level_symbols[tech.saturation_level]}")
            lines.append(f"   äº®åº¦: {tech.brightness:.1f} {level_symbols[tech.brightness_level]}")
            lines.append(f"   å¯¹æ¯”åº¦: {tech.contrast:.1f} {level_symbols[tech.contrast_level]}")
            if tech.noise_level > 0:
                noise_level = "ä½" if tech.noise_level < 5 else ("ä¸­" if tech.noise_level < 10 else "é«˜")
                lines.append(f"   å™ªç‚¹: {tech.noise_level:.1f} ({noise_level})")
            lines.append("")
        
        # é—®é¢˜å’Œå»ºè®®
        if report.issues:
            lines.append("âš ï¸ å‘ç°é—®é¢˜:")
            for issue in report.issues:
                lines.append(f"   â€¢ {issue}")
            lines.append("")
        
        if report.suggestions:
            lines.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for suggestion in report.suggestions:
                lines.append(f"   â€¢ {suggestion}")
            lines.append("")
        
        lines.append("=" * 60)
        
        return "\n".join(lines)
    
    def log_report(
        self,
        report: ImageQualityReport,
        level: str = "info"
    ):
        """
        å°†æŠ¥å‘Šè¾“å‡ºåˆ°æ—¥å¿—
        
        Args:
            report: è´¨é‡æŠ¥å‘Š
            level: æ—¥å¿—çº§åˆ« (debug, info, warning)
        """
        formatted = self.format_report(report, verbose=(level == "debug"))
        
        log_func = getattr(logger, level, logger.info)
        for line in formatted.split("\n"):
            log_func(line)
    
    def unload(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾èµ„æº"""
        if self.face_analyzer is not None:
            del self.face_analyzer
            self.face_analyzer = None
            
        import gc
        import torch
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.debug("å›¾åƒè´¨é‡åˆ†æå™¨å·²å¸è½½")


def analyze_image(
    image_path: str,
    reference_path: Optional[str] = None,
    threshold: float = 0.7
) -> ImageQualityReport:
    """
    åˆ†æå›¾åƒè´¨é‡çš„ä¾¿æ·å‡½æ•°
    
    Args:
        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        reference_path: å‚è€ƒå›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
    Returns:
        ImageQualityReport åˆ†ææŠ¥å‘Š
    """
    analyzer = ImageQualityAnalyzer()
    try:
        return analyzer.analyze(
            image_path,
            reference_image=reference_path,
            similarity_threshold=threshold
        )
    finally:
        analyzer.unload()


if __name__ == "__main__":
    """æµ‹è¯•å›¾åƒè´¨é‡åˆ†æå™¨"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python image_quality_analyzer.py <image_path> [reference_path]")
        sys.exit(1)
    
    image_path = sys.argv[1]
    reference_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    try:
        analyzer = ImageQualityAnalyzer()
        report = analyzer.analyze(
            image_path,
            reference_image=reference_path,
            similarity_threshold=0.7
        )
        
        # æ‰“å°æŠ¥å‘Š
        print(analyzer.format_report(report))
        
        # ä¿å­˜ JSON
        json_path = Path(image_path).with_suffix('.quality.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            f.write(report.to_json())
        print(f"\nğŸ“ JSON æŠ¥å‘Šå·²ä¿å­˜: {json_path}")
        
        analyzer.unload()
        
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

# Flux LoRA è®­ç»ƒæ˜¾å­˜ä¼˜åŒ–æŒ‡å—

## âš ï¸ æ˜¾å­˜ä¸è¶³é—®é¢˜

å¦‚æœé‡åˆ° `CUDA out of memory` é”™è¯¯ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤ä¼˜åŒ–ï¼š

---

## ğŸ”§ ä¼˜åŒ–æ–¹æ¡ˆ

### 1. å‡å°‘æ‰¹æ¬¡å¤§å°

```bash
python train_flux_lora_final.py \
    --batch-size 1 \  # ä» 2 æ”¹ä¸º 1
    --gradient-accumulation 4 \  # ä» 2 æ”¹ä¸º 4ï¼ˆä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼‰
    --use-bf16
```

### 2. é™ä½åˆ†è¾¨ç‡

```bash
python train_flux_lora_final.py \
    --resolution 512 \  # ä» 1024 æ”¹ä¸º 512
    --batch-size 1 \
    --use-bf16
```

### 3. æ£€æŸ¥å¹¶å…³é—­å…¶ä»–å ç”¨æ˜¾å­˜çš„è¿›ç¨‹

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi

# æŸ¥çœ‹å ç”¨æ˜¾å­˜çš„è¿›ç¨‹
fuser -v /dev/nvidia*

# å¦‚æœéœ€è¦ï¼Œå¯ä»¥ kill å…¶ä»–è¿›ç¨‹ï¼ˆè°¨æ…æ“ä½œï¼‰
# kill -9 <PID>
```

### 4. ä½¿ç”¨æ›´å°çš„ LoRA rank

```bash
python train_flux_lora_final.py \
    --lora-rank 16 \  # ä» 32 æ”¹ä¸º 16
    --lora-alpha 8 \  # ä» 16 æ”¹ä¸º 8
    --batch-size 1 \
    --use-bf16
```

### 5. å¯ç”¨ç¼“å­˜ latentsï¼ˆå¦‚æœæ”¯æŒï¼‰

ä¿®æ”¹è„šæœ¬ä»¥ç¼“å­˜ VAE ç¼–ç çš„ latentsï¼Œé¿å…æ¯æ¬¡é‡æ–°ç¼–ç ã€‚

---

## ğŸ“‹ æ¨èé…ç½®ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶ï¼‰

```bash
source /vepfs-dev/shawn/venv/py312/bin/activate
cd /vepfs-dev/shawn/vid/fanren/gen_video

python train_flux_lora_final.py \
    --data-dir train_data/host_person \
    --output-dir models/lora/host_person \
    --base-model models/flux1-dev \
    --epochs 10 \
    --batch-size 1 \
    --gradient-accumulation 4 \
    --learning-rate 1e-4 \
    --lora-rank 16 \
    --lora-alpha 8 \
    --resolution 512 \
    --use-bf16
```

---

## âœ… å·²å®ç°çš„ä¼˜åŒ–

è„šæœ¬å·²åŒ…å«ä»¥ä¸‹ä¼˜åŒ–ï¼š

1. âœ… **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼ˆgradient checkpointingï¼‰- èŠ‚çœçº¦ 40% æ˜¾å­˜
2. âœ… **é‡åŒ–æ¨¡å‹**ï¼ˆbf16/fp16ï¼‰- èŠ‚çœçº¦ 50% æ˜¾å­˜
3. âœ… **æ˜¾å­˜æ¸…ç†**ï¼ˆtorch.cuda.empty_cache()ï¼‰- åŠæ—¶é‡Šæ”¾æœªä½¿ç”¨çš„æ˜¾å­˜
4. âœ… **8bit AdamW**ï¼ˆå¦‚æœå®‰è£…äº† bitsandbytesï¼‰- èŠ‚çœä¼˜åŒ–å™¨æ˜¾å­˜

---

## ğŸ” æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§æ˜¾å­˜
watch -n 1 nvidia-smi

# æŸ¥çœ‹ PyTorch æ˜¾å­˜ä½¿ç”¨
python -c "import torch; print(f'å·²åˆ†é…: {torch.cuda.memory_allocated()/1024**3:.2f} GB'); print(f'å·²ç¼“å­˜: {torch.cuda.memory_reserved()/1024**3:.2f} GB')"
```

---

## ğŸ’¡ å…¶ä»–å»ºè®®

1. **ä½¿ç”¨ç¯å¢ƒå˜é‡**ï¼ˆå¦‚æœ PyTorch æ”¯æŒï¼‰ï¼š
   ```bash
   export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
   ```

2. **å‡å°‘æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹**ï¼š
   ```python
   num_workers=0  # ä» 2 æ”¹ä¸º 0
   ```

3. **ä½¿ç”¨æ›´å°çš„éªŒè¯é¢‘ç‡**ï¼š
   ```python
   --save-steps 500  # ä» 200 æ”¹ä¸º 500
   ```

---

## ğŸ“Š æ˜¾å­˜ä¼°ç®—

| é…ç½® | ä¼°ç®—æ˜¾å­˜ |
|------|---------|
| batch_size=2, resolution=1024 | ~85 GB |
| batch_size=1, resolution=1024 | ~65 GB |
| batch_size=1, resolution=512 | ~45 GB |
| batch_size=1, resolution=512, rank=16 | ~40 GB |

---

## ğŸš¨ å¦‚æœä»ç„¶æ˜¾å­˜ä¸è¶³

1. **æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨æ˜¾å­˜**
2. **é‡å¯æœåŠ¡å™¨é‡Šæ”¾æ˜¾å­˜**
3. **è€ƒè™‘ä½¿ç”¨å¤š GPU è®­ç»ƒ**ï¼ˆå¦‚æœå¯ç”¨ï¼‰
4. **ä½¿ç”¨æ›´å°çš„æ¨¡å‹**ï¼ˆFlux.1-schnell æˆ–å…¶ä»–è½»é‡ç‰ˆæœ¬ï¼‰


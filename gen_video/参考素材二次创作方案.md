# 基于原动漫素材的二次AI创作方案

## 📋 方案概述

本方案旨在帮助您基于原动漫的图片和视频素材，进行二次AI创作，生成符合解说场景的新内容，而不是直接拼接原视频。

## 🎯 核心思路

> **重要更新**：推荐使用**整体风格学习方案**，而非单张图片参考。详见 `整体风格学习方案.md`

### 方案A：整体风格学习（最推荐）⭐⭐⭐

**核心思路：**
- 从原动漫素材中提取20-50张关键帧
- 训练风格LoRA模型，学习整体风格
- 生成时使用风格LoRA + 角色LoRA + InstantID
- 所有场景保持统一的原动漫风格

**优点：**
- ✅ 学习整体风格，而非单张图片
- ✅ 风格统一，所有场景保持一致
- ✅ 训练一次，长期使用

详见：`整体风格学习方案.md`

### 方案B：参考图像风格迁移（备选）

**流程：**
```
原动漫图片/视频关键帧 
  ↓ 提取关键帧
参考图像库
  ↓ IP-Adapter / ControlNet
风格迁移 + 场景描述
  ↓ InstantID + SDXL
新生成的图像（保持风格，符合场景）
  ↓ SVD
新生成的视频
```

**优点：**
- ✅ 保持原动漫的视觉风格
- ✅ 可以自由调整场景内容
- ✅ 生成质量高，风格统一
- ✅ 支持批量处理

**适用场景：**
- 需要保持原动漫风格
- 场景描述与原素材类似但需要调整
- 需要生成多个相似场景

### 方案B：img2img二次创作

**流程：**
```
原动漫图片
  ↓ img2img (strength=0.4-0.6)
轻微修改 + 场景描述
  ↓ InstantID + SDXL
新生成的图像（基于原图但符合新场景）
  ↓ SVD
新生成的视频
```

**优点：**
- ✅ 最大程度保留原图构图和细节
- ✅ 可以精确控制修改程度
- ✅ 生成速度快

**适用场景：**
- 原图构图很好，只需要微调
- 需要保持相似的画面布局
- 快速生成变体

### 方案C：混合方案（最佳）

**流程：**
```
1. 从原动漫视频提取关键帧
2. 建立参考图像库（按场景分类）
3. 根据解说场景，选择最相似的参考图像
4. 使用 IP-Adapter + img2img 组合：
   - IP-Adapter: 控制整体风格
   - img2img: 基于参考图进行二次创作
   - ControlNet: 控制构图和结构
5. 生成新图像 → 生成新视频
```

**优点：**
- ✅ 结合多种技术的优势
- ✅ 灵活度高，可精确控制
- ✅ 风格一致性强

## 🛠️ 技术实现

### 1. 参考图像提取工具

从原动漫视频中提取关键帧，建立参考图像库：

```python
# 工具：extract_reference_frames.py
# 功能：
# - 从视频中提取关键帧（按场景变化）
# - 自动分类和标注
# - 生成参考图像索引
```

### 2. IP-Adapter 风格迁移

使用 IP-Adapter 将原动漫风格迁移到新场景：

```python
# 在 image_generator.py 中扩展
# 支持：
# - 多参考图像（风格、构图、色彩）
# - IP-Adapter 权重调节
# - 与 InstantID 结合使用
```

### 3. img2img 二次创作

基于参考图像进行 img2img 生成：

```python
# 在 image_generator.py 中扩展
# 支持：
# - 参考图像作为 init_image
# - 可调节 strength（0.3-0.7）
# - 结合 prompt 进行场景调整
```

### 4. 智能参考图像匹配

根据场景描述，自动选择最相似的参考图像：

```python
# 工具：match_reference_image.py
# 功能：
# - 场景描述 → 语义搜索
# - 图像特征提取和匹配
# - 自动选择最佳参考图像
```

## 📁 目录结构

```
gen_video/
├── reference_materials/          # 参考素材目录
│   ├── images/                   # 原动漫图片
│   │   ├── scene_001.jpg
│   │   ├── scene_002.jpg
│   │   └── ...
│   ├── videos/                   # 原动漫视频片段
│   │   ├── clip_001.mp4
│   │   ├── clip_002.mp4
│   │   └── ...
│   └── extracted_frames/        # 提取的关键帧
│       ├── clip_001/
│       │   ├── frame_0001.jpg
│       │   ├── frame_0025.jpg
│       │   └── ...
│       └── ...
├── reference_index.json          # 参考图像索引（包含场景描述、特征等）
└── tools/
    ├── extract_reference_frames.py    # 提取关键帧工具
    ├── match_reference_image.py       # 匹配参考图像工具
    └── build_reference_index.py       # 建立参考图像索引
```

## ⚙️ 配置示例

在 `config.yaml` 中添加：

```yaml
# 参考素材配置
reference_materials:
  # 参考素材根目录
  base_dir: gen_video/reference_materials
  # 参考图像目录
  images_dir: gen_video/reference_materials/images
  # 参考视频目录
  videos_dir: gen_video/reference_materials/videos
  # 提取的关键帧目录
  extracted_frames_dir: gen_video/reference_materials/extracted_frames
  # 参考图像索引文件
  index_file: gen_video/reference_index.json
  
  # IP-Adapter 配置（用于风格迁移）
  ip_adapter:
    enabled: true
    # 参考图像权重（控制风格强度）
    scale: 0.6
    # 使用多个参考图像
    use_multiple_references: true
    # 参考图像数量
    num_references: 1
  
  # img2img 配置（用于二次创作）
  img2img:
    enabled: true
    # 修改强度（0.0-1.0，越小越接近原图）
    strength: 0.5
    # 是否自动选择参考图像
    auto_select_reference: true
  
  # 智能匹配配置
  matching:
    # 使用语义搜索匹配参考图像
    use_semantic_search: true
    # 相似度阈值（0.0-1.0）
    similarity_threshold: 0.6
    # 是否使用图像特征匹配
    use_feature_matching: true
```

## 🚀 使用流程

### 步骤1：准备参考素材

```bash
# 1. 将原动漫图片放到 reference_materials/images/
# 2. 将原动漫视频片段放到 reference_materials/videos/

# 3. 从视频中提取关键帧
python tools/extract_reference_frames.py \
    --input reference_materials/videos/ \
    --output reference_materials/extracted_frames/ \
    --method scene_change  # 按场景变化提取
```

### 步骤2：建立参考图像索引

```bash
# 建立参考图像索引（包含场景描述、特征向量等）
python tools/build_reference_index.py \
    --input reference_materials/ \
    --output reference_index.json
```

### 步骤3：配置生成参数

在场景JSON中，可以指定使用参考素材：

```json
{
  "id": 1,
  "narration": "韩立来到洞府前，仔细观察周围的环境",
  "description": "洞府入口，夜色，月光",
  "reference": {
    "enabled": true,
    "type": "ip_adapter",  // 或 "img2img"
    "auto_select": true,   // 自动选择参考图像
    "image_path": null,    // 或手动指定路径
    "strength": 0.5        // img2img 强度
  }
}
```

### 步骤4：生成视频

```bash
# 正常使用，系统会自动使用参考素材
python main.py --script episode_1.json --output episode_1
```

## 🎨 技术细节

### IP-Adapter 风格迁移

1. **单参考图像**：使用一个参考图像控制整体风格
2. **多参考图像**：使用多个参考图像（风格、构图、色彩分别控制）
3. **权重调节**：通过 `ip_adapter_scale` 控制风格强度（0.4-0.8）

### img2img 二次创作

1. **低强度** (strength=0.3-0.4)：轻微修改，保留大部分原图
2. **中强度** (strength=0.5-0.6)：适度修改，平衡原图和新场景
3. **高强度** (strength=0.7-0.8)：大幅修改，主要保留构图

### 智能匹配

1. **语义搜索**：使用 CLIP 模型提取场景描述的语义特征，匹配参考图像
2. **特征匹配**：使用图像特征（颜色、构图、物体）进行匹配
3. **混合匹配**：结合语义和特征，选择最佳参考图像

## 📊 效果对比

| 方案 | 风格一致性 | 场景匹配度 | 生成速度 | 推荐度 |
|------|----------|----------|---------|--------|
| 直接拼接原视频 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ |
| IP-Adapter | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |
| img2img | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| 混合方案 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

## 🔧 实现优先级

1. **第一阶段**：实现参考图像提取和索引建立
2. **第二阶段**：实现 IP-Adapter 风格迁移
3. **第三阶段**：实现 img2img 二次创作
4. **第四阶段**：实现智能匹配和自动化流程

## 💡 最佳实践

1. **参考图像选择**：
   - 选择高质量、风格统一的参考图像
   - 按场景类型分类存储
   - 建立详细的索引和标注

2. **参数调节**：
   - IP-Adapter scale: 0.5-0.7（平衡风格和场景）
   - img2img strength: 0.4-0.6（适度修改）
   - 根据场景类型动态调整

3. **质量控制**：
   - 生成后检查风格一致性
   - 对比原素材和新生成内容
   - 根据效果调整参数

## 🎯 总结

通过本方案，您可以：
- ✅ 充分利用原动漫素材的风格和构图
- ✅ 生成符合解说场景的新内容
- ✅ 保持视觉风格的一致性
- ✅ 实现批量自动化处理

**推荐使用混合方案**，结合 IP-Adapter 和 img2img，既能保持风格，又能灵活调整场景内容。


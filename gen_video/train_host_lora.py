#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒç§‘æ™®ä¸»æŒäºº LoRA
ä½¿ç”¨ diffusers + PEFT è¿›è¡Œè®­ç»ƒ
"""

import os
import torch
from pathlib import Path
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from diffusers import DiffusionPipeline, UNet2DConditionModel, DDPMScheduler
from peft import LoraConfig, get_peft_model, set_peft_model_state_dict
from transformers import CLIPTokenizer
from accelerate import Accelerator
from tqdm import tqdm
import argparse


class HostDataset(Dataset):
    """ä¸»æŒäººè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, data_dir: str, tokenizer, size: int = 1024):
        self.data_dir = Path(data_dir)
        self.tokenizer = tokenizer
        self.size = size
        
        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
        self.images = []
        image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.JPG', '.JPEG', '.PNG', '.WEBP'}
        
        for img_file in sorted(self.data_dir.iterdir()):
            if img_file.suffix in image_extensions:
                # ä»æ–‡ä»¶åæå–æç¤ºè¯
                prompt = self._extract_prompt_from_filename(img_file.name)
                self.images.append({
                    'path': img_file,
                    'prompt': prompt
                })
        
        print(f"âœ… æ‰¾åˆ° {len(self.images)} å¼ è®­ç»ƒå›¾ç‰‡")
    
    def _extract_prompt_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæå–æç¤ºè¯"""
        # æ–‡ä»¶åæ ¼å¼ï¼š_repeat_10_æç¤ºè¯.jpg
        if '_repeat_' in filename:
            parts = filename.split('_repeat_', 1)
            if len(parts) > 1:
                prompt_part = parts[1].split('_', 1)
                if len(prompt_part) > 1:
                    prompt = prompt_part[1]
                    # ç§»é™¤æ‰©å±•å
                    prompt = prompt.rsplit('.', 1)[0]
                    return prompt
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤æç¤ºè¯
        return "ç§‘æ™®ä¸»æŒäººï¼Œä¸“ä¸šå½¢è±¡"
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        item = self.images[idx]
        
        # åŠ è½½å›¾ç‰‡
        image = Image.open(item['path']).convert('RGB')
        
        # è°ƒæ•´å¤§å°ï¼ˆå¦‚æœå·²ç»æ˜¯ 1024x1024 å¯ä»¥è·³è¿‡ï¼‰
        if image.size != (self.size, self.size):
            image = image.resize((self.size, self.size), Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸º tensor (å½’ä¸€åŒ–åˆ° [-1, 1])
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # å½’ä¸€åŒ–åˆ° [-1, 1]
        ])
        image_tensor = transform(image)
        
        # Tokenize æç¤ºè¯
        prompt = item['prompt']
        text_inputs = self.tokenizer(
            prompt,
            padding="max_length",
            max_length=77,
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            'pixel_values': image_tensor,
            'input_ids': text_inputs.input_ids.squeeze(),
            'prompt': prompt
        }


def train_lora(
    data_dir: str,
    output_dir: str,
    base_model_path: str,
    num_train_epochs: int = 10,
    train_batch_size: int = 1,
    gradient_accumulation_steps: int = 4,
    learning_rate: float = 1e-4,
    lora_rank: int = 32,
    lora_alpha: int = 16,
    save_steps: int = 200,
    resolution: int = 1024
):
    """
    è®­ç»ƒ LoRA
    
    Args:
        data_dir: è®­ç»ƒæ•°æ®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆFlux.1ï¼‰
        num_train_epochs: è®­ç»ƒè½®æ•°
        train_batch_size: æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        learning_rate: å­¦ä¹ ç‡
        lora_rank: LoRA ç»´åº¦
        lora_alpha: LoRA alpha
        save_steps: æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡
        resolution: å›¾ç‰‡åˆ†è¾¨ç‡
    """
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒç§‘æ™®ä¸»æŒäºº LoRA")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=gradient_accumulation_steps,
        mixed_precision="fp16"
    )
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    # ä½¿ç”¨ "balanced" è€Œä¸æ˜¯ "auto"ï¼ˆFlux æ¨¡å‹è¦æ±‚ï¼‰
    pipe = DiffusionPipeline.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="balanced"
    )
    
    # 3. é…ç½® LoRA
    print(f"\nğŸ”§ é…ç½® LoRA (rank={lora_rank}, alpha={lora_alpha})")
    
    # Flux æ¨¡å‹ä½¿ç”¨ transformer è€Œä¸æ˜¯ unet
    # æ£€æŸ¥æ¨¡å‹æ¶æ„
    if hasattr(pipe, 'transformer'):
        # Flux æ¨¡å‹ï¼šä½¿ç”¨ transformer
        model_component = pipe.transformer
        model_name = "transformer"
        
        # Flux transformer çš„æ³¨æ„åŠ›å±‚åç§°
        target_modules = [
            "attn.to_k",
            "attn.to_q",
            "attn.to_v",
            "attn.to_out.0",
        ]
    elif hasattr(pipe, 'unet'):
        # æ ‡å‡† SDXL/SD æ¨¡å‹ï¼šä½¿ç”¨ unet
        model_component = pipe.unet
        model_name = "unet"
        
        # UNet çš„æ³¨æ„åŠ›å±‚åç§°
        target_modules = [
            "to_k",
            "to_q",
            "to_v",
            "to_out.0",
        ]
    else:
        raise ValueError("æ— æ³•æ‰¾åˆ° transformer æˆ– unet ç»„ä»¶")
    
    print(f"  â„¹ æ£€æµ‹åˆ°æ¨¡å‹ç»„ä»¶: {model_name}")
    print(f"  â„¹ ç›®æ ‡æ¨¡å—: {target_modules}")
    
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.1,
    )
    
    # 4. åº”ç”¨ LoRA åˆ°æ¨¡å‹ç»„ä»¶
    if model_name == "transformer":
        pipe.transformer = get_peft_model(pipe.transformer, lora_config)
        trainable_model = pipe.transformer
    else:
        pipe.unet = get_peft_model(pipe.unet, lora_config)
        trainable_model = pipe.unet
    
    # 5. å‡†å¤‡æ•°æ®é›†
    print(f"\nğŸ“ å‡†å¤‡è®­ç»ƒæ•°æ®: {data_dir}")
    dataset = HostDataset(
        data_dir=data_dir,
        tokenizer=pipe.tokenizer,
        size=resolution
    )
    
    if len(dataset) == 0:
        raise ValueError(f"æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼è¯·æ£€æŸ¥ç›®å½•: {data_dir}")
    
    dataloader = DataLoader(
        dataset,
        batch_size=train_batch_size,
        shuffle=True,
        num_workers=2
    )
    
    # 6. è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        trainable_model.parameters(),
        lr=learning_rate
    )
    
    # 7. å‡†å¤‡è®­ç»ƒ
    trainable_model, optimizer, dataloader = accelerator.prepare(
        trainable_model, optimizer, dataloader
    )
    
    # 8. è®­ç»ƒå¾ªç¯
    num_update_steps_per_epoch = len(dataloader) // gradient_accumulation_steps
    max_train_steps = num_train_epochs * num_update_steps_per_epoch
    
    print(f"\nğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   è®­ç»ƒè½®æ•°: {num_train_epochs}")
    print(f"   æ€»æ­¥æ•°: {max_train_steps}")
    print(f"   æ‰¹æ¬¡å¤§å°: {train_batch_size}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   åˆ†è¾¨ç‡: {resolution}x{resolution}")
    
    global_step = 0
    progress_bar = tqdm(range(max_train_steps), desc="è®­ç»ƒä¸­")
    
    trainable_model.train()
    
    for epoch in range(num_train_epochs):
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(trainable_model):
                # è·å–è®¾å¤‡ï¼ˆç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ï¼‰
                device = next(trainable_model.parameters()).device
                
                # å‰å‘ä¼ æ’­ - VAE ç¼–ç 
                # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…ï¼ˆVAE ä½¿ç”¨ float16ï¼‰
                pixel_values = batch['pixel_values'].to(device, dtype=torch.float16)
                with torch.no_grad():
                    latents = pipe.vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * pipe.vae.config.scaling_factor
                
                # ä¿å­˜åŸå§‹ latents å½¢çŠ¶ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
                original_latent_shape = latents.shape
                
                # æ·»åŠ å™ªå£°ï¼ˆFlux ä½¿ç”¨ Flow Matchingï¼Œéœ€è¦ä¸åŒçš„å¤„ç†ï¼‰
                noise = torch.randn_like(latents, device=device, dtype=latents.dtype)
                
                # æ£€æŸ¥è°ƒåº¦å™¨ç±»å‹
                scheduler_name = type(pipe.scheduler).__name__
                if "Flow" in scheduler_name or "FlowMatch" in scheduler_name:
                    # Flux Flow Matchingï¼šä½¿ç”¨æ—¶é—´æ­¥é‡‡æ ·
                    # Flow Matching ä½¿ç”¨è¿ç»­æ—¶é—´ t âˆˆ [0, 1]
                    timesteps = torch.rand(
                        (latents.shape[0],),
                        device=device,
                        dtype=latents.dtype
                    )  # éšæœºæ—¶é—´æ­¥ [0, 1]
                    
                    # Flow Matching çš„å™ªå£°æ·»åŠ æ–¹å¼
                    # x_t = (1 - t) * x_0 + t * x_1ï¼Œå…¶ä¸­ x_1 æ˜¯å™ªå£°
                    t = timesteps.view(-1, 1, 1, 1)  # å¹¿æ’­åˆ°ç©ºé—´ç»´åº¦
                    noisy_latents = (1 - t) * latents + t * noise
                else:
                    # æ ‡å‡†æ‰©æ•£æ¨¡å‹ï¼ˆDDPM/DDIMï¼‰
                    timesteps = torch.randint(
                        0, pipe.scheduler.config.num_train_timesteps,
                        (latents.shape[0],),
                        device=device
                    )
                    noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)
                
                # ç¼–ç æç¤ºè¯ï¼ˆFlux ä½¿ç”¨åŒ T5 ç¼–ç å™¨ï¼‰
                input_ids = batch['input_ids'].to(device)
                with torch.no_grad():
                    # Flux ä½¿ç”¨ text_encoder_1 å’Œ text_encoder_2ï¼ˆT5ï¼‰
                    if hasattr(pipe, 'text_encoder_1') and hasattr(pipe, 'text_encoder_2'):
                        # Flux çš„åŒç¼–ç å™¨
                        prompt_embeds_1 = pipe.text_encoder_1(input_ids)[0]
                        prompt_embeds_2 = pipe.text_encoder_2(input_ids)[0]
                        encoder_hidden_states = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)
                    elif hasattr(pipe, 'text_encoder'):
                        # æ ‡å‡†ç¼–ç å™¨ï¼ˆå¤‡ç”¨ï¼‰
                        encoder_hidden_states = pipe.text_encoder(input_ids)[0]
                    else:
                        raise ValueError("æ— æ³•æ‰¾åˆ° text encoder")
                
                # é¢„æµ‹å™ªå£°ï¼ˆFlux ä½¿ç”¨ transformerï¼ŒSDXL ä½¿ç”¨ unetï¼‰
                is_flow_matching = "Flow" in scheduler_name or "FlowMatch" in scheduler_name
                
                if model_name == "transformer":
                    # Flux transformer çš„è°ƒç”¨æ–¹å¼
                    # Flux transformer éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼
                    # éœ€è¦å°† latents é‡å¡‘ä¸ºæ­£ç¡®çš„å½¢çŠ¶
                    batch_size = noisy_latents.shape[0]
                    height, width = noisy_latents.shape[2], noisy_latents.shape[3]
                    
                    # Flux transformer æœŸæœ›çš„è¾“å…¥æ ¼å¼
                    # éœ€è¦å°† (B, C, H, W) é‡å¡‘ä¸º (B*H*W, C) æˆ–ç±»ä¼¼æ ¼å¼
                    # ä½†å®é™…æ ¼å¼å¯èƒ½ä¸åŒï¼Œéœ€è¦æ ¹æ®æ¨¡å‹è¦æ±‚è°ƒæ•´
                    
                    if is_flow_matching:
                        # Flow Matchingï¼šæ—¶é—´æ­¥å·²ç»æ˜¯ [0, 1] èŒƒå›´
                        try:
                            # å°è¯•æ ‡å‡†è°ƒç”¨
                            model_pred = pipe.transformer(
                                hidden_states=noisy_latents,
                                timestep=timesteps,
                                encoder_hidden_states=encoder_hidden_states,
                            ).sample
                        except RuntimeError as e:
                            if "shapes cannot be multiplied" in str(e):
                                # å½¢çŠ¶ä¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„è¾“å…¥æ ¼å¼
                                # Flux å¯èƒ½éœ€è¦å°† latents å±•å¹³æˆ–é‡å¡‘
                                # å°è¯•ä½¿ç”¨ pipe çš„ encode_prompt å’Œæ ‡å‡†ç”Ÿæˆæµç¨‹
                                # æˆ–è€…ä½¿ç”¨ pipe çš„ __call__ æ–¹æ³•
                                print(f"âš ï¸  Flux transformer è¾“å…¥å½¢çŠ¶é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ pipe çš„æ ‡å‡†æ–¹æ³•")
                                # å¯¹äºè®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦ç›´æ¥è°ƒç”¨ transformer
                                # å¯èƒ½éœ€è¦è°ƒæ•´è¾“å…¥å½¢çŠ¶
                                # å°è¯•ï¼šå°† latents é‡å¡‘ä¸º transformer æœŸæœ›çš„æ ¼å¼
                                # Flux transformer å¯èƒ½éœ€è¦ (B, H*W, C) æ ¼å¼
                                latent_height, latent_width = noisy_latents.shape[2], noisy_latents.shape[3]
                                noisy_latents_reshaped = noisy_latents.permute(0, 2, 3, 1).reshape(
                                    batch_size, latent_height * latent_width, -1
                                )
                                model_pred = pipe.transformer(
                                    hidden_states=noisy_latents_reshaped,
                                    timestep=timesteps,
                                    encoder_hidden_states=encoder_hidden_states,
                                ).sample
                                # é‡å¡‘å›åŸå§‹å½¢çŠ¶
                                model_pred = model_pred.reshape(
                                    batch_size, latent_height, latent_width, -1
                                ).permute(0, 3, 1, 2)
                            else:
                                raise
                    else:
                        # æ ‡å‡†æ‰©æ•£
                        model_pred = pipe.transformer(
                            hidden_states=noisy_latents,
                            timestep=timesteps,
                            encoder_hidden_states=encoder_hidden_states,
                        ).sample
                else:
                    # æ ‡å‡† UNet
                    model_pred = pipe.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states
                    ).sample
                
                # è®¡ç®—æŸå¤±
                if is_flow_matching:
                    # Flow Matchingï¼šé¢„æµ‹é€Ÿåº¦åœº v_t = x_1 - x_0
                    # ç›®æ ‡é€Ÿåº¦åœºæ˜¯ noise - latents
                    target_velocity = noise - latents
                    loss = torch.nn.functional.mse_loss(model_pred.float(), target_velocity.float())
                else:
                    # æ ‡å‡†æ‰©æ•£ï¼šé¢„æµ‹å™ªå£°
                    loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float())
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()
                
                # æ›´æ–°è¿›åº¦æ¡
                if step % 10 == 0:  # æ¯ 10 æ­¥æ›´æ–°ä¸€æ¬¡
                    progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
            
            global_step += 1
            progress_bar.update(1)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if global_step % save_steps == 0:
                if accelerator.is_main_process:
                    checkpoint_dir = Path(output_dir) / f"checkpoint-{global_step}"
                    checkpoint_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ä¿å­˜ LoRA æƒé‡
                    trainable_model.save_pretrained(str(checkpoint_dir))
                    print(f"\nğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_dir}")
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {output_dir}")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    if accelerator.is_main_process:
        trainable_model.save_pretrained(str(output_path))
        
        # ä¹Ÿä¿å­˜ä¸º safetensors æ ¼å¼ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            from safetensors.torch import save_file
            # åªä¿å­˜ LoRA æƒé‡ï¼ˆä¸æ˜¯å®Œæ•´æ¨¡å‹ï¼‰
            state_dict = {}
            for name, param in trainable_model.named_parameters():
                if 'lora' in name.lower():
                    state_dict[name] = param.data.cpu()
            
            if state_dict:
                safetensors_path = output_path / "pytorch_lora_weights.safetensors"
                save_file(state_dict, str(safetensors_path))
                print(f"âœ… å·²ä¿å­˜ safetensors: {safetensors_path}")
            else:
                print("âš ï¸  æœªæ‰¾åˆ° LoRA æƒé‡ï¼Œä½¿ç”¨ save_pretrained ä¿å­˜")
        except ImportError:
            print("âš ï¸  æœªå®‰è£… safetensorsï¼Œè·³è¿‡ safetensors æ ¼å¼ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ safetensors æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨ save_pretrained ä¿å­˜")
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“ ä½¿ç”¨æ–¹å¼:")
    print(f"   lora_path = '{output_dir}/pytorch_lora_weights.safetensors'")
    print(f"   æˆ– '{output_dir}' (ç›®å½•)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è®­ç»ƒç§‘æ™®ä¸»æŒäºº LoRA")
    parser.add_argument(
        "--data-dir",
        type=str,
        default="train_data/host_person",
        help="è®­ç»ƒæ•°æ®ç›®å½•ï¼ˆé»˜è®¤: train_data/host_personï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="models/lora/host_person",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: models/lora/host_personï¼‰"
    )
    parser.add_argument(
        "--base-model",
        type=str,
        default="models/flux1-dev",
        help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: models/flux1-devï¼‰"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=10,
        help="è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 10ï¼‰"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 1ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰"
    )
    parser.add_argument(
        "--gradient-accumulation",
        type=int,
        default=4,
        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆé»˜è®¤: 4ï¼‰"
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-4,
        help="å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 1e-4ï¼‰"
    )
    parser.add_argument(
        "--lora-rank",
        type=int,
        default=32,
        help="LoRA ç»´åº¦ï¼ˆé»˜è®¤: 32ï¼‰"
    )
    parser.add_argument(
        "--lora-alpha",
        type=int,
        default=16,
        help="LoRA alphaï¼ˆé»˜è®¤: 16ï¼‰"
    )
    parser.add_argument(
        "--save-steps",
        type=int,
        default=200,
        help="æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡ï¼ˆé»˜è®¤: 200ï¼‰"
    )
    parser.add_argument(
        "--resolution",
        type=int,
        default=1024,
        help="å›¾ç‰‡åˆ†è¾¨ç‡ï¼ˆé»˜è®¤: 1024ï¼‰"
    )
    
    args = parser.parse_args()
    
    train_lora(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        base_model_path=args.base_model,
        num_train_epochs=args.epochs,
        train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        save_steps=args.save_steps,
        resolution=args.resolution
    )

